[{"content":"Sentiment Analysis A to B: Episode 1 In this series, I will work my way into different Sentiment Analysis methods and experiment with other techniques. I will use the data from the IMDB review dataset acquired from Kaggle. The series is called A to B since I need to cover all the methods and the best, for that matter. I am covering some I find exciting and test-worthy.\nIn this episode, I will be examining/going over the following:\nData preprocessing for sentiment analysis 2 different feature representations: Sparse vector representation Word frequency counts Comparison using logistic regression Feature Representation Your model will be, at most, as good as your data, and your data will be only as good as you understand them to be, hence the features. I want to see the most useless or naive approaches and agile methods and benchmark them for both measures of prediction success and for training and prediction time.\nBefore anything else, let\u0026rsquo;s load, organize and clean our data really quick:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import CSV def get_data(): with open(\u0026#39;data/kaggle_data/movie.csv\u0026#39;, \u0026#39;r\u0026#39;) as f: data = list(CSV.reader(f, delimiter=\u0026#39;,\u0026#39;)) return data def split_data(data): # split 80/10/10 train_split = int(0.8 * len(data)) val_split = int(0.9 * len(data)) return data[:train_split], data[train_split:val_split], data[val_split:] def save_data(data, filename): with open(filename, \u0026#39;w\u0026#39;) as f: writer = csv.writer(f) writer.writerows(data) def main(): data = get_data() train, val, test = split_data(data[1:]) save_data(train, \u0026#39;data/train.csv\u0026#39;) save_data(val, \u0026#39;data/val.csv\u0026#39;) save_data(test, \u0026#39;data/test.csv\u0026#39;) Let\u0026rsquo;s start with creating a proper and clean vocabulary that we will use for all the representations we will examine.\nClean Vocabulary We just read all the words as a set, to begin with,\n1 2 3 4 5 6 7 # Get all the words words = [w for s in train_data for w in s[0].split()] # len(words) = 7391216 # Get the vocabulary dirty_vocab = set(words) # len(dirty_vocab) = 331056 So for the beginning of the representation, we have 331.056 words in our vocabulary. This number is every non-sense included, though. We also didn\u0026rsquo;t consider any lowercase - uppercase conversion. So let\u0026rsquo;s clean these step by step.\n1 2 3 4 5 6 7 8 9 # Convert to lowercase vocab = set([w.lower() for w in dirty_vocab]) # len(vocab) = 295827 # Remove punctuation from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer(r\u0026#39;\\w+\u0026#39;) vocab = set([w.lower() for w in tokenizer.tokenize(\u0026#39; \u0026#39;.join(vocab))]) # len(vocab) = 84757 We reduced the number from 331.056 to 84.757. We can do more. With this method, we encode every word we see in every form possible. So, for example, \u0026ldquo;called,\u0026rdquo; \u0026ldquo;calling,\u0026rdquo; \u0026ldquo;calls,\u0026rdquo; and \u0026ldquo;call\u0026rdquo; will all be a separate words. Let\u0026rsquo;s get rid of that and make them reduce to their roots. Here we start getting help from the dedicated NLP library NLTK since I don\u0026rsquo;t want to define all these rules myself (nor could I):\n1 2 3 4 5 # Reduce words to their stems from nltk.stem import PorterStemmer stemmer = PorterStemmer() vocab = set([stemmer.stem(w) for w in vocab]) # len(vocab) = 58893 The last step towards cleaning will be to get rid of stopwords. These are \u0026rsquo;end,\u0026rsquo; \u0026lsquo;are,\u0026rsquo; \u0026lsquo;is,\u0026rsquo; etc. words in the English language.\n1 2 3 4 5 6 7 8 # Remove connectives import nltk nltk.download(\u0026#39;stopwords\u0026#39;) from nltk.corpus import stopwords stop_words = set(stopwords.words(\u0026#39;English)) vocab = vocab - stop_words # len(vocab) = 58764 Now that we have good words, we can set up a lookup table to keep encodings for each word.\n1 2 # Vocabulary dictionary vocab_dict = {w: i for i, w in enumerate(vocab)} Now we have a dictionary for every proper word we have in the data set. Therefore, we are ready to prepare different feature representations.\nSince we will convert sentences in this clean form, again and again, later on, let\u0026rsquo;s create a function that combines all these methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Function to combine all the above to clean a sentence def clean_sentence(sentence): # Convert to lowercase sentence = sentence.lower() # Words words = sentence.split() # Remove punctuation tokenizer = RegexpTokenizer(r\u0026#39;\\w+\u0026#39;) words = tokenizer.tokenize(\u0026#39; \u0026#39;.join(words)) # Remove stop words stop_words = set(stopwords.words(\u0026#39;English)) words = [w for w in words if w not in stop_words] # Reduce words to their stems stemmer = PorterStemmer() words = [stemmer.stem(w) for w in words] # remove repeated words return set(words) Ideally, we could initialize tokenizer stemmer and stop_words globally (or as a class parameter), so we don\u0026rsquo;t have to keep initializing.\nSparse Vector Representation This will represent every word we see in the database as a feature… Sounds unfeasible? Yeah, it should be. I see multiple problems here. The main one we all think about is this is a massive vector for each sentence with a lot of zeros (hence the name). This means most of the data we have is telling us practically the same thing as the minor part; we have these words in this sentence vs. we don\u0026rsquo;t have all these words. Second, we are not keeping any correlation between words (since we are just examining word by word).\nWe go ahead and create a function for encoding every word for a sentence:\n1 2 3 4 5 6 7 8 9 # function to convert a sentence to a vector encoding def encode_sparse(sentence): words = sentence.split() vec = np.zeros(len(vocab)) clean_words = clean_sentence(sentence) for w in clean_words: if w in vocab_dict: vec[vocab_dict[w]] += 1 return vec We then convert all the data we have using this encoding (in a single matrix):\n1 2 train_data_sparse = np.array([encode_sparse(s[0]) for s in train_data]), np.array([int(s[1]) for s in train_data]) val_data_sparse = np.array([encode_sparse(s[0]) for s in val_data]), np.array([int(s[1]) for s in val_data]) That\u0026rsquo;s it for this representation.\nWord Frequency Representation This version practically reduces the 10.667 dimensions to 3 instead. We are going to count the number of negative sentences a word passes in as well as positive sentences. This will give us a table indicating how many positive and negative sentences a word has found in:\n1 2 3 4 5 6 7 # Counting frequency of words freqs = np.zeros((len(vocab), 2)) # [positive, negative] for i, s in enumerate(train_data): words = clean_sentence(s[0]) for w in words: if w in vocab_dict: freqs[vocab_dict[w], int(s[1])] += 1 The next thing to do is to convert these enormous numbers into probabilities. There are multiple points to add here: First, we are getting the probability of this single word being in many positive and negative sentences, so the values will be minimal. Hence we need to use a log scale to avoid floating point problems. Second is, we might get words that don\u0026rsquo;t appear in our dictionary, which will have a likelihood of 0. Since we don\u0026rsquo;t want a 0 division, we add laplacian smoothing, like normalizing all the values with a small initial. Here goes the code:\n1 2 # Convert to log probabilities with Laplace smoothing freqs = np.log((counts + 1) / (np.sum(counts, axis=0) + len(vocab))) After getting the frequencies and fixing the problems we mentioned, we now define the new encoding method for this version of the features\n1 2 3 4 5 6 7 8 def encode_freq(sentence): words = clean_sentence(sentence) vec = np.array([1., 0., 0.]) # [bias, positive, negative] for word in words: if word in vocab_dict: vec[1] += freqs[vocab_dict[word], 0] vec[2] += freqs[vocab_dict[word], 1] return vec We end by converting our data as before\n1 2 train_data_pos_neg = np.array([encode_freq(s[0]) for s in train_data]), np.array([int(s[1]) for s in train_data]) val_data_pos_neg = np.array([encode_freq(s[0]) for s in val_data]), np.array([int(s[1]) for s in val_data]) Let\u0026rsquo;s take a sneak peek at what our data looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Visualize the data with PCA from sklearn.decomposition import PCA import matplotlib.pyplot as plt %matplotlib inline # Create a PCA instance. # This will reduce the data to 2 dimensions # as opposed to 3, where we have 2 features and a bias, more on that in next episode. PCA = PCA(n_components=2) # Fit the PCA instance to the training data x_data = train_data_pos_neg[0] PCA.fit(x_data) # Transform the training data to 2 dimensions ignoring the bias. This is due to the fact that the bias is a constant and will not affect the PCA x_data_2d = PCA.transform(x_data) plt.scatter(x_data_2d[:, 0], x_data_2d[:, 1], c=train_data_pos_neg[1], cmap=\u0026#39;bwr\u0026#39;) plt.xlabel(\u0026#39;First Principal Component\u0026#39;) plt.ylabel(\u0026#39;Second Principal Component\u0026#39;) # Setup legend import matplotlib.patches as mpatches red_patch = mpatches.Patch(color=\u0026#39;red\u0026#39;, label=\u0026#39;Negative\u0026#39;) blue_patch = mpatches.Patch(color=\u0026#39;blue\u0026#39;, label=\u0026#39;Positive\u0026#39;) plt.legend(handles=[red_patch, blue_patch]) A better would be to use PCA for this kind of representation, but for now, we will ignore that fact since we want to explore that in episode 2.\nModel Development This episode mainly focuses on cleaning the data and developing decent representations. This is why I will only include a single model to test everything; Logistic Regression.\nLogistic Regression Logistic regression is a simple single-layer network with sigmoid activation. This is an excellent baseline as it is one of the simplest binary classification methods. I am not explaining this method in depth, so if you want to learn more, please do so. I will use a simple PyTorch implementation.\n1 2 3 4 5 6 7 class LogisticRegression(nn.Module): def __init__(self, input_dim): super(LogisticRegression, self).__init__() self.linear = nn.Linear(input_dim, 1) def forward(self, x): return torch.sigmoid(self.linear(x)) We then define the loss function and the optimizer to use. I am using Binary Cross Entropy for the loss function and Adam for the optimization with a learning rate of 0.01.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 device = torch.device(\u0026#39;cuda\u0026#39; if torch.Cuda.is_available() else \u0026#39;CPU\u0026#39;) model = LogisticRegression(x_data.shape[1]).to(device) optimizer = optim.Adam(model.parameters(), lr=0.01) criterion = nn.BCELoss() # Train the model num_epochs = 100 train_loss = [] val_loss = [] for epoch in range(num_epochs): # Forward pass outputs = model(X_train) loss = criterion(outputs, y_train.unsqueeze(1)) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() train_loss.append(loss.item()) # Validation with torch.no_grad(): outputs = model(X_val) loss = criterion(outputs, y_val.unsqueeze(1)) val_loss.append(loss.item()) if (epoch) % 100 == 0: print (\u0026#39;Epoch [{}/{}], Loss: {:.4f}\u0026#39; .format(epoch, num_epochs, loss.item())) Sparse Representation Training We first start with training the sparse representation. I trained for 100 epochs and reached 0.614 training accuracy and 0.606 validation accuracy. Here is the learning curve\nWord Frequency Representation Training I trained using the same parameter settings above, reaching 0.901 training accuracy and 0.861 validation accuracy. Here is the learning curve in the log scale\n","permalink":"https://bedirtapkan.com/posts/blog_posts/sentiment_ab_1/","summary":"Sentiment Analysis A to B: Episode 1 In this series, I will work my way into different Sentiment Analysis methods and experiment with other techniques. I will use the data from the IMDB review dataset acquired from Kaggle. The series is called A to B since I need to cover all the methods and the best, for that matter. I am covering some I find exciting and test-worthy.\nIn this episode, I will be examining/going over the following:","title":"Sentiment Analysis A to B: Episode 1"},{"content":" This post includes my notes from the lecture “Makemore Part 3: Activations \u0026amp; Gradients, BatchNorm” by Andrej Karpathy.\nInitialization Fixing the initial Loss:\nInitial loss must be arranged (the value depends on the question), in our case its a uniform probability. When initializing make sure the numbers do not take extreme values (* .01) Do not initialize to 0 Having 0 and 1 in softmax (a lot of them) is really bad, since the gradient will be 0 (vanishing gradient). This is called saturated tanh. So in summary: What we did is basically just making sure initially we give the network random values such that it is varied, and not making gradients 0 (no dead neurons). In the case of tanh, when we use softmax to squash the values, if the initial values were too broad, we will get a lot of vanishing gradients due to values ending up above 1 or below -1. So we first reduce the initial values and then use softmax on them (and continue training process). Kaiming Init Okay we know how to fix initialization now, but how much should we reduce these numbers? Meaning what is the value we should scale the layers with. Here comes Kaiming init.\nHere are two plots, left is for x and right is for y (pre activation, x @ w) layer values. We see that even though x and w are uniform gaussian with unit mean and standard deviation, the result of their dot product, y, has a non-unit standard deviation (still gaussian).\nWe don’t want this in a neural network, we want the nn to have relatively simple activations, so we want unit gaussian throughout the network.\nTo keep std of y unit, we need to scale w down, as shown in the figure below (w scaled by 0.2), but with what exactly?\nMathematically this scale is equal to the square root of fan-in (number of input dimensions, e.g 10 for a tensor of (10, 1000)).\nDepending on the activation function used, this value needs to be scaled by a gain. This gain is $\\frac{5}{3}$ for tanh and 1 for linear, and $\\sqrt{2}$ for relu. These values are due to shrinking and clamping the values (on relu and tanh).\nKaiming init is implemented in pytorch as torch.nn.init.kaiming_normal_.\nSince the development of more sophisticated techniques in neural networks the importance of accurately initializing weights became unnecessary. To name some; residual connections, some normalizations (batch normalization etc.), optimizers (adam, rmsprop).\nIn practice, just normalizing by square root of fan-in is enough.\nNow that we see how to initialize the network, and mentioned some methods that makes this process more relaxed, let’s talk about one of these innovations; batch normalization\nBatch Normalization We mentioned while training the network that we want balance in the pre-activation values, we don’t want them to be zero, or too small so that tanh actually does something, and we don’t want them to be too large because then tanh is saturated.\nSo we want roughly a uniform gaussian at initialization.\nBatch Normalization basically says, why don’t we just take the hidden states and normalize them to be gaussian.\nRight before the activation, we standardize the weights to be unit gaussian. We will do this by getting the mean and std of the batch, and scaling the values. Since all these operations are easily differentiable there will be no issues during the backprop phase.\nFor our example;\n1 2 3 4 # Forward pass emb = C[x_train[idx]] embcat = emb.view(-1, emb.shape[1] * emb.shape[2]) hpreact = embcat @ W1 + b1 Will have the batch norm before the activation is introduced. For this we need to calculate the mean and standard deviation of the batch;\n1 2 hmean = hpreact.mean(0, keepdim=True) hstd = hpreact.std(0, keepdim=True) Here we use 0 for the dimension since the shape of preact is [num_samples, num_hidden_layers] and we want the mean and std for all the samples for the weight connecting to one hidden layer. So the dimensions of hmean and hstd will be [1, num_hidden_layers]. So in the end we update our hpreact to;\n1 hpreact_bn = (hpreact - hmean) / hstd If we leave it at that, we now have the weights forced to be unit gaussian at every step of the training. We want this to be the case only at the initialization. In general case we want the neural network to be able to move the distribution and scale it. So we introduce one more component called scale and shift.\nThese will be two new parameter set we add on our list that we start the scale with 1 and shift with 0. We then backpropagate through these values and give the network the freedom to shift and scale the distribution;\n1 2 bnorm_scale = torch.ones((1, num_hidden_neurons)) bnorm_bias = torch.zeros((1, num_hidden_neurons)) We then update our hpreact_bn :\n1 hpreact_bn = ((hpreact - hmean) / hstd) * bnorm_scale + bnorm_bias We also add the new parameters in our parameters, to update while optimize the network:\n1 parameters = [C, W1, b1, W2, b2, bnorm_scale, bnorm_bias] It’s common to use batch norm throughout the neural network to be able to have a more relaxed initializations.\nWhen introduced batch norm, we make the results of the forward and backward pass of any one input dependent on the batches. Meaning the result of a single sample is now not just dependent on itself but the batch it came with as well. Surprisingly, this is unexpectedly proven to be a good thing, acting as a regularizer.\nThis coupling effect is not always desired, which is why some scholars looked into other non-coupling regularizers such as Linear normalization.\nOne thing that still needs adjustment is how to use batch norm in testing phase. We trained the network on batches using batch mean and std but when the model is deployed, we want to use a single sample and get the result based on that. First method for accomplishing this is to calculate the exact mean and std on the complete dataset after training, like:\n1 2 3 4 5 6 with torch.no_grad(): emb = C[x_train] embcat = emb.view(-1, emb.shape[1] * emb.shape[2]) hpreact = embcat @ W1 + b1 bn_mean = hpreact.mean(0, keepdim=True) bn_std = hpreact.std(0, keepdim=True) And using bn_mean and bn_std instead of hmean and hstd from the training loop.\nWe can further eliminate this step using a running mean and std. For this purpose we introduce two new parameters:\n1 2 running_mean = torch.zeros((1, num_hidden_neurons)) running_std = torch.ones((1, num_hidden_neurons)) Then in the main training loop we update these values slowly. This will give us a close estimate.\n1 2 3 4 # Updating running mean and std with torch.no_grad(): running_mean = 0.999 * running_mean + 0.001 * hmean running_std = 0.999 * running_std + 0.001 * hstd There is a minor addition of $\\epsilon$ on the paper to the denominator of the batch normalization. The reason is to avoid division by zero. We did not make use of this epsilon since it is highly unlikely that we get a zero std in our question.\nLast fix we need to do is on bias. When we introduced the batch norm we made the bias b1 useless. This is due to the subtracting the mean after applying the bias. Since the mean includes bias in it, we are practically adding and removing the same value, hence doing an unnecessary operation. In the case of batch norm, we do not need to use explicit bias for that layer, instead the batch norm bias, or bnorm_bias will handle the shifting of the values.\nUse batchnorm carefully. It is really easy to make mistakes, mainly due to coupling. More recent networks usually prefer using layer normalization or group normalization. Batchnorm was very influential around 2015, since it introduced a reliable training for deeper networks because batchnorm was effective on controlling the statistics of the activations.\nDiagnostic Tools Training neural networks without the use of tools that makes initialization more relaxed, such as adam or batch normalization, is excruciating. Here we introduce multitudes of techniques to evaluate the correctness of the neural network.\nActivation Distribution First of, activation distribution throughout the layers. We are using a somehow deep network to be able to see the effects, with 5 layers. Each linear layer is followed by a tanh. As we saw before, tanh kaiming scale is $\\frac{5}{3}$. Here is how the activations look like when we have it right:\nWe see that the layers have somehow similar activations throughout, saturation is around 5% which is what we wanted. If we change the scaling value to $1$ instead:\nWe get an unbalanced activations with 0 saturation. To see even more clear, let’s set the value to $0.5$:\nGradient Distribution The next test is on gradients. Same as before we want the gradients throughout the layers to be similar. Here is the gradient distribution when we actually use $5/3$ as our scaling value:\nAs opposed to $$ $3$:\nWe can see here that the gradients are shrinking.\nWeight-Gradient Distribution on Parameters What are we checking:\nThe std should be similar across layers The mean should be similar across layers The grad:data ratio should be similar across layers Grad:data ratio gives us an intuition of what is the scale of the gradient compared to the actual values. This is important because we will be taking a step update of the form w = w - lr * grad. If the gradient is too large compared to the actual values, we will be overshooting the minimum. If the gradient is too small compared to the actual values, we will be taking too many steps to reach the minimum.\nThe std of the gradient is a measure of how much the gradient changes across the weights. If the std for a layer is too different from the std of the other layers, this will be an issue because this layer will be learning at a different rate than the other layers.\nThis is for initialization phase. If we let the network train for a while, it will fix this issue itself. Nevertheless, this is an issue especially if we are using a simple optimizer like SGD. If we are using an optimizer like Adam, this issue will be fixed automatically.\nHere are examples;\n1 2 3 4 5 6 7 weights torch.Size([33, 10]) - mean +0.000207 - std 3.741454e-03 - grad:data ratio 3.559389e-03 weights torch.Size([50, 100]) - mean +0.000027 - std 2.011833e-03 - grad:data ratio 4.706446e-03 weights torch.Size([100, 100]) - mean -0.000008 - std 1.438244e-03 - grad:data ratio 4.848074e-03 weights torch.Size([100, 100]) - mean -0.000005 - std 9.275978e-04 - grad:data ratio 3.078747e-03 weights torch.Size([100, 100]) - mean -0.000007 - std 7.061330e-04 - grad:data ratio 2.373874e-03 weights torch.Size([100, 100]) - mean -0.000000 - std 5.087151e-04 - grad:data ratio 1.693161e-03 weights torch.Size([100, 33]) - mean +0.000000 - std 2.043289e-02 - grad:data ratio 2.027916e+00 We can see that the ratio of the last layer is way too large, as well as its standard deviation. Which is why the pink line on the graph is too wide.\nUpdate Ratio We calculate the update std’s ratio with real value, and this gives us a measure for learning rate. Roughly the layers are all should be around -3\nThe formula is for each epoch: [(lr * p.grad.std() / p.data.std()).log().item() for p in params].\n","permalink":"https://bedirtapkan.com/posts/blog_posts/makemore3/","summary":"This post includes my notes from the lecture “Makemore Part 3: Activations \u0026amp; Gradients, BatchNorm” by Andrej Karpathy.\nInitialization Fixing the initial Loss:\nInitial loss must be arranged (the value depends on the question), in our case its a uniform probability. When initializing make sure the numbers do not take extreme values (* .01) Do not initialize to 0 Having 0 and 1 in softmax (a lot of them) is really bad, since the gradient will be 0 (vanishing gradient).","title":"Lecture Notes - Makemore Part 3: Activations \u0026 Gradients, BatchNorm"},{"content":"Github Link\nTools Used: C++, Graphviz\nTopics: Deep Learning, Machine Learning\nIntroduction MicroGradCpp is a C++ implementation of MicroGrad, a minimalistic deep learning library. It includes an API to calculate gradients, and to train neural networks in single neuron level. It is a very simple library, and it is very easy to use. It is a very good tool to learn how neural networks work, and how they are trained.\nThe project is a replicate of Andrej Karpathy\u0026rsquo;s MicroGrad with a few extra features and in C++.\nFeatures Custom value and gradient system. Graphviz support for visualizing the graph. Fully working neural network implementation. Here is an implementation of a single neuron:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Value x1 = Value(2.0, \u0026#34;x1\u0026#34;); Value x2 = Value(0.0, \u0026#34;x2\u0026#34;); // weights Value w1 = Value(-3.0, \u0026#34;w1\u0026#34;); Value w2 = Value(1.0, \u0026#34;w2\u0026#34;); // bias Value b = Value(6.8813735870195432, \u0026#34;b\u0026#34;); // neuron (x1*w1 + x2*w2 + b) Value x1w1 = x1 * w1; x1w1.set_label(\u0026#34;x1w1\u0026#34;); Value x2w2 = x2 * w2; x2w2.set_label(\u0026#34;x2w2\u0026#34;); Value x1w1_x2w2 = x1w1 + x2w2; x1w1_x2w2.set_label(\u0026#34;x1w1_x2w2\u0026#34;); Value n = x1w1_x2w2 + b; n.set_label(\u0026#34;n\u0026#34;); // output w tanh Value o = n.tanh(); o.set_label(\u0026#34;o\u0026#34;); o.backward(); Graph gs; gs.draw(o, \u0026#34;file_name\u0026#34;); Which gives the following graph:\n","permalink":"https://bedirtapkan.com/projects/microgradcpp/","summary":"Github Link\nTools Used: C++, Graphviz\nTopics: Deep Learning, Machine Learning\nIntroduction MicroGradCpp is a C++ implementation of MicroGrad, a minimalistic deep learning library. It includes an API to calculate gradients, and to train neural networks in single neuron level. It is a very simple library, and it is very easy to use. It is a very good tool to learn how neural networks work, and how they are trained.","title":"MicroGradCpp"},{"content":" Tools Used: Unreal Engine, C++, Blender\nTopics: Unreal Engine, Game Development, Imperfect Information Games\nIntroduction Dark Hex Online is a multiplayer version of Dark Hex. It is a game where players can play against each other, or against the AI. The game is built on Unreal Engine 5, and is currently in development. The game is still in early stages.\nWe are hoping to release the game on Steam, and we are currently working on the Steam integration. We are also working on adding more features to the game, and improving the game.\nThe reason we are building this game is to make Dark Hex more accessible to the public. We want to make it easier for people to play the game, and to make it easier for people to train agents. We are also hoping to make it easier for people to develop new strategies, and to come up with new ideas. On top of this we want to present a platform where people can play Dark Hex, and have fun, and hopefully build a better community around the game.\nTodos and Progress Design the game; pieces and board. (Blender) Create a dynamic board system. Setup lighting and camera. Setup piece movement and effects. Setup ghost cell system. Setup the game logic. (Hex) Setup the game logic. (Dark Hex) Network setup / online multiplayer. UI design and implementation. Adding the trained agents. Adding AI arena and easily addable AI systems. Setup Steam integration. Setup ranking system / leaderboards. Intro and tutorial. Different game modes (Abrupt, Flash DH, Noisy DH, etc.) ","permalink":"https://bedirtapkan.com/projects/dark_hex_ue5/","summary":"Tools Used: Unreal Engine, C++, Blender\nTopics: Unreal Engine, Game Development, Imperfect Information Games\nIntroduction Dark Hex Online is a multiplayer version of Dark Hex. It is a game where players can play against each other, or against the AI. The game is built on Unreal Engine 5, and is currently in development. The game is still in early stages.\nWe are hoping to release the game on Steam, and we are currently working on the Steam integration.","title":"Dark Hex Online (UE5)"},{"content":"Github Link\nTools Used: Python, C++, Tensorflow, Open-Spiel, Pandas, Numpy, Matplotlib, PyGame, PyDot, Tkinter\nTopics: Reinforcement Learning, Game Theory, Imperfect Information Games, Deep Learning, Machine Learning\nIntroduction Dark Hex is an imperfect information version of the Hex. Dark Hex is a phantom game, where a player have a chance to play concecutively. Due to this property Dark Hex is an extremely huge game. Which makes it really hard to train agents, and develop algorithms for.\nIn this project I implemented certain tools to help generate new strategies, and come up with better players eventually. The project includes my thesis work, along side all the results and experiments I have done. We have the best known players implemented (that we developed) as well as the methods we used to train them.\nWe base the tools on DeepMind\u0026rsquo;s Open-Spiel library, most of the game specific functions are used from there. I am yet to finish documentation for the project, but I will be adding it soon.\nWe heavily rely on MCCFR and NFSP algorithms for the training phase.\nTools MCCFR implementation for training agents in Dark Hex. PolGen: A UI-based tool to generate complete policies for the agents. When we are creating new strategies/policies, we need to make sure we covered all the cases possible. PolGen helps us do that, and makes it even easier with extra features we added. Tree Generator: A tool to generate the game tree for two players. This is a crucial part of evaluating and understanding the agent behaviour. We can use this tool to generate the game tree for any given state, and see the probabilities of the agent choosing a certain action. SimPly/SimPly+: The algorithms we used to train our agents. The details of the algorithm can be found in the thesis. More tools are on the previous version. I am still cleaning up and organizing the code, and will be adding them soon. ","permalink":"https://bedirtapkan.com/projects/dark_hex/","summary":"Github Link\nTools Used: Python, C++, Tensorflow, Open-Spiel, Pandas, Numpy, Matplotlib, PyGame, PyDot, Tkinter\nTopics: Reinforcement Learning, Game Theory, Imperfect Information Games, Deep Learning, Machine Learning\nIntroduction Dark Hex is an imperfect information version of the Hex. Dark Hex is a phantom game, where a player have a chance to play concecutively. Due to this property Dark Hex is an extremely huge game. Which makes it really hard to train agents, and develop algorithms for.","title":"Dark Hex"},{"content":"Prerequisites Intro to Linear Methods Semi-Gradient Prediction Semi-Gradient SARSA What is continuous? Let\u0026rsquo;s first describe the main task we will be handling; continuity. Continuous problems are tasks that has no specific terminal state, therefor will go on forever. As simple as it sounds, it is not a piece of cake to tackle the issues it brings with itself. Some examples could be the stock-market, where there is no end and you keep getting data. Or as book suggests access-control queuing task (Example 10.2).\nI will follow a simple format so that we all can stay on the same page and everything is clear cut:\nWhy is discounting not applicable for continuing tasks? The Remedy: Average Reward Logic behind it Why is it true: Math Differential Semi-Gradient SARSA So let\u0026rsquo;s start.\nWhy is discounting not applicable for continuing tasks? First of all, we should know that discounting works well for tabular cases. The issue we will be talking about rises when we start to use approximations.\nWe have a sequence of episodes that has no beginning or end, and no way to clearly distinguish them. As the book suggests, we have the feature vectors to maybe have a use of, but then the issue of clearly seperable arises. We might have two feature vectors that has no to little difference between them, which won\u0026rsquo;t be possible to be able to distinguish.\nSince we have no start point or end point, and since there is no clear line in between episodes, using discounting is not possible. Well it actually is possible. But it is not needed. Actually using $\\gamma = 0$ will give the same results as any other one. That\u0026rsquo;s because the discounted rewards are proportional to average reward. That\u0026rsquo;s why instead we will only use average reward. Here I will put the proof that both will results in the same order (discounted and without discounting):\nThe main issue with discounting in the approximation cases is that, since we have states depending on the same features, we do not have the policy improvement theorem anymore. Which was stating that we can get the optimal policy, just by changing all the action selections to the optimal ones for each state. Since we could choose the probabilities for one state without effecting the others it was pretty easy to handle. Now that we lost that property there is no guaranteed improvement over policy.\nAs Rich puts it \u0026ldquo;This is an area with multiple open theoretical questions\u0026rdquo;. If you are interested.\nThe Remedy: Average Reward Average reward is a pretty popular technique used in dynamic programming. Later on included into the Reinforcement Learning setting. We use average reward for approximated continual setting as we discussed above. Without discounting means that we care about each reward equally without thinking of if it occurs in far future etc.\nWe denote it as $r(\\pi)$. Not much detail but for the intuition part I will give the main definition for it: $$ r(\\pi) \\doteq \\sum_{s}\\mu_\\pi\\sum_{a}\\pi(a|s)\\sum_{r, s\u0026rsquo;}p(r, s\u0026rsquo;|s, a) r $$ Basically we consider the best policy as the policy which has the most $r(\\pi)$. For average reward we define returns as the difference between the $r(\\pi)$ and the reward received at that point, this is called the differential return: $$ G_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + \\ldots $$ I believe differential return holds almost all the properties normal returns had. Only change we will do is to replace the reward with the difference i.e. $R_{t+1} - r(\\pi)$. This goes for TD errors, bellman equations etc.\nQuick Math So we already saw the formula for $r(\\pi)$ but we didn\u0026rsquo;t actually see how it came to existence or what all those things mean. $$ r(\\pi) \\doteq \\lim_{h\\rightarrow\\infty} \\frac{1}{h} \\sum_{t=1}^{h}\\mathbb{E}[R_t|S_0, A_{0:t-1} \\sim \\pi] $$ Let\u0026rsquo;s explain what\u0026rsquo;s happening here. We are assuming we have $h$ number of rewards, we are summing expected value of all the rewards given the first state and the action trajectory following the policy $\\pi$, and we are dividing it to $h$ to get to the average of these rewards. So we simply had $h$ many rewards and we got the average. Then; $$ = \\lim_{t\\rightarrow\\infty} \\mathbb{E}[R_t|S_0, A_{0:t-1} \\sim \\pi] $$ Since I have the expectation inside the summation, we can actually simplify the summation with the division. We do have to put $t\\rightarrow\\infty$ to ccorrect the formula, as we will have number of samples approaching infinity. Next jump on the book seems fuzzy, but when you open it up it is extremely easy to see how it happens.\nSo if we have a randomness over something, what we want to do is to get the expectation of it. If we get the expectation that means we can formulate it, therefor no more randomness. In an MDP we have three kind of randomness possibly can happen.\nStates are random Actions are random Dynamics are random What does this mean? It means we can be in a state, and we don\u0026rsquo;t know what state that might be, and from there we will take an action, but we don\u0026rsquo;t know for sure which action will that be. And the last one is that we take that action but since we don\u0026rsquo;t know the dynamics of the environment (if stochastic even if we do know) we don\u0026rsquo;t know which state we will end up in. So actually this formula goes like; $$ \\mathbb{E}[\\mathbb{E} [ \\mathbb{E}[R_t|S_t, A_t]]] $$ Where the inner most is for the states and in the middle its the actions, the last one is the dynamics. So we know from bellman equations how to write this down; $$ \\mathbb{E}[R_t] = \\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s, a) r $$ This is the expected reward is it not ? Now lets add the action selection on top: $$ \\mathbb{E}[R_t|A_t] = \\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s, a) r $$ One last thing left is the state selection. We are using $\\mu_\\pi(s)$ to specify state distribution given the state (which the book covered earlier - Chapter 9). So the last piece of the puzzle; $$ \\mathbb{E}[R_t|A_t, S_t] = \\sum_{s}\\mu_\\pi(s)\\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s, a) r $$ That\u0026rsquo;s all, we therefor have the average reward formula covered.\nIn practice we will be using moving mean to calculate average reward.\nDifferential Semi-Gradient SARSA Well, I don\u0026rsquo;t really have much to add. If you read the Semi-Gradient SARSA post, this is mostly just changing the update rule for the continuous setting. That will be the change for $G_{t:t+n}$.\n$$G_{t:t+n}=R_{t+1}-\\bar{R}_{t+1}$$\n$$+R_{t+2}-\\bar{R}_{t+2}$$\n$$+\\ldots+ R_{t+n}-\\bar{R}_{t+n}$$\n$$+\\hat{q}(S_{t+n},A_{t+n},w_{t+n-1})$$\nThe TD error then will be like:\n$$ \\delta_t = G_{t:t+n} - \\hat{q}(S_t, A_t, w) $$\nand we will use another step size parameter $\\beta$ to update the average reward value. Here is the pseudocode:\nAnd here is my implementation of it, which does not require much explanation I assume:\n1 2 3 4 5 6 7 8 9 10 11 12 13 def update(self, observations, actions, rewards): if len(observations) \u0026gt; self.n: observations.pop(0) rewards.pop(0) actions.pop(0) if len(rewards) == self.n: G = sum([(r - self.avg_rew) for r in rewards[:-1]]) G += self._q_hat(observations[-1], actions[-1]) delta = G - self._q_hat(observations[0], actions[0]) self.avg_rew += self.beta * delta self.w += self.alpha * delta * self._grad_q_hat(observations[0], actions[0]) It is basically almost the same with the previous version. We are first checking if we have more elements than $n$ which means we need to remove the first elements from the storage. Then we have a check which sees if we have enough elements, because we won\u0026rsquo;t be making any updates if there is not at least $n$ elements in the trajectory. The rest is the same update as in the pseudocode.\nAgain we run an experiment using the same settings as before which results in a high varience learning, thought it does learn which is the point here right now 😄.\nI have a blog series on RL algorithms that you can check out. Also you can check BetterRL where I share raw python RL code for both environments and algorithms. Any comments are appreciated!\nFor full code\n","permalink":"https://bedirtapkan.com/posts/blog_posts/average_reward/","summary":"Prerequisites Intro to Linear Methods Semi-Gradient Prediction Semi-Gradient SARSA What is continuous? Let\u0026rsquo;s first describe the main task we will be handling; continuity. Continuous problems are tasks that has no specific terminal state, therefor will go on forever. As simple as it sounds, it is not a piece of cake to tackle the issues it brings with itself. Some examples could be the stock-market, where there is no end and you keep getting data.","title":"Average Reward, Continuing Tasks and Discounting"},{"content":"Prerequisites Semi-Gradient Prediction Intro to Linear Methods If you read the prediction part for the semi gradient methods, it is pretty easy to extend what we know to the control case. We know that control is almost all the time just adding policy improvement over the prediction case. That\u0026rsquo;s exactly the case for us here for semi-gradient control methods as well.\nWe already have describe and understood a formula back in prediction part (if you read it somewhere else that\u0026rsquo;s also fine), and now we want to extend our window a little.\nFor prediction we were using $S_t \\mapsto U_t$ examples, now since we have action-values instead of state-values (because we will pick the best action possible), we will use examples of form $S_t, A_t \\mapsto U_t$ meaning that instead of $v_\\pi(S_t)$ we will be using estimations for $q_\\pi(S_t, A_t)$.\nSo our general update rule would be (following from the formula for prediction);\n$$ w_{t+1} = w_t + \\alpha [U_t - \\hat{q}(S_t, A_t, w_t)] \\nabla\\hat{q}(S_t, A_t, w_t) $$\nAs we always do, you can replace $U_t$ with any approximation method you want, so it could have been a Monte Carlo method (Though I believe this does not count as semi-gradient, because it will be a direct stochastic gradient since it does not use any bootstrapping, but the book says otherwise so I am just passing the information 😄). Therefor we can implement an $n$-step episodic SARSA with an infinite option, which will correspond to Monte-Carlo (We will learn a better method to do this in future posts).\nThe last piece of information to add is the policy improvement part, since we are doing control, we need to update our policy and make it better as we go of course. Which won\u0026rsquo;t be hard cause we will just be using a soft approximation method, I will use the classic $\\epsilon$-greedy policy.\nOne more thing to note, which I think is pretty important, for continuous action spaces, or large discrete action spaces methods for the control part is still not clear. Meaning we don\u0026rsquo;t know what is the best way to approach yet. That is if you think of a large choices of actions, there is no good way to apply a soft approximation technique for the action selection as you can imagine.\nFor the implementation, as usual we will just go linear, as it is the best way to grasp every piece of information. But first I will as usual give the pseudo-code given in the book.\nI only took the pseudocode from chapter 10.2 because we don\u0026rsquo;t really the one before, as it is only the one step version. We are interested in the general version therefor n-step.\nImplementation 1 2 3 4 5 6 7 8 9 10 11 12 def __init__(self, feature_space, action_space, alpha = 0.0001, gamma = 0.99, eps = .1): self.alpha = alpha self.gamma = gamma self.eps = eps self.feature_space = feature_space self.action_space = action_space self.reset_weights() def reset_weights(self): self.w = np.random.rand(self.feature_space * self.action_space) Initialize We start by initializing the necessary things; we need step size $\\alpha$ also $\\gamma$ and $\\epsilon$. Other then these we need to initialize our weight vector. We will have a weight vector that is for each action concatenated after one another. So if we assume that we have 4 observations lets say [1 0 1 0], meaning weights 0 and 2 are active, and if want to update the weights for action 0, we will have [1 0 1 0 0 0 0 0 0 0 0 0] if we had 3 possible actions in total. After when we are using $\\epsilon$-greedy this will make more sense.\n1 2 3 4 5 def step(self, obs): if np.random.sample() \u0026gt; self.eps: return np.argmax(self._act(obs)) else: return np.random.randint(0, self.action_space) Let\u0026rsquo;s move next thing is to take a step, meaning we will pick the action according to our action-values at hand. We take the observations as input, this will come from the environment, and assuming we get an array of the probabilities for each action given the observations from _act(obs). Then all we have to do is to roll the die and decide if we will choose a random action or we will choose the action that has the most value for the current time, and thats exactly what we do here ($\\epsilon$-greedy action selection).\n1 2 3 4 5 def _act(self, obs): q_vals = np.zeros(self.action_space) for a in range(self.action_space): q_vals[a] = self.q_hat(obs, a) return q_vals Best $\\hat{q}$-value now we need to fill the function _act(obs). Which basically will call $\\hat{q}(s, a, w)$ for each action and store them in an array and return it.\n1 2 def q_hat(self, obs, action): return self.w.T.dot(self._x(obs, action)) Continuing from there we have the $\\hat{q}(s,a,w)$ to implement. Which is just writing down the linear formula since we are implementing it linearly. Therefor $\\hat{q}(s,a,w) = w^Tx(s, a)$ where $x(s,a)$ is the state action representation. In our case as I already mention this will just be the one hot vector, all the observations are added after one another for each action.\n1 2 3 4 def _x(self, obs, action): one_hot = np.zeros_like(self.w) one_hot[action * self.feature_space:(action+1) * self.feature_space] = obs return one_hot Finally $x(s, a)$ - as I already mentioned twice 😄 we create the $x$ in a vector that everything 0 other than the active action.\nThat was the last thing for us to be able to choose the action for a given state. So let\u0026rsquo;s have a broader respective and assume that we are using the step(obs) here is how it would be like:\n1 2 action = agent.step(obs) obs, reward, done = env.step(action) Now we see what is left ? Update\u0026hellip; 🤦‍♂️ Yeah without update there is no change basically. Which will also be the one differs for the $n$. Let\u0026rsquo;s remember the formula;\n$$ w_{t+1} = w_t + \\alpha[R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^n\\hat{q}(S_{t+n},A_{t+n},w_{t}) - \\hat{q}(S_{t},A_{t},w_{t})] \\nabla\\hat{q}(S_t, A_t, w_t) $$\n1 2 3 4 5 6 7 8 9 10 11 def update(self, observations, actions, rewards): if len(observations) \u0026gt; self.n+1: observations.pop(0) rewards.pop(0) actions.pop(0) if len(rewards) == self.n+1: G = sum([(self.gamma ** t) * r for t,r in enumerate(rewards[:-1])]) G += (self.gamma ** (self.n)) * self._q_hat(observations[-1], actions[-1]) self.w += self.alpha * (G - self._q_hat(observations[0], actions[0])) * \\ self._grad_q_hat(observations[0], actions[0]) There is a bit of a change here, from the pseudocode I provided. Since we want a full seperation between the agent-environment-experiment we need a class system for the algorithms therefor we won\u0026rsquo;t be following what is on the pseudocode.\nUpdate what happens here is actually not that different, since we only need $n+1$ elements to make the update happen we won\u0026rsquo;t keep the rest of the trajectory. Whenever we use n numbered trajectory the first element becomes useless for the next update. Therefor we remove the first element from the trajectory and use the rest to make our update.\nTerminal we also have a terminal state, and as can be seen in the pseudocode there are some differences that should be changed for the updates when we reach the terminal state. Logical enough, we do not have n+1 element left to complete the calculation we were doing therefor we will just use the rewards rather than $\\hat{q}(s,a,w)$ . Therefor we need another function to handle this, which we call end() in our structure;\n1 2 3 4 5 6 7 8 9 def end(self, observations, actions, rewards): for _ in range(self.n): observations.pop(0) rewards.pop(0) actions.pop(0) G = sum([(self.gamma ** t) * r for t,r in enumerate(rewards)]) self.w += self.alpha * (G - self._q_hat(observations[0], actions[0])) * \\ self._grad_q_hat(observations[0], actions[0]) Here as we can see we are not doing something too different. It is just that we are using the last elements we have left and we will remove all the elements from the trajectory while making the last updates to our weights.\nYeah and we are almost done, exept that I didn\u0026rsquo;t show the grad_q_hat() yet, which basically gives the $\\nabla\\hat{q}(s,a,w)$.\n1 2 def grad_q_hat(self, obs, action): return self._x(obs, action) Surprise.. Yeah since we are using linear functions, $\\nabla w^Tx(s, a) = x(s,a)$. That\u0026rsquo;s all.\nLet\u0026rsquo;s see how would be the experiment part and run the code to get some results then.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 params = { \u0026#39;num_of_episodes\u0026#39; : 1000, \u0026#39;max_steps\u0026#39; : 1000, \u0026#39;alpha\u0026#39; : 2 ** (-14), \u0026#39;gamma\u0026#39; : 0.98, # Creating the tilings \u0026#39;grid_size\u0026#39; : 5, \u0026#39;tile_size\u0026#39; : 4, \u0026#39;num_of_tiles\u0026#39; : 5 } # environment env = grid_world(portal=True) action_space = env.action_space.shape[0] # tile coding tilings = tile_coding(env.grid_size[0], params[\u0026#39;num_of_tiles\u0026#39;], params[\u0026#39;tile_size\u0026#39;], action_space) state_space = tilings.num_of_tilings # Keep stats for final print and data episode_rewards = np.zeros(params[\u0026#39;num_of_episodes\u0026#39;]) # Agent created n = 8 agent = SG_SARSA(state_space, action_space, n, params[\u0026#39;alpha\u0026#39;], params[\u0026#39;gamma\u0026#39;]) np.random.seed(1) for ep in range(params[\u0026#39;num_of_episodes\u0026#39;]): rewards = [] observations = [] actions = [] obs = tilings.active_tiles(env.reset()) # a x d score = 0 for t in range(params[\u0026#39;max_steps\u0026#39;]): action = agent.step(obs) observations.append(obs) obs, reward, done = env.step(action) obs = tilings.active_tiles(obs) rewards.append(reward) actions.append(action) score += reward if done: agent.end(observations, actions, rewards) break else: agent.update(observations, actions, rewards) episode_rewards[ep] = score print(\u0026#34;EP: {} -------- Return: {} \u0026#34;.format(ep, score), end=\u0026#34;\\r\u0026#34;, flush=True) I used tile coding and the grid world environment in our library. If you want you can modify a little to use another state representation or Rich Sutton\u0026rsquo;s tile coding library, or for environment gym.\nAnyways, what we do is pretty simple if you read through, and you can ask for clarification on any point if looks weird.\nMain point here are the agent functions and how we use them, all three are used as we said, on each step we have the agent.step(), for each step we have the update() called except the terminal state. Which we will call end() instead.\nI will give only one graph as result as usual, here is 100 runs on the stochastic grid world environment.\nIf you liked this post follow BetterRL, and keep a like down below. I have a blog series on RL algorithms that you can check out. Also you can check the repo where I share raw python RL code for both environments and algorithms. Any comments are appreciated!\nFor full code\n","permalink":"https://bedirtapkan.com/posts/blog_posts/semi_gradient_control/","summary":"Prerequisites Semi-Gradient Prediction Intro to Linear Methods If you read the prediction part for the semi gradient methods, it is pretty easy to extend what we know to the control case. We know that control is almost all the time just adding policy improvement over the prediction case. That\u0026rsquo;s exactly the case for us here for semi-gradient control methods as well.\nWe already have describe and understood a formula back in prediction part (if you read it somewhere else that\u0026rsquo;s also fine), and now we want to extend our window a little.","title":"Semi-Gradient Control Methods"},{"content":" Tools Used: Python, PyTorch, OpenAI Gym\nTopics: Reinforcement Learning, Deep Learning, Neural Networks, Machine Learning\nIntroduction Co-Agent Networks is an actor-critic framework that utilizes some set of interactive nodes. We took this idea under investigation and developed multiple supervised and unsupervised settings that utilizes the nodes as actor-critic agents. The idea of Co-Agent systems is introduced by Philips S. Thomas, and is still being investigated.\nFeatures We have developed a fully working framework that utilizes the nodes as actor-critic agents. We have completed multiple experiments and reported the results for the future researchers.\nRead more about Co-MDP and Co-Agent Networks here:\nConjugate Markov Decision Processes Policy Gradient Coagent Networks Asynchronous Coagent Networks To learn more about our implementation and results check out my blog post titled Co-agent Networks, Neural Network that think.\n","permalink":"https://bedirtapkan.com/projects/co_agent_networks/","summary":"Tools Used: Python, PyTorch, OpenAI Gym\nTopics: Reinforcement Learning, Deep Learning, Neural Networks, Machine Learning\nIntroduction Co-Agent Networks is an actor-critic framework that utilizes some set of interactive nodes. We took this idea under investigation and developed multiple supervised and unsupervised settings that utilizes the nodes as actor-critic agents. The idea of Co-Agent systems is introduced by Philips S. Thomas, and is still being investigated.\nFeatures We have developed a fully working framework that utilizes the nodes as actor-critic agents.","title":"Co-Agent Networks"},{"content":"Tools Used: Python, PyTorch, Numpy, Pandas, JavaScript, Scikit-learn, Flask\nTopics: Machine Learning, Deep Learning, Time-Series, Data Visualization, Data Processing\nProblem The International Space Station (ISS) is a space station, or a habitable artificial satellite, in low Earth orbit. Given that it is in space, with no gravity, air circulation is very hard to manage. Due to lack of circulation any CO2 that is produced by the astronauts is not dispersed, and accumulates in the same area. This can be a problem, as the CO2 levels can reach a dangerous level, and can cause health issues for the astronauts. Especially when they are sleeping, or doing other activities that require them to be in a specific area for a long time. Lack of oxygen first causes dizziness, and headaches, and if the levels are high enough, it can cause unconsciousness, and even death.\nSolution Astranouts have a very strict schedule, and they are always doing something based on that schedule. Some of the activities requires heavy physical activity, and some of them are more relaxed. The CO2 levels are higher during the physical activities, and lower during the relaxed activities. We can use the schedule information to see which activities are being performed at any time that the CO2 levels are high. This way we can detect the CO2 clusters, and see which activities are causing them. We can then use this information to make the schedule more efficient, and reduce the CO2 levels.\nData The data is collected from the ISS, and it is a time-series data. The data is collected every 5 minutes, and it contains the CO2 levels in localized areas from the sensors placed on the ISS, and for certain times on the astronauts themselves. The sensors that were placed in the room are less effective since the CO2\u0026rsquo;s are packed in small areas, and the sensors are not able to detect them. The sensors that are placed on the astronauts are more effective, since they are able to detect the CO2\u0026rsquo;s that are being produced by the astronauts.\nThe data collected proved not to be quite enough and that further collection plans with new systems was needed. (This is a part of this project).\nUnfortunately, the data is not public, even I have not seen the full data due to restrictions by NASA.\nSolution The solution has multiple stages as we have multiple problems to solve. The first stage is to detect the CO2 clusters, and see which activities are causing them. The second stage is to collect more data by providing better tools, and improve the model. The third stage is to use the detected clusters to make the schedule more efficient, and reduce the CO2 levels.\nStage 1 The first stage was to detect the CO2 clusters, and see which activities are causing them. We first adjusted the datasets to match each other and be usable in sync, since we had multiple sensors, from different rooms, and multiple astranouts with different schedules it was quite a challange to arrange the dataset properly.\nAfter arranging the dataset we looked for a suitable model to predict the CO2 levels based on activities performed. We tried a few different models, including linear ML models such as ARIMA, but ended up using a custom LSTM model. After long testing sessions we have concluded that the data was not enough to train a better model, and we needed to collect more data. In the end we achieved near 75% accuracy on the test set, which is not bad, but not good enough for our purposes. So we decided to move on to the next stage, and leave the further model improvements for after we collect more data.\nStage 2 The second stage was to collect more data. We decided to build an easy to use offline tool (since internet is not an option) that is easily installable to ISS computers, and can be used to collect data. So we decided to build a web application that can be used to visualize the data, and make it easier to understand for the astranouts. The tool is written in Python, and uses the Flask framework. For the visualization we used plotly and javascript and made it easy to use for the astranouts. The tool collects data from the sensors, and the astranouts annotate where needed. The data is then saved in a database, and can be used for further analysis.\nWe sent the tool to ISS, and they started using it. Since the wait times are long to send anything to ISS, we delivered the data collection tools along with the models for the further studies. Another team will continue the work for the third stage once they have collected enough data.\nHere is the poster we have presented at the NASA Wearable Technologies\n","permalink":"https://bedirtapkan.com/projects/iss_co2/iss_co2/","summary":"Tools Used: Python, PyTorch, Numpy, Pandas, JavaScript, Scikit-learn, Flask\nTopics: Machine Learning, Deep Learning, Time-Series, Data Visualization, Data Processing\nProblem The International Space Station (ISS) is a space station, or a habitable artificial satellite, in low Earth orbit. Given that it is in space, with no gravity, air circulation is very hard to manage. Due to lack of circulation any CO2 that is produced by the astronauts is not dispersed, and accumulates in the same area.","title":"CO2 clustring detection on ISS"},{"content":"Github Link\nTools Used: Python, Tflearn, Numpy, Pandas, BeautifulSoup, Tweepy\nTopics: Machine Learning, Deep Learning, Natural Language Processing, Twitter Bot, Web Scraping, Recurrent Neural Networks\nIntroduction Cakma Sair is a twitter bot that generates turkish poems using NLP. It uses a dataset of 1000+ turkish poems we scraped from multiple turkish poetry websites. We think that poetry is a very important part of turkish culture, and we wanted to create a bot that can generate turkish poems.\nWe are using Recurrent Neural Networks (RNN) with word2vec to train the model. We then use tweepy to tweet the generated poems based on people\u0026rsquo;s inputs.\nTools CakmaSair: The main tool that generates the poems. It is a python script that uses a trained model to generate poems. It uses tweepy to tweet the generated poems. ScrapePoems: A tool that scrapes turkish poems from multiple turkish poetry websites. It uses BeautifulSoup to scrape the poems and save them to a txt file. Here is a sample of the generated poems:\nGo ahead and tweet at @CakmaSair to generate a poem!\n","permalink":"https://bedirtapkan.com/projects/cakmasair/","summary":"Github Link\nTools Used: Python, Tflearn, Numpy, Pandas, BeautifulSoup, Tweepy\nTopics: Machine Learning, Deep Learning, Natural Language Processing, Twitter Bot, Web Scraping, Recurrent Neural Networks\nIntroduction Cakma Sair is a twitter bot that generates turkish poems using NLP. It uses a dataset of 1000+ turkish poems we scraped from multiple turkish poetry websites. We think that poetry is a very important part of turkish culture, and we wanted to create a bot that can generate turkish poems.","title":"Cakma Sair"},{"content":"Counting sort is a nice in-place sorting algorithm that we can use for sorting instantly. (This is mostly used for competitive programming.) What I meant by this is, we can use counting sort when we are getting the input. This will not get any additional cost for us, and really good technique for using in-place sorting. I\u0026rsquo;ll get there after explaining the algorithm.\nComplexity ? Let me tell you the complexity if you are wondering, but I will explain \u0026ldquo;why\u0026rdquo; after the algorithm itself. O(n+k) : n , is size of the array that we will sort and k is the maximum element we have.\nAlgorithm OK. Let\u0026rsquo;s think of an array. For the sake of simplicity, let\u0026rsquo;s make it short\u0026hellip;\nmyArray = [2, 3, 7, 4, 3, 9] We have an array -unsorted- , minimum number is 2, maximum is 9 and we have 6 elements in the array. Alright so let\u0026rsquo;s think of one more array. Which is from 0 (array starting point) to our maximum number (9) and all the values are initially 0.\nINDEXES 0 1 2 3 4 5 6 7 8 9 weAreCounting = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ATTENTION! We are going to iterate over first array one by one, and we will increment the values that we have (i.e. for first step it is myArray[0] which is 2 so go to weAreCounting array and increment [2] by 1):\nfor i in myArray: weAreCounting[i] += 1 NOW WHAT WE HAVE AT THE END OF THIS LOOP?\nINDEXES 0 1 2 3 4 5 6 7 8 9 weAreCounting = [0, 0, 1, 2, 1, 0, 0, 1, 0, 1] So weAreCounting array basically shows us how many of these numbers we do have (i.e. we have 2 threes so weAreCounting[3] = 2).\nThis part is how you have all the items counted. Now we can use this as sorted array (with iterating over it) or we can have our new array that will have the sorted array directly. For the second version:\nMy logic will be to use same array before (myArray) so that it will be more efficient (space-wise).\nSo I iterate thorough weAreCounting and if the number is bigger than 0 I will add it into myArray. That is all of the logic.\nCode Here is the c++ code, as simplified as possible. ENJOY!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void countingSort (int * arr) { int countingArray[MAX_NUM] = {0}; for (i = 0 ; i \u0026lt; ARRAY_SIZE ; i++) countingArray[arr[i]]++; int output_Index = 0; for (i = 0 ; i \u0026lt; MAX_NUM ; i++) while ( countingArray[i]-- ) // Process will continue until the elements reach to 0 arr[output_Index++] = i; // PS: Incrementing will be after the line_process // Instead of these two lines we could use memset function too... } And here is python3.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def countingSort(arr): k = max(arr) countingArray = [0] * (k + 1) for i in arr: countingArray[i] += 1 j = 0 for i in range(k+1): while(countingArray[i] \u0026gt; 0): arr[j] = i countingArray[i] -= 1 j += 1 return arr a = countingSort([3, 4 ,5 ,1, 10, 3, 2]) If you check the complexity in the code, you will see that we have two loops, one is doing n operation (going through the array that we will sort). Second loop is doing k operation (which is the maximum number that we have in the array that we will sort). So time complexity will be O(n+k).\nThe space complexity: we have two arrays (we could have 3 but we decrease it to 3 because we used the one at the beginig two times. Since at the end we will not going to need that one.) one is size of n and one is size of k so our complexity will be O(n+k).\nNow I suggest you to go to the link below, and try to solve the questions in Week 3/ Counting Sort section. So that you will have full understanding about this question. If you like the concept of it you can star or watch out repository as well. Have a great one!\nYou can also find other algorithms explained and full code samples about this one here (Check Week 3)\n","permalink":"https://bedirtapkan.com/posts/blog_posts/counting_sort/","summary":"\u003cp\u003eCounting sort is a nice in-place sorting algorithm that we can use for sorting instantly. (This is mostly used for competitive programming.) What I meant by this is, we can use counting sort when we are getting the input. This will not get any additional cost for us, and really good technique for using in-place sorting. I\u0026rsquo;ll get there after explaining the algorithm.\u003c/p\u003e","title":"Counting Sort"},{"content":"Before starting- What is Prime Factorization ? What is a Prime number ? If you are curious about these please checkout this link before proceed because I will not explain them here :)\nWikipedia - Prime Numbers\nSince we all know what a prime number and composite number is, let\u0026rsquo;s look at our realllly simple algorithm. Actually there is nothing fancy here, we are just using simple Sieve of Eratoshenes(Hardest name to pronounce, I checked online if I am right) algorithm. By the way that topic also pre-requised for this post, but fortunetely we already have a tutorial-explanation for it. If you don\u0026rsquo;t know or confused about it in some ways please check the links below:\nSome sources to learn about Sieve of Eretosthenes\nSince \u0026ldquo;WE\u0026rdquo; covered everything required, let me involve in this learning process too\u0026hellip; Prime factorization: This is highly important topic. All your passwords , your bank accounts and stuff are protected by these numbers. Anyway that is why we actually have couple algorithms about Prime Factorization. There is a good answer on quora.com about the prime number algorithms:\nDifferent algorithms get used based on how large the number is. It goes something like this:\nSmall Numbers : Use simple sieve algorithms to create list of primes and do plain factorization. Works blazingly fast for small numbers.\nBig Numbers : Use Pollard\u0026rsquo;s rho algorithm, Shanks\u0026rsquo; square forms factorization (Thanks to Dana Jacobsen for the pointer)\nLess Than 10^25 : Use Lenstra elliptic curve factorization\nLess Than 10^100 : Use Quadratic sieve\nMore Than 10^100 : Use General number field sieve\nCurrently, in the very large integer factorization arena, GNFS is the leader. It was the winner of the RSA factoring challenge of the 232 digit number\nArun Iyer quora.com\nOK cool, we have a lot of options, although you can see that these numbers are gigantic. $10^{25}$ ?? This was the smallest one mentioned above by the way. So we don\u0026rsquo;t really care about them, they are exist because like I said before, these numbers are extremely powerful so people need biiig ones. Since our languages supports (for C++) until $10^{19}$ , and our tutorials are for ACM-ICPC kind programming contests, considering that these contests have time limit and %100 sure that if $10^{25}$ will given\u0026hellip; you probably should search for some trick in question, because we cannot compete that many operations on time.\nFinally the Algorithm Anyway after all explanation lets talk about our \u0026ldquo;small\u0026rdquo; algorithm. It really is nothing much than using Sieve algorithm. We are just going to optimize it a little bit. Let\u0026rsquo;s say we already runned our sieve function:\n1 sieve(10001); Now we have an array or vector , I don\u0026rsquo;t know how you implemented so I will go with mine -\u0026gt; you can check it out:\n1 2 3 4 5 6 7 8 9 10 11 12 vector\u0026lt;int\u0026gt; primes; void sieve(int size) { bitset\u0026lt;10000010\u0026gt; was; // You can also use boolean array or vector, but this is optimized for bool (C++ is best :) ) was.set(); // Initilizing all bitset to true was[0] = was[1] = 0;\t// Except 0 and 1 of course for (int i = 2; i \u0026lt;= size; i++) if (was[i]) { primes.push_back(i); for (int j = i * i; j \u0026lt;= size; j += i) was[j] = 0; } } We have a vector named primes and it has all the primes from begining(2) to size.\n$$primes -\u0026gt; [ 2 , 3 , 5 , 7 , 11 , 13 , 19 , 21 , \u0026hellip; ]$$\nWhat will we do is we will use basic logic and check every prime number and if it can divide our number N. If it can divide , we will just put it into our new vector (If you don\u0026rsquo;t know vector you still can use list or array, depends on the language). If we can divide we will divide it, with this way we will decrement our operations. So let\u0026rsquo;s say we have 18 as our N. We start with first element in the primes which is 2.\nIs 2 dividing N = 18 ? Yes obviously so:\nPut 2 into our Factors vector; So -\u0026gt;\n$$Factors -\u0026gt; [ 2 ]$$\nAnd we will divide our N by 2:\nN = 18/2 = 9 Continue to check if 2 is dividing N which is not becuese N = 9. So lets pass 2 and go to 3:\nIs 3 dividing N = 9 ? Yes Put 3 into our Factors vector; $$Factors -\u0026gt; [ 2 , 3 ]$$\nN = 9/3 = 3 Is 3 dividing N = 3 ? Yes Put 3 into our Factors vector; $$Factors -\u0026gt; [ 2 , 3 , 3 ]$$\nN = 3/3 = 1 Is 3 dividing N = 1 ? Nope Proceed to 5. 5 ? Yes we will stop here. This is the next optimization, at most we will go until $p^2 \\leq N$ (and p is my prime number that I am checking). This is what determines my complexity in this method. So I have $\\sqrt{N}$ here. This is also my number that will go into O notation -\u0026gt; O($\\sqrt{N}$). (Mathematically this complexity is represented with $O(\\pi(\\sqrt{N})) = O(\\sqrt{N}\\times lnN)$)You can further check the code C++ implementation. I commented it so you can see what is going on in each step. After understanding the code I highly recommend you to solve questions about this topic, we have our list for this question as well, check the link at the bottom.\nImplementation C++ 1 2 3 4 5 6 7 8 9 10 11 12 13 vector\u0026lt;int\u0026gt; primeFactors(int N){ vector\u0026lt;int\u0026gt; vc; // An empty vector for us to fill with our numbers factors. int idx = 0, f = primes[idx]; // f standing for FACTOR - idx is index that we will increment while(N != 1 \u0026amp;\u0026amp; N \u0026gt;= f * f){ // f * f ... This is the part with sqrt(N) so the loop continues until our factor is bigger than sqrt(N) while(N % f == 0){ // I will continuously check if N is divisible by this prime, until it become wrong. N /= f; // Dividing N to my prime. vc.push_back(f); // adding that prime to my vector. } } if(N != 1) vc.push_back(N); // This case is for prime numbers itself, if the number is prime than we should add it to our vector. // If some value, after our loop is still not equals to 1 than it is a prime itself. (because of sqrt(N)) return vc; } We have a well designed Curriculum on Github, also the questions about this algorithm are there too, check it out here\nACM-ICPC Curriculum\n","permalink":"https://bedirtapkan.com/posts/blog_posts/prime_factorization/","summary":"\u003cp\u003e\u003cstrong\u003eBefore starting-\u003c/strong\u003e What is Prime Factorization ? What is a Prime number ? If you are curious about these please checkout this link before proceed because I will not explain them here :)\u003c/p\u003e","title":"Factorizing a Number"},{"content":"What we will learn today is, how to find the , in optimal solution. First, let\u0026rsquo;s clarify the goal a bit.\nWhat is subarray? Subarray is an array that is included in the bigger array. So if we have an array that has 7 elements in it. What we have is elements that have indexes of: 0, 1, 2, 3, 4, 5, 6 . A subarray is smaller array inside of this big array. So for example 1, 2, 3 or 4, 5 are subarrays. But 1, 3 is not a subarray because the subarray should be contiguous. So our task is to find the largest contiguous array in our big array.\nSince we clarify our objective let\u0026rsquo;s look at the solutions we have. First let\u0026rsquo;s see what will be the brute force solution since that will be the first one which comes to mind. What we would do is, we would start from 0 index and hold it, check every elements before that index and keep the largest one. So if we have [3, 5, 7, 9] in the array. We would first check the 3. We would see that it is the largest subarray since there is none other than that. And than we would check the index 1 -\u0026gt; 5, we have 5 and 5 + 3 = 8. The bigger one is 8 so we keep 8. Than we go for 2nd index -\u0026gt; 7. We have 7, 7+5, 7+5+3 so that biggest one will be 7+5+3 which is 15, we keep it. Then next one : index 3 -\u0026gt; 9. We have 9, 9+7, 9+7+5, 9+7+5+3. Largest one will be 9+7+5+3 = 24. So we compare the ones we found as sum of subarrays and the greatest one will be 24, the last one we checked, that\u0026rsquo;s because we have no negative elements in the array. Anyways that would be the brute force solution and still a smart one. But the time complexity would be O(n^2). Since we take the index and check every others that we can combine with this index. Let me visualize this one:\nOK, we got this part. So we got the question, now what is the optimal solution for this problem. What is this guy , Kadane , found. Here is the algorithm then. This algorithm is dynamic, which means we will approach the result using the ones we find before. OK, this guy teaches us a way that has complexity O(n), linear time.\nLet\u0026rsquo;s go with an example so it will be more clear. Our array is [5, -2, -4, 4, 4]. Kadane says that in each iteration we have only two options to get the max subarray:\nIt can be only itself It can be itself combined with the maximum subarray that previous index has. Man, this is a smart solution. OK, what he says is let\u0026rsquo;s say we calculated the sum of maximum subarrays until index 1 which has value -2. For the sake of understanding let\u0026rsquo;s calculate with brute force. We have -2 and -2+5. The greater one is -2+5 = 3. So let\u0026rsquo;s proceed. This time let\u0026rsquo;s use Kadane\u0026rsquo;s Algorithm for calculating the 3rd step. What are the options:\nIt can be only itself (We have -4 as the 2nd indexed value so = -4) It can be itself combined with the maximum subarray that previous index has. (We have 3 as 1st index\u0026rsquo;s max Subarray sum, so = 3-4 = -1) So we have a -1 and -4 \u0026hellip; -1 indeed. But is it always the second option then? Let\u0026rsquo;s see with another part of our array. Let\u0026rsquo;s proceed one more step. Don\u0026rsquo;t forget that we have -1 as our current max. 3rd index -\u0026gt;\nIt can be only itself (We have 4 as the value = 4) It can be itself combined with the maximum subarray that previous index has. (We have -1 as the previous index\u0026rsquo;s maximum -\u0026gt; -1+4=3) Now we approached the first option, 4 \u0026gt; 3 so we will keep 4 instead of 3. And repeat this until the end \u0026hellip; Really that\u0026rsquo;s all.\nNow that we understand the logic. Let\u0026rsquo;s proceed to the code. I will give pseudocode here.\nkadane(Array){ generalMaximum = currentMaximum = Array[0] for (i = 1 until n) { currentMaximum = maximum of(Array[i], currentMaximum + Array[i]); if(currentMaximum \u0026gt;= generalMaximum) generalMaximum = currentMaximum; } return generalMaximum; } If you are interested on learning or practicing more algorithms, you can visit our curriculum from github ACM-ICPC Preparation. There are also questions and source code\u0026rsquo;s about this topic. ENJOY!\n","permalink":"https://bedirtapkan.com/posts/blog_posts/kadane/","summary":"\u003cp\u003eWhat we will learn today is, how to find the , in optimal solution. First, let\u0026rsquo;s clarify the goal a bit.\u003c/p\u003e","title":"Kadane's Algorithm"},{"content":"Reaaally introductory post for Machine Learning basics. What is Supervised and Unsupervised learning?\nSupervised Learning Supervised learning is the most common machine learning problem. In Supervised Learning we already know what the correct output should be like.\nThere are two categories for supervised learning, first one is \u0026ldquo;regression problem\u0026rdquo; and the second one is \u0026ldquo;classification problem\u0026rdquo;. I will explain both with examples.\nRegression Problems In regression we have an output that is continous and we are trying to predict what will be the correct answer/output/label for our input. Lets see an example to understand the concept better.\nExamples: Let\u0026rsquo;s say we have a friend who has a chocalate company. He has a lot of money and he wants to make his product sell as many as Snickers. OK. But his chocalates are not famous as Snickers. Now , what he should do is, take a look at the competitor. There is a chart which has two dimensions. One is the price. Another is the popularity. Now that since we have continous output for the prices. We will predict the one that we are looking for. (I will just give the popularities according to myself.)\nNow, looking at this output. What should we do is, putting a straight or polinomial line to the outputs.\nThen we will have our line that will help us to predict the price. According to the surveys, our chocalates have 8 point for the popularity. So what will be the best price according to the survey and the industry\u0026hellip;\nIt seems something like 70¢ \u0026hellip;\nThis is the regression problem \u0026hellip;\nClassification Problems In classification, the simplest one, binary classification, we have two options, either true or false. We also can say that we will predict the result in a binary map. Let\u0026rsquo;s check an example.\nExamples: Let\u0026rsquo;s give an absurd example so that it will be more permament. So we have a friend who just ate 5 kilos of Nutella and he is 24 years old. We want to predict if he will get sick or not. And we have a dataset that have people\u0026rsquo;s ages that ate 5 kilos of Nutella and got the sick or not !!\nSo according to this graph our friend will get sick or not. It is a binary example. There is just two probabalities. This is a classification problem. Let\u0026rsquo;s see the expected result \u0026hellip;\n(He will probably get sick, according to our prediction.)\nUnsupervised Learning The unsupervised learning is the second most common machine learning problem. In unsupervised learning we don\u0026rsquo;t know the result for each input. We will obtain a structure form the data. We do not know what are the exact effects of our inputs. We will use clustering for this.\nClustering Problem We basically will seperate the data according to variables.\nLet\u0026rsquo;s say you got hundred different composition classes\u0026rsquo; final essays. They all have different topics. What clustering do is, classifying all the essays according to their topics. So that if we use clustering, all these classes\u0026rsquo; articles will be separated. This is just one variable (topic). If you want, you can add more variables to make the groups more specific. In this case we can add words count for example.\n","permalink":"https://bedirtapkan.com/posts/blog_posts/ml_basics/","summary":"\u003cp\u003eReaaally introductory post for Machine Learning basics. What is Supervised and Unsupervised learning?\u003c/p\u003e","title":"Machine Learning Basics"},{"content":"\nHi! Let me talk a bit about myself, without boring you I hope. I am a Machine Learning engineer \u0026amp; researcher, currently focusing on Reinforcement Learning and Game Theory.\nProfessional Me Now I have recently finished my Masters degree at University of Alberta where I was supervised by Martin Mueller and Ryan Hayward. My thesis titled Dark Hex: A Large Scale Imperfect Information Game was on a zero-sum imperfect information game called Dark Hex. I have also worked on some open source projects including DeepMinds very own OpenSpiel. We are expecting multiple papers to be published soon!\nProfessional Me Before Let me mention my background a little more! I have started my programming career, and interest a year before my undergrad with mobile app development. I have learned Swift and iOS development and have completed multiple projects with multiple teams. After the first year I have shifted my interest on competetive programming. This marks also the time I got really involved with my community and got managing positions on my universities ACM branch. I have then participated ACM-ICPC competitions multiple times, and thought lectures at our ACM branch to people who were also interested in competitive programming. I have created an open source curriculum for studying competitive programming and coding questions in general that received a lot of interest from the community (ACM-ICPC Curriculum).\nLast 2 years of my undergrad was where I got really interested in Machine Learning. I have taken multiple courses on Machine Learning and Deep Learning, and have also taken a course on Reinforcement Learning. I got a gold medal on HackHouston hackathon with a successful machine learning project (MLRPro). With this new interest, I started looking for more opportunities to develop my skills and hopefully apply them on a real world project. NASA gave me an oppotunity at that moment, I have worked with a professional team, and brought a real-life working product under their tool belt.\nI applied for Masters degree at University of Alberta due to my extreme interest in Reinforcement Learning, and got accepted. During my wait I have got a short Computer Science teaching position.\nWhen I got to UofA, I took on a project with Martha White on Co-Agent Networks. After a while I discovered my interest on Imperfect Information Games, or Partially Observable Environments. I then started working on Dark Hex, and got supervised by Martin Mueller and Ryan Hayward. During this period I have learned a lot on Game Theory, and took Advanced Reinforcement Learning class from the legend himself, Richard Sutton.\nDuring these times I have had a lot of teaching experience. I TAed many times, I thought many classes voluntarily as well. I have thought Reinforcement Learning, Machine Learning, Python, C++ and many more.\nFor the past year, I took up Blender and learned 3D modelling and sculpting. For the past couple months I also have learned Unreal Engine, and have been working on it since.\nPersonal Me Other than my professional background: I love going outdoors; the fresh air and the quiet is just phenomenal. I love reading, especially philosophy, you can follow me on my goodreads. I enjoy baking, I am pretty sure my friends do too. I love playing soccer and volleyball when I get the chance. Time to time I play some Dota 2 or FIFA with some friends. I also did a lot of professional Graphic Designing during my undergrad. I still do it as a hobby, I am specialized on logo design and branding. I have a Behance account if you are interested.\nI am recently working on my youtube channels, one for my blender projects where I sculpt and model, and the other is for lecture series I have been developing on certain expertise I have. Currently three series are on the way: Introduction to Multi-Agent Reinforcement Learning, Imperfect Information Games, and Single Agent Reinforcement Learning.\n","permalink":"https://bedirtapkan.com/about/","summary":"Hi! Let me talk a bit about myself, without boring you I hope. I am a Machine Learning engineer \u0026amp; researcher, currently focusing on Reinforcement Learning and Game Theory.\nProfessional Me Now I have recently finished my Masters degree at University of Alberta where I was supervised by Martin Mueller and Ryan Hayward. My thesis titled Dark Hex: A Large Scale Imperfect Information Game was on a zero-sum imperfect information game called Dark Hex.","title":"About"},{"content":"Just got my HUION 22 Pro a couple weeks ago, had to jump in and make something interesting. Didn\u0026rsquo;t think much :) Just pen on the tablet!\nDidn\u0026rsquo;t spend much time texturing or adding fine details. Just a quick sculpting and some basic lighting. Still took me around an hour and half to get it done.\nEnjoy!\n","permalink":"https://bedirtapkan.com/design/not_a_beast/","summary":"Just got my HUION 22 Pro a couple weeks ago, had to jump in and make something interesting. Didn\u0026rsquo;t think much :) Just pen on the tablet!\nDidn\u0026rsquo;t spend much time texturing or adding fine details. Just a quick sculpting and some basic lighting. Still took me around an hour and half to get it done.\nEnjoy!","title":"I'm Not a Beast"},{"content":"","permalink":"https://bedirtapkan.com/tags/","summary":"","title":"Tags"}]