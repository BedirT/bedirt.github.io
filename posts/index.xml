<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Bedir Tapkan</title>
    <link>https://bedirtapkan.com/posts/</link>
    <description>Recent content in Posts on Bedir Tapkan</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Â© 2022</copyright>
    <lastBuildDate>Tue, 01 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://bedirtapkan.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sentiment Analysis A to B: Episode 1</title>
      <link>https://bedirtapkan.com/posts/blog_posts/sentiment_ab_1/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/sentiment_ab_1/</guid>
      <description>Sentiment Analysis Experiments, Data Prep, Representations and Logistic Regression</description>
      <content:encoded><![CDATA[<h1 id="sentiment-analysis-a-to-b-episode-1">Sentiment Analysis A to B: Episode 1</h1>
<p>In this series, I will work my way into different Sentiment Analysis methods and experiment with other techniques. I will use the data from the <a href="https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis">IMDB review dataset</a> acquired from Kaggle. The series is called A to B since I need to cover all the methods and the best, for that matter. I am covering some I find exciting and test-worthy.</p>
<p>In this episode, I will be examining/going over the following:</p>
<ul>
<li>Data preprocessing for sentiment analysis</li>
<li>2 different feature representations:
<ul>
<li>Sparse vector representation</li>
<li>Word frequency counts</li>
</ul>
</li>
<li>Comparison using logistic regression</li>
</ul>
<h2 id="feature-representation">Feature Representation</h2>
<p>Your model will be, at most, as good as your data, and your data will be only as good as you understand them to be, hence the features. I want to see the most useless or naive approaches and agile methods and benchmark them for both measures of prediction success and for training and prediction time.</p>
<p>Before anything else, let&rsquo;s load, organize and clean our data really quick:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-1"> 1</a>
</span><span class="lnt" id="hl-0-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-2"> 2</a>
</span><span class="lnt" id="hl-0-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-3"> 3</a>
</span><span class="lnt" id="hl-0-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-4"> 4</a>
</span><span class="lnt" id="hl-0-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-5"> 5</a>
</span><span class="lnt" id="hl-0-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-6"> 6</a>
</span><span class="lnt" id="hl-0-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-7"> 7</a>
</span><span class="lnt" id="hl-0-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-8"> 8</a>
</span><span class="lnt" id="hl-0-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-9"> 9</a>
</span><span class="lnt" id="hl-0-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-10">10</a>
</span><span class="lnt" id="hl-0-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-11">11</a>
</span><span class="lnt" id="hl-0-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-12">12</a>
</span><span class="lnt" id="hl-0-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-13">13</a>
</span><span class="lnt" id="hl-0-14"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-14">14</a>
</span><span class="lnt" id="hl-0-15"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-15">15</a>
</span><span class="lnt" id="hl-0-16"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-16">16</a>
</span><span class="lnt" id="hl-0-17"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-17">17</a>
</span><span class="lnt" id="hl-0-18"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-18">18</a>
</span><span class="lnt" id="hl-0-19"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-19">19</a>
</span><span class="lnt" id="hl-0-20"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-20">20</a>
</span><span class="lnt" id="hl-0-21"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-21">21</a>
</span><span class="lnt" id="hl-0-22"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-22">22</a>
</span><span class="lnt" id="hl-0-23"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-23">23</a>
</span><span class="lnt" id="hl-0-24"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-24">24</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">CSV</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_data</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/kaggle_data/movie.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">CSV</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">split_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># split 80/10/10</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">data</span><span class="p">[:</span><span class="n">train_split</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">train_split</span><span class="p">:</span><span class="n">val_split</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">val_split</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">save_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">writer</span><span class="o">.</span><span class="n">writerows</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">split_data</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_data</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="s1">&#39;data/train.csv&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_data</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="s1">&#39;data/val.csv&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_data</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="s1">&#39;data/test.csv&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Let&rsquo;s start with creating a proper and clean vocabulary that we will use for all the representations we will examine.</p>
<h3 id="clean-vocabulary">Clean Vocabulary</h3>
<p>We just read all the words as a set, to begin with,</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-1">1</a>
</span><span class="lnt" id="hl-1-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-2">2</a>
</span><span class="lnt" id="hl-1-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-3">3</a>
</span><span class="lnt" id="hl-1-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-4">4</a>
</span><span class="lnt" id="hl-1-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-5">5</a>
</span><span class="lnt" id="hl-1-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-6">6</a>
</span><span class="lnt" id="hl-1-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-7">7</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Get all the words</span>
</span></span><span class="line"><span class="cl"><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_data</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># len(words) = 7391216</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Get the vocabulary</span>
</span></span><span class="line"><span class="cl"><span class="n">dirty_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># len(dirty_vocab) = 331056</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>So for the beginning of the representation, we have 331.056 words in our vocabulary. This number is every non-sense included, though. We also didn&rsquo;t consider any lowercase - uppercase conversion. So let&rsquo;s clean these step by step.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-1">1</a>
</span><span class="lnt" id="hl-2-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-2">2</a>
</span><span class="lnt" id="hl-2-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-3">3</a>
</span><span class="lnt" id="hl-2-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-4">4</a>
</span><span class="lnt" id="hl-2-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-5">5</a>
</span><span class="lnt" id="hl-2-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-6">6</a>
</span><span class="lnt" id="hl-2-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-7">7</a>
</span><span class="lnt" id="hl-2-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-8">8</a>
</span><span class="lnt" id="hl-2-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-9">9</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Convert to lowercase</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">dirty_vocab</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># len(vocab) = 295827</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Remove punctuation</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">RegexpTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\w+&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">vocab</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># len(vocab) = 84757</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We reduced the number from 331.056 to 84.757. We can do more. With this method, we encode every word we see in every form possible. So, for example, &ldquo;called,&rdquo; &ldquo;calling,&rdquo; &ldquo;calls,&rdquo; and &ldquo;call&rdquo; will all be a separate words. Let&rsquo;s get rid of that and make them reduce to their roots. Here we start getting help from the dedicated NLP library NLTK since I don&rsquo;t want to define all these rules myself (nor could I):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-1">1</a>
</span><span class="lnt" id="hl-3-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-2">2</a>
</span><span class="lnt" id="hl-3-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-3">3</a>
</span><span class="lnt" id="hl-3-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-4">4</a>
</span><span class="lnt" id="hl-3-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-5">5</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Reduce words to their stems</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
</span></span><span class="line"><span class="cl"><span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># len(vocab) = 58893</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>The last step towards cleaning will be to get rid of stopwords. These are &rsquo;end,&rsquo; &lsquo;are,&rsquo; &lsquo;is,&rsquo; etc. words in the English language.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-1">1</a>
</span><span class="lnt" id="hl-4-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-2">2</a>
</span><span class="lnt" id="hl-4-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-3">3</a>
</span><span class="lnt" id="hl-4-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-4">4</a>
</span><span class="lnt" id="hl-4-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-5">5</a>
</span><span class="lnt" id="hl-4-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-6">6</a>
</span><span class="lnt" id="hl-4-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-7">7</a>
</span><span class="lnt" id="hl-4-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-8">8</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Remove connectives</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">nltk</span>
</span></span><span class="line"><span class="cl"><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
</span></span><span class="line"><span class="cl"><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;English))</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span> <span class="o">-</span> <span class="n">stop_words</span>
</span></span><span class="line"><span class="cl"><span class="c1"># len(vocab) = 58764</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now that we have good words, we can set up a lookup table to keep encodings for each word.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-5-1">1</a>
</span><span class="lnt" id="hl-5-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-5-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Vocabulary dictionary</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now we have a dictionary for every proper word we have in the data set. Therefore, we are ready to prepare different feature representations.</p>
<p>Since we will convert sentences in this clean form, again and again, later on, let&rsquo;s create a function that combines all these methods:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-6-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-1"> 1</a>
</span><span class="lnt" id="hl-6-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-2"> 2</a>
</span><span class="lnt" id="hl-6-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-3"> 3</a>
</span><span class="lnt" id="hl-6-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-4"> 4</a>
</span><span class="lnt" id="hl-6-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-5"> 5</a>
</span><span class="lnt" id="hl-6-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-6"> 6</a>
</span><span class="lnt" id="hl-6-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-7"> 7</a>
</span><span class="lnt" id="hl-6-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-8"> 8</a>
</span><span class="lnt" id="hl-6-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-9"> 9</a>
</span><span class="lnt" id="hl-6-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-10">10</a>
</span><span class="lnt" id="hl-6-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-11">11</a>
</span><span class="lnt" id="hl-6-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-12">12</a>
</span><span class="lnt" id="hl-6-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-13">13</a>
</span><span class="lnt" id="hl-6-14"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-14">14</a>
</span><span class="lnt" id="hl-6-15"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-15">15</a>
</span><span class="lnt" id="hl-6-16"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-16">16</a>
</span><span class="lnt" id="hl-6-17"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-17">17</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Function to combine all the above to clean a sentence</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">clean_sentence</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Convert to lowercase</span>
</span></span><span class="line"><span class="cl">    <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Words</span>
</span></span><span class="line"><span class="cl">    <span class="n">words</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Remove punctuation</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\w+&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">words</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Remove stop words</span>
</span></span><span class="line"><span class="cl">    <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;English))</span>
</span></span><span class="line"><span class="cl">    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Reduce words to their stems</span>
</span></span><span class="line"><span class="cl">    <span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># remove repeated words</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Ideally, we could initialize <code>tokenizer</code> <code>stemmer</code> and <code>stop_words</code> globally (or as a class parameter), so we don&rsquo;t have to keep initializing.</p>
<h3 id="sparse-vector-representation">Sparse Vector Representation</h3>
<p>This will represent every word we see in the database as a featureâ¦ Sounds unfeasible? Yeah, it should be. I see multiple problems here. The main one we all think about is this is a massive vector for each sentence with a lot of zeros (hence the name). This means most of the data we have is telling us practically the same thing as the minor part; we have these words in this sentence vs. we don&rsquo;t have all these words. Second, we are not keeping any correlation between words (since we are just examining word by word).</p>
<p>We go ahead and create a function for encoding every word for a sentence:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-7-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-1">1</a>
</span><span class="lnt" id="hl-7-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-2">2</a>
</span><span class="lnt" id="hl-7-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-3">3</a>
</span><span class="lnt" id="hl-7-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-4">4</a>
</span><span class="lnt" id="hl-7-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-5">5</a>
</span><span class="lnt" id="hl-7-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-6">6</a>
</span><span class="lnt" id="hl-7-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-7">7</a>
</span><span class="lnt" id="hl-7-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-8">8</a>
</span><span class="lnt" id="hl-7-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-9">9</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># function to convert a sentence to a vector encoding</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">encode_sparse</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">words</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">clean_words</span> <span class="o">=</span> <span class="n">clean_sentence</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">clean_words</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">vec</span><span class="p">[</span><span class="n">vocab_dict</span><span class="p">[</span><span class="n">w</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">vec</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We then convert all the data we have using this encoding (in a single matrix):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-8-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-1">1</a>
</span><span class="lnt" id="hl-8-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">train_data_sparse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">encode_sparse</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">val_data_sparse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">encode_sparse</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>That&rsquo;s it for this representation.</p>
<h3 id="word-frequency-representation">Word Frequency Representation</h3>
<p>This version practically reduces the 10.667 dimensions to 3 instead. We are going to count the number of negative sentences a word passes in as well as positive sentences. This will give us a table indicating how many positive and negative sentences a word has found in:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-9-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-1">1</a>
</span><span class="lnt" id="hl-9-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-2">2</a>
</span><span class="lnt" id="hl-9-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-3">3</a>
</span><span class="lnt" id="hl-9-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-4">4</a>
</span><span class="lnt" id="hl-9-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-5">5</a>
</span><span class="lnt" id="hl-9-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-6">6</a>
</span><span class="lnt" id="hl-9-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-7">7</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Counting frequency of words</span>
</span></span><span class="line"><span class="cl"><span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># [positive, negative]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">words</span> <span class="o">=</span> <span class="n">clean_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">freqs</span><span class="p">[</span><span class="n">vocab_dict</span><span class="p">[</span><span class="n">w</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>The next thing to do is to convert these enormous numbers into probabilities. There are multiple points to add here: First, we are getting the probability of this single word being in many positive and negative sentences, so the values will be minimal. Hence we need to use a log scale to avoid floating point problems. Second is, we might get words that don&rsquo;t appear in our dictionary, which will have a likelihood of 0. Since we don&rsquo;t want a 0 division, we add laplacian smoothing, like normalizing all the values with a small initial. Here goes the code:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-10-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-10-1">1</a>
</span><span class="lnt" id="hl-10-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-10-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Convert to log probabilities with Laplace smoothing</span>
</span></span><span class="line"><span class="cl"><span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="n">counts</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>After getting the frequencies and fixing the problems we mentioned, we now define the new encoding method for this version of the features</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-11-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-1">1</a>
</span><span class="lnt" id="hl-11-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-2">2</a>
</span><span class="lnt" id="hl-11-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-3">3</a>
</span><span class="lnt" id="hl-11-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-4">4</a>
</span><span class="lnt" id="hl-11-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-5">5</a>
</span><span class="lnt" id="hl-11-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-6">6</a>
</span><span class="lnt" id="hl-11-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-7">7</a>
</span><span class="lnt" id="hl-11-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-11-8">8</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">encode_freq</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">words</span> <span class="o">=</span> <span class="n">clean_sentence</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span> <span class="c1"># [bias, positive, negative]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">vocab_dict</span><span class="p">[</span><span class="n">word</span><span class="p">],</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">vec</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">vocab_dict</span><span class="p">[</span><span class="n">word</span><span class="p">],</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">vec</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We end by converting our data as before</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-12-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-12-1">1</a>
</span><span class="lnt" id="hl-12-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-12-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">train_data_pos_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">encode_freq</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">val_data_pos_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">encode_freq</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Let&rsquo;s take a sneak peek at what our data looks like:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-13-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-1"> 1</a>
</span><span class="lnt" id="hl-13-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-2"> 2</a>
</span><span class="lnt" id="hl-13-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-3"> 3</a>
</span><span class="lnt" id="hl-13-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-4"> 4</a>
</span><span class="lnt" id="hl-13-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-5"> 5</a>
</span><span class="lnt" id="hl-13-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-6"> 6</a>
</span><span class="lnt" id="hl-13-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-7"> 7</a>
</span><span class="lnt" id="hl-13-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-8"> 8</a>
</span><span class="lnt" id="hl-13-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-9"> 9</a>
</span><span class="lnt" id="hl-13-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-10">10</a>
</span><span class="lnt" id="hl-13-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-11">11</a>
</span><span class="lnt" id="hl-13-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-12">12</a>
</span><span class="lnt" id="hl-13-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-13">13</a>
</span><span class="lnt" id="hl-13-14"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-14">14</a>
</span><span class="lnt" id="hl-13-15"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-15">15</a>
</span><span class="lnt" id="hl-13-16"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-16">16</a>
</span><span class="lnt" id="hl-13-17"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-17">17</a>
</span><span class="lnt" id="hl-13-18"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-18">18</a>
</span><span class="lnt" id="hl-13-19"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-19">19</a>
</span><span class="lnt" id="hl-13-20"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-20">20</a>
</span><span class="lnt" id="hl-13-21"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-21">21</a>
</span><span class="lnt" id="hl-13-22"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-22">22</a>
</span><span class="lnt" id="hl-13-23"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-23">23</a>
</span><span class="lnt" id="hl-13-24"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-24">24</a>
</span><span class="lnt" id="hl-13-25"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-13-25">25</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Visualize the data with PCA</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a PCA instance. </span>
</span></span><span class="line"><span class="cl"><span class="c1"># This will reduce the data to 2 dimensions </span>
</span></span><span class="line"><span class="cl"><span class="c1"># as opposed to 3, where we have 2 features and a bias, more on that in next episode.</span>
</span></span><span class="line"><span class="cl"><span class="n">PCA</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Fit the PCA instance to the training data</span>
</span></span><span class="line"><span class="cl"><span class="n">x_data</span> <span class="o">=</span> <span class="n">train_data_pos_neg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">PCA</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Transform the training data to 2 dimensions ignoring the bias. This is due to the fact that the bias is a constant and will not affect the PCA</span>
</span></span><span class="line"><span class="cl"><span class="n">x_data_2d</span> <span class="o">=</span> <span class="n">PCA</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">train_data_pos_neg</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bwr&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;First Principal Component&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Second Principal Component&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Setup legend</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>
</span></span><span class="line"><span class="cl"><span class="n">red_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Negative&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">blue_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Positive&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="p">[</span><span class="n">red_patch</span><span class="p">,</span> <span class="n">blue_patch</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p></p>
<p>A better would be to use PCA for this kind of representation, but for now, we will ignore that fact since we want to explore that in episode 2.</p>
<h2 id="model-development">Model Development</h2>
<p>This episode mainly focuses on cleaning the data and developing decent representations. This is why I will only include a single model to test everything; Logistic Regression.</p>
<h3 id="logistic-regression">Logistic Regression</h3>
<p>Logistic regression is a simple single-layer network with sigmoid activation. This is an excellent baseline as it is one of the simplest binary classification methods. I am not explaining this method in depth, so if you want to learn more, please do so. I will use a simple <code>PyTorch</code> implementation.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-14-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-14-1">1</a>
</span><span class="lnt" id="hl-14-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-14-2">2</a>
</span><span class="lnt" id="hl-14-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-14-3">3</a>
</span><span class="lnt" id="hl-14-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-14-4">4</a>
</span><span class="lnt" id="hl-14-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-14-5">5</a>
</span><span class="lnt" id="hl-14-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-14-6">6</a>
</span><span class="lnt" id="hl-14-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-14-7">7</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We then define the loss function and the optimizer to use. I am using Binary Cross Entropy for the loss function and Adam for the optimization with a learning rate of <code>0.01</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-15-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-1"> 1</a>
</span><span class="lnt" id="hl-15-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-2"> 2</a>
</span><span class="lnt" id="hl-15-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-3"> 3</a>
</span><span class="lnt" id="hl-15-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-4"> 4</a>
</span><span class="lnt" id="hl-15-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-5"> 5</a>
</span><span class="lnt" id="hl-15-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-6"> 6</a>
</span><span class="lnt" id="hl-15-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-7"> 7</a>
</span><span class="lnt" id="hl-15-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-8"> 8</a>
</span><span class="lnt" id="hl-15-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-9"> 9</a>
</span><span class="lnt" id="hl-15-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-10">10</a>
</span><span class="lnt" id="hl-15-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-11">11</a>
</span><span class="lnt" id="hl-15-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-12">12</a>
</span><span class="lnt" id="hl-15-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-13">13</a>
</span><span class="lnt" id="hl-15-14"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-14">14</a>
</span><span class="lnt" id="hl-15-15"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-15">15</a>
</span><span class="lnt" id="hl-15-16"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-16">16</a>
</span><span class="lnt" id="hl-15-17"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-17">17</a>
</span><span class="lnt" id="hl-15-18"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-18">18</a>
</span><span class="lnt" id="hl-15-19"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-19">19</a>
</span><span class="lnt" id="hl-15-20"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-20">20</a>
</span><span class="lnt" id="hl-15-21"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-21">21</a>
</span><span class="lnt" id="hl-15-22"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-22">22</a>
</span><span class="lnt" id="hl-15-23"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-23">23</a>
</span><span class="lnt" id="hl-15-24"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-24">24</a>
</span><span class="lnt" id="hl-15-25"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-25">25</a>
</span><span class="lnt" id="hl-15-26"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-26">26</a>
</span><span class="lnt" id="hl-15-27"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-27">27</a>
</span><span class="lnt" id="hl-15-28"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-28">28</a>
</span><span class="lnt" id="hl-15-29"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-29">29</a>
</span><span class="lnt" id="hl-15-30"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-15-30">30</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">Cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the model</span>
</span></span><span class="line"><span class="cl"><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Forward pass</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Backward and optimize</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Validation</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_val</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">val_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Epoch [</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">], Loss: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span> 
</span></span><span class="line"><span class="cl">               <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Sparse Representation Training</strong> We first start with training the sparse representation. I trained for <code>100</code> epochs and reached <code>0.614</code> training accuracy and <code>0.606</code> validation accuracy. Here is the learning curve</p>
<p></p>
<p><strong>Word Frequency Representation</strong> <strong>Training</strong> I trained using the same parameter settings above, reaching <code>0.901</code> training accuracy and <code>0.861</code> validation accuracy. Here is the learning curve in the log scale</p>
<p></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Lecture Notes - Makemore Part 3: Activations &amp; Gradients, BatchNorm</title>
      <link>https://bedirtapkan.com/posts/blog_posts/makemore3/</link>
      <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/makemore3/</guid>
      <description>My lecture notes for Makemore Part 3 from Andrej Karpathy</description>
      <content:encoded><![CDATA[<!-- raw HTML omitted -->
<p>This post includes my notes from the lecture âMakemore Part 3:  Activations &amp; Gradients, BatchNormâ by Andrej Karpathy.</p>
<h2 id="initialization">Initialization</h2>
<p>Fixing the initial Loss:</p>
<ul>
<li>Initial loss must be arranged (the value depends on the question), in our case its a uniform probability.</li>
<li>When initializing make sure the numbers do not take extreme values (<code>* .01</code>)</li>
<li>Do not initialize to 0</li>
<li>Having <code>0</code> and <code>1</code> in softmax (a lot of them) is really bad, since the gradient will be 0 (vanishing gradient). This is called <strong>saturated tanh</strong>.</li>
<li><strong>So in summary:</strong> What we did is basically just making sure initially we give the network random values such that it is varied, and not making gradients 0 (no dead neurons). In the case of tanh, when we use softmax to squash the values, if the initial values were too broad, we will get a lot of vanishing gradients due to values ending up above <code>1</code> or below <code>-1</code>. So we first reduce the initial values and then use softmax on them (and continue training process).</li>
</ul>
<h2 id="kaiming-init">Kaiming Init</h2>
<p>Okay we know how to fix initialization now, but how much should we reduce these numbers? Meaning what is the value we should scale the layers with. Here comes <strong>Kaiming init</strong>.</p>
<ul>
<li>
<p>Here are two plots, left is for <code>x</code> and right is for <code>y</code> (pre activation, <code>x @ w</code>) layer values. We see that even though <code>x</code> and <code>w</code> are uniform gaussian with unit mean and standard deviation, the result of their dot product, y, has a non-unit standard deviation (still gaussian).</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph.png" type="" alt="Graph"  /></p>
</li>
<li>
<p>We donât want this in a neural network, we want the nn to have relatively simple activations, so we want unit gaussian throughout the network.</p>
</li>
<li>
<p>To keep std of <code>y</code> unit, we need to scale <code>w</code> down, as shown in the figure below (<code>w</code> scaled by <code>0.2</code>), but with what exactly?</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph1.png" type="" alt="Graph"  /></p>
</li>
<li>
<p>Mathematically this scale is equal to the square root of fan-in (number of input dimensions, e.g <code>10</code> for a tensor of <code>(10, 1000)</code>).</p>
</li>
<li>
<p>Depending on the activation function used, this value needs to be scaled by a <strong>gain</strong>. This gain is $\frac{5}{3}$ for tanh and 1 for linear, and $\sqrt{2}$ for relu. These values are due to shrinking and clamping the values (on relu and tanh).</p>
</li>
<li>
<p>Kaiming init is implemented in pytorch as <code>torch.nn.init.kaiming_normal_</code>.</p>
</li>
<li>
<p>Since the development of more sophisticated techniques in neural networks the importance of accurately initializing weights became unnecessary. To name some; residual connections, some normalizations (batch normalization etc.), optimizers (adam, rmsprop).</p>
</li>
<li>
<p>In practice, just normalizing by square root of fan-in is enough.</p>
</li>
</ul>
<p>Now that we see how to initialize the network, and mentioned some methods that makes this process more relaxed, letâs talk about one of these innovations; <strong>batch normalization</strong></p>
<h2 id="batch-normalization">Batch Normalization</h2>
<ul>
<li>
<p>We mentioned while training the network that we want balance in the pre-activation values, we donât want them to be zero, or too small so that tanh actually does something, and we donât want them to be too large because then tanh is saturated.</p>
</li>
<li>
<p>So we want roughly a uniform gaussian at initialization.</p>
</li>
<li>
<p>Batch Normalization basically says, <em>why donât we just take the hidden states and normalize them to be gaussian.</em></p>
</li>
<li>
<p>Right before the activation, we standardize the weights to be unit gaussian. We will do this by getting the mean and std of the batch, and scaling the values. Since all these operations are easily differentiable there will be no issues during the backprop phase.</p>
</li>
<li>
<p>For our example;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-1">1</a>
</span><span class="lnt" id="hl-0-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-2">2</a>
</span><span class="lnt" id="hl-0-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-3">3</a>
</span><span class="lnt" id="hl-0-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-4">4</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Forward pass</span>
</span></span><span class="line"><span class="cl"><span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Will have the batch norm before the activation is introduced. For this we need to calculate the mean and standard deviation of the batch;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-1">1</a>
</span><span class="lnt" id="hl-1-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">hstd</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Here we use <code>0</code> for the dimension since the shape of <code>preact</code> is <code>[num_samples, num_hidden_layers]</code> and we want the mean and std for all the samples for the weight connecting to one hidden layer. So the dimensions of <code>hmean</code> and <code>hstd</code> will be <code>[1, num_hidden_layers]</code>. So in the end we update our hpreact to;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hpreact_bn</span> <span class="o">=</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">hmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">hstd</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>If we leave it at that, we now have the weights forced to be unit gaussian at every step of the training. We want this to be the case only at the initialization. In general case we want the neural network to be able to move the distribution and scale it. So we introduce one more component called <strong>scale and shift</strong>.</p>
</li>
<li>
<p>These will be two new parameter set we add on our list that we start the scale with <code>1</code> and shift with <code>0</code>. We then backpropagate through these values and give the network the freedom to shift and scale the distribution;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-1">1</a>
</span><span class="lnt" id="hl-3-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">bnorm_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">bnorm_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We then update our <code>hpreact_bn</code> :</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hpreact_bn</span> <span class="o">=</span> <span class="p">((</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">hmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">hstd</span><span class="p">)</span> <span class="o">*</span> <span class="n">bnorm_scale</span> <span class="o">+</span> <span class="n">bnorm_bias</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We also add the new parameters in our parameters, to update while optimize the network:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-5-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bnorm_scale</span><span class="p">,</span> <span class="n">bnorm_bias</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>Itâs common to use batch norm throughout the neural network to be able to have a more relaxed initializations.</p>
</li>
<li>
<p>When introduced batch norm, we make the results of the forward and backward pass of any one input dependent on the batches. Meaning the result of a single sample is now not just dependent on itself but the batch it came with as well. Surprisingly, this is unexpectedly proven to be a good thing, acting as a regularizer.</p>
</li>
<li>
<p>This coupling effect is not always desired, which is why some scholars looked into other non-coupling regularizers such as Linear normalization.</p>
</li>
<li>
<p>One thing that still needs adjustment is how to use batch norm in testing phase. We trained the network on batches using batch mean and std but when the model is deployed, we want to use a single sample and get the result based on that. First method for accomplishing this is to calculate the exact mean and std on the complete dataset after training, like:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-6-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-1">1</a>
</span><span class="lnt" id="hl-6-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-2">2</a>
</span><span class="lnt" id="hl-6-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-3">3</a>
</span><span class="lnt" id="hl-6-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-4">4</a>
</span><span class="lnt" id="hl-6-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-5">5</a>
</span><span class="lnt" id="hl-6-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-6">6</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x_train</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
</span></span><span class="line"><span class="cl">    <span class="n">bn_mean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">bn_std</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>And using <code>bn_mean</code> and <code>bn_std</code> instead of <code>hmean</code> and <code>hstd</code> from the training loop.</p>
<p>We can further eliminate this step using a running mean and std. For this purpose we introduce two new parameters:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-7-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-1">1</a>
</span><span class="lnt" id="hl-7-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">running_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Then in the main training loop we update these values slowly. This will give us a close estimate.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-8-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-1">1</a>
</span><span class="lnt" id="hl-8-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-2">2</a>
</span><span class="lnt" id="hl-8-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-3">3</a>
</span><span class="lnt" id="hl-8-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-4">4</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Updating running mean and std</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">running_mean</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">running_mean</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">hmean</span>
</span></span><span class="line"><span class="cl">    <span class="n">running_std</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">running_std</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">hstd</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>There is a minor addition of $\epsilon$ on the paper to the denominator of the batch normalization. The reason is to avoid division by zero. We did not make use of this epsilon since it is highly unlikely that we get a zero std in our question.</p>
</li>
<li>
<p>Last fix we need to do is on bias. When we introduced the batch norm we made the bias <code>b1</code> useless. This is due to the subtracting the mean after applying the bias. Since the mean includes bias in it, we are practically adding and removing the same value, hence doing an unnecessary operation. In the case of batch norm, we do not need to use explicit bias for that layer, instead the batch norm bias, or <code>bnorm_bias</code> will handle the shifting of the values.</p>
</li>
<li>
<p>Use batchnorm carefully. It is really easy to make mistakes, mainly due to coupling. More recent networks usually prefer using layer normalization or group normalization. Batchnorm was very influential around 2015, since it introduced a reliable training for deeper networks because batchnorm was effective on controlling the statistics of the activations.</p>
</li>
</ul>
<h2 id="diagnostic-tools">Diagnostic Tools</h2>
<p>Training neural networks without the use of tools that makes initialization more relaxed, such as adam or batch normalization, is excruciating. Here we introduce multitudes of techniques to evaluate the correctness of the neural network.</p>
<h3 id="activation-distribution">Activation Distribution</h3>
<ul>
<li>
<p>First of, activation distribution throughout the layers. We are using a somehow deep network to be able to see the effects, with 5 layers. Each linear layer is followed by a <code>tanh</code>. As we saw before, <code>tanh</code> kaiming scale is $\frac{5}{3}$. Here is how the activations look like when we have it right:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph2.png" type="" alt="Graph"  /></p>
<p>We see that the layers have somehow similar activations throughout, saturation is around 5% which is what we wanted. If we change the scaling value to $1$ instead:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph3.png" type="" alt="Graph"  /></p>
<p>We get an unbalanced activations with 0 saturation. To see even more clear, letâs set the value to $0.5$:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph4.png" type="" alt="Graph"  /></p>
</li>
</ul>
<h3 id="gradient-distribution">Gradient Distribution</h3>
<ul>
<li>
<p>The next test is on gradients. Same as before we want the gradients throughout the layers to be similar. Here is the gradient distribution when we actually use $5/3$ as our scaling value:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph5.png" type="" alt="Graph"  /></p>
<p>As opposed to $$ $3$:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph6.png" type="" alt="Graph"  /></p>
<p>We can see here that the gradients are shrinking.</p>
</li>
</ul>
<h3 id="weight-gradient-distribution-on-parameters">Weight-Gradient Distribution on Parameters</h3>
<p>What are we checking:</p>
<ul>
<li>The std should be similar across layers</li>
<li>The mean should be similar across layers</li>
<li>The grad:data ratio should be similar across layers</li>
</ul>
<p>Grad:data ratio gives us an intuition of what is the scale of the gradient compared to the actual values. This is important because we will be taking a step update of the form <code>w = w - lr * grad</code>. If the gradient is too large compared to the actual values, we will be overshooting the minimum. If the gradient is too small compared to the actual values, we will be taking too many steps to reach the minimum.</p>
<p>The std of the gradient is a measure of how much the gradient changes across the weights. If the std for a layer is too different from the std of the other layers, this will be an issue because this layer will be learning at a different rate than the other layers.</p>
<p>This is for initialization phase. If we let the network train for a while, it will fix this issue itself. Nevertheless, this is an issue especially if we are using a simple optimizer like SGD. If we are using an optimizer like Adam, this issue will be fixed automatically.</p>
<p>Here are examples;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-9-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-1">1</a>
</span><span class="lnt" id="hl-9-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-2">2</a>
</span><span class="lnt" id="hl-9-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-3">3</a>
</span><span class="lnt" id="hl-9-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-4">4</a>
</span><span class="lnt" id="hl-9-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-5">5</a>
</span><span class="lnt" id="hl-9-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-6">6</a>
</span><span class="lnt" id="hl-9-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-7">7</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">33</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">+</span><span class="mf">0.000207</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">3.741454e-03</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">3.559389e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">+</span><span class="mf">0.000027</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">2.011833e-03</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">4.706446e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000008</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">1.438244e-03</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">4.848074e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000005</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">9.275978e-04</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">3.078747e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000007</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">7.061330e-04</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">2.373874e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000000</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">5.087151e-04</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">1.693161e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">33</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">+</span><span class="mf">0.000000</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">2.043289e-02</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">2.027916e+00</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph7.png" type="" alt="Graph"  /></p>
<p>We can see that the ratio of the last layer is way too large, as well as its standard deviation. Which is why the pink line on the graph is too wide.</p>
<h3 id="update-ratio">Update Ratio</h3>
<p>We calculate the update stdâs ratio with real value, and this gives us a measure for learning rate. Roughly the layers are all should be around <code>-3</code></p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph8.png" type="" alt="Graph"  /></p>
<p>The formula is for each epoch: <code>[(lr * p.grad.std() / p.data.std()).log().item() for p in params]</code>.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Average Reward, Continuing Tasks and Discounting</title>
      <link>https://bedirtapkan.com/posts/blog_posts/average_reward/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/average_reward/</guid>
      <description>Prime numbers are really important, how do we find them though?</description>
      <content:encoded><![CDATA[<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>Intro to Linear Methods</li>
<li>Semi-Gradient Prediction</li>
<li><a href="https://bedirt.github.io/reinforcement%20learning/control%20methods/2020/03/10/Semi-Gradient-Control.html">Semi-Gradient SARSA</a></li>
</ul>
<h2 id="what-is-continuous">What is continuous?</h2>
<p>Let&rsquo;s first describe the main task we will be handling; continuity. Continuous problems are tasks that has no specific terminal state, therefor will go on forever. As simple as it sounds, it is not a piece of cake to tackle the issues it brings with itself. Some examples could be the stock-market, where there is no end and you keep getting data. Or as book suggests access-control queuing task (Example 10.2).</p>
<p>I will follow a simple format so that we all can stay on the same page and everything is clear cut:</p>
<ul>
<li>Why is discounting not applicable for continuing tasks?</li>
<li>The Remedy: Average Reward
<ul>
<li>Logic behind it</li>
<li>Why is it true: Math</li>
</ul>
</li>
<li>Differential Semi-Gradient SARSA</li>
</ul>
<p>So let&rsquo;s start.</p>
<h2 id="why-is-discounting-not-applicable-for-continuing-tasks">Why is discounting not applicable for continuing tasks?</h2>
<p>First of all, we should know that discounting works well for tabular cases. The issue we will be talking about rises when we start to use approximations.</p>
<p>We have a sequence of episodes that has no beginning or end, and no way to clearly distinguish them. As the book suggests, we have the feature vectors to maybe have a use of, but then the issue of <strong>clearly seperable</strong> arises. We might have two feature vectors that has no to little difference between them, which won&rsquo;t be possible to be able to distinguish.</p>
<p>Since we have no start point or end point, and since there is no clear line in between episodes, using discounting is not possible. Well it actually is possible. But it is not needed. Actually using $\gamma = 0$ will give the same results as any other one. That&rsquo;s because the discounted rewards are proportional to average reward. That&rsquo;s why instead we will only use average reward. Here I will put the proof that both will results in the same order (discounted and without discounting):</p>
<p><img loading="lazy" src="/posts/blog_posts/average_reward/images/discounting_proof.png" type="" alt=""  /></p>
<p>The main issue with discounting in the approximation cases is that, since we have states depending on the same features, we do not have the <strong>policy improvement theorem</strong> anymore. Which was stating that we can get the optimal policy, just by changing all the action selections to the optimal ones for each state. Since we could choose the probabilities for one state without effecting the others it was pretty easy to handle. Now that we lost that property there is no guaranteed improvement over policy.</p>
<p>As Rich puts it <em>&ldquo;This is an area with multiple open theoretical questions&rdquo;</em>. If you are interested.</p>
<h2 id="the-remedy-average-reward">The Remedy: Average Reward</h2>
<p>Average reward is a pretty popular technique used in dynamic programming. Later on included into the Reinforcement Learning setting. We use average reward for approximated continual setting as we discussed above. Without discounting means that we care about each reward equally without thinking of if it occurs in far future etc.</p>
<p>We denote it as $r(\pi)$. Not much detail but for the intuition part I will give the main definition for it:
$$
r(\pi) \doteq \sum_{s}\mu_\pi\sum_{a}\pi(a|s)\sum_{r, s&rsquo;}p(r, s&rsquo;|s, a) r
$$
Basically we consider the best policy as the policy which has the most $r(\pi)$. For average reward we define returns as the difference between the $r(\pi)$ and the reward received at that point, this is called the differential return:
$$
G_t = R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + \ldots
$$
I believe differential return holds almost all the properties normal returns had. Only change we will do is to replace the reward with the difference i.e. $R_{t+1} - r(\pi)$. This goes for TD errors, bellman equations etc.</p>
<h3 id="quick-math">Quick Math</h3>
<p>So we already saw the formula for $r(\pi)$ but we didn&rsquo;t actually see how it came to existence or what all those things mean.
$$
r(\pi) \doteq \lim_{h\rightarrow\infty} \frac{1}{h} \sum_{t=1}^{h}\mathbb{E}[R_t|S_0, A_{0:t-1} \sim \pi]
$$
Let&rsquo;s explain what&rsquo;s happening here. We are assuming we have $h$ number of rewards, we are summing expected value of all the rewards given the first state and the action trajectory following the policy $\pi$, and we are dividing it to $h$ to get to the average of these rewards. So we simply had $h$ many rewards and we got the average. Then;
$$
= \lim_{t\rightarrow\infty} \mathbb{E}[R_t|S_0, A_{0:t-1} \sim \pi]
$$
Since I have the expectation inside the summation, we can actually simplify the summation with the division. We do have to put $t\rightarrow\infty$ to ccorrect the formula, as we will have number of samples approaching infinity. Next jump on the book seems fuzzy, but when you open it up it is extremely easy to see how it happens.</p>
<p>So if we have a randomness over something, what we want to do is to get the expectation of it. If we get the expectation that means we can formulate it, therefor no more randomness. In an MDP we have three kind of randomness possibly can happen.</p>
<ul>
<li>States are random</li>
<li>Actions are random</li>
<li>Dynamics are random</li>
</ul>
<p>What does this mean? It means we can be in a state, and we don&rsquo;t know what state that might be, and from there we will take an action, but we don&rsquo;t know for sure which action will that be. And the last one is that we take that action but since we don&rsquo;t know the dynamics of the environment (if stochastic even if we do know) we don&rsquo;t know which state we will end up in. So actually this formula goes like;
$$
\mathbb{E}[\mathbb{E} [ \mathbb{E}[R_t|S_t, A_t]]]
$$
Where the inner most is for the states and in the middle its the actions, the last one is the dynamics. So we know from bellman equations how to write this down;
$$
\mathbb{E}[R_t] = \sum_{s&rsquo;,r}p(s&rsquo;,r|s, a) r
$$
This is the expected reward is it not ? Now lets add the action selection on top:
$$
\mathbb{E}[R_t|A_t] = \sum_{a}\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s, a) r
$$
One last thing left is the state selection. We are using $\mu_\pi(s)$ to specify state distribution given the state (which the book covered earlier - Chapter 9). So the last piece of the puzzle;
$$
\mathbb{E}[R_t|A_t, S_t] = \sum_{s}\mu_\pi(s)\sum_{a}\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s, a) r
$$
That&rsquo;s all, we therefor have the average reward formula covered.</p>
<p>In practice we will be using moving mean to calculate average reward.</p>
<h2 id="differential-semi-gradient-sarsa">Differential Semi-Gradient SARSA</h2>
<p>Well, I don&rsquo;t really have much to add. If you read the <a href="https://bedirt.github.io/reinforcement%20learning/control%20methods/2020/03/10/Semi-Gradient-Control.html">Semi-Gradient SARSA</a> post, this is mostly just changing the update rule for the continuous setting. That will be the change for $G_{t:t+n}$.</p>
<p>$$G_{t:t+n}=R_{t+1}-\bar{R}_{t+1}$$</p>
<p>$$+R_{t+2}-\bar{R}_{t+2}$$</p>
<p>$$+\ldots+ R_{t+n}-\bar{R}_{t+n}$$</p>
<p>$$+\hat{q}(S_{t+n},A_{t+n},w_{t+n-1})$$</p>
<p>The TD error then will be like:</p>
<p>$$
\delta_t = G_{t:t+n} - \hat{q}(S_t, A_t, w)
$$</p>
<p>and we will use another step size parameter $\beta$ to update the average reward value. Here is the pseudocode:</p>
<p><img loading="lazy" src="/posts/blog_posts/average_reward/images/sarsa_differential_pseudo.png" type="" alt=""  /></p>
<p>And here is my implementation of it, which does not require much explanation I assume:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-1"> 1</a>
</span><span class="lnt" id="hl-0-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-2"> 2</a>
</span><span class="lnt" id="hl-0-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-3"> 3</a>
</span><span class="lnt" id="hl-0-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-4"> 4</a>
</span><span class="lnt" id="hl-0-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-5"> 5</a>
</span><span class="lnt" id="hl-0-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-6"> 6</a>
</span><span class="lnt" id="hl-0-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-7"> 7</a>
</span><span class="lnt" id="hl-0-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-8"> 8</a>
</span><span class="lnt" id="hl-0-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-9"> 9</a>
</span><span class="lnt" id="hl-0-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-10">10</a>
</span><span class="lnt" id="hl-0-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-11">11</a>
</span><span class="lnt" id="hl-0-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-12">12</a>
</span><span class="lnt" id="hl-0-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-13">13</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observations</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  	<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">observations</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">rewards</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">actions</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">G</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="n">r</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_rew</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">        <span class="n">G</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">delta</span> <span class="o">=</span> <span class="n">G</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">avg_rew</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">delta</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> 
</span></span><span class="line"><span class="cl">          <span class="bp">self</span><span class="o">.</span><span class="n">_grad_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>It is basically almost the same with the previous version. We are first checking if we have more elements than $n$ which means we need to remove the first elements from the storage. Then we have a check which sees if we have enough elements, because we won&rsquo;t be making any updates if there is not at least $n$ elements in the trajectory. The rest is the same update as in the pseudocode.</p>
<p>Again we run an experiment using the same settings as before which results in a high varience learning, thought it does learn which is the point here right now ð.</p>
<p><img loading="lazy" src="/posts/blog_posts/average_reward/images/sarsa_differential_figure.jpg" type="" alt=""  /></p>
<p>I have a blog series on RL algorithms that you can <a href="https://bedirt.github.io/tags/#betterrl-series">check out</a>. Also you can check <a href="https://github.com/BedirT/BetterRL">BetterRL</a> where I share raw python RL code for both environments and algorithms. Any comments are appreciated!</p>
<p><a href="https://github.com/BedirT/BetterRL/blob/master/value_based/Semi_Gradient_differential_SARSA.py">For full code</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Semi-Gradient Control Methods</title>
      <link>https://bedirtapkan.com/posts/blog_posts/semi_gradient_control/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/semi_gradient_control/</guid>
      <description>Prime numbers are really important, how do we find them though?</description>
      <content:encoded><![CDATA[<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>Semi-Gradient Prediction</li>
<li>Intro to Linear Methods</li>
</ul>
<p>If you read the prediction part for the semi gradient methods, it is pretty easy to extend what we know to the control case. We know that control is almost all the time just adding policy improvement over the prediction case. That&rsquo;s exactly the case for us here for semi-gradient control methods as well.</p>
<p>We already have describe and understood a formula back in prediction part (if you read it somewhere else that&rsquo;s also fine), and now we want to extend our window a little.</p>
<p>For prediction we were using $S_t \mapsto U_t$ examples, now since we have action-values instead of state-values (because we will pick the best action possible), we will use examples of form $S_t, A_t \mapsto U_t$ meaning that instead of $v_\pi(S_t)$ we will be using estimations for $q_\pi(S_t, A_t)$.</p>
<p>So our general update rule would be (following from the formula for prediction);</p>
<p>$$
w_{t+1} = w_t + \alpha [U_t - \hat{q}(S_t, A_t, w_t)] \nabla\hat{q}(S_t, A_t, w_t)
$$</p>
<p>As we always do, you can replace $U_t$ with any approximation method you want, so it could have been a Monte Carlo method (Though I believe this does not count as semi-gradient, because it will be a direct stochastic gradient since it does not use any bootstrapping, but the book says otherwise so I am just passing the information ð). Therefor we can implement an $n$-step episodic SARSA with an infinite option, which will correspond to Monte-Carlo (We will learn a better method to do this in future posts).</p>
<p>The last piece of information to add is the policy improvement part, since we are doing control, we need to update our policy and make it better as we go of course. Which won&rsquo;t be hard cause we will just be using a soft approximation method, I will use the classic $\epsilon$-greedy policy.</p>
<p>One more thing to note, which I think is pretty important, for continuous action spaces, or large discrete action spaces methods for the control part is still not clear. Meaning we don&rsquo;t know what is the best way to approach yet. That is if you think of a large choices of actions, there is no good way to apply a soft approximation technique for the action selection as you can imagine.</p>
<p>For the implementation, as usual we will just go linear, as it is the best way to grasp every piece of information. But first I will as usual give the pseudo-code given in the book.</p>
<p><img loading="lazy" src="/posts/blog_posts/semi_gradient_control/images/sg_sarsa.png" type="" alt=""  /></p>
<p>I only took the pseudocode from chapter 10.2 because we don&rsquo;t really the one before, as it is only the one step version. We are interested in the general version therefor n-step.</p>
<h3 id="implementation">Implementation</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-1"> 1</a>
</span><span class="lnt" id="hl-0-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-2"> 2</a>
</span><span class="lnt" id="hl-0-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-3"> 3</a>
</span><span class="lnt" id="hl-0-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-4"> 4</a>
</span><span class="lnt" id="hl-0-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-5"> 5</a>
</span><span class="lnt" id="hl-0-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-6"> 6</a>
</span><span class="lnt" id="hl-0-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-7"> 7</a>
</span><span class="lnt" id="hl-0-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-8"> 8</a>
</span><span class="lnt" id="hl-0-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-9"> 9</a>
</span><span class="lnt" id="hl-0-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-10">10</a>
</span><span class="lnt" id="hl-0-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-11">11</a>
</span><span class="lnt" id="hl-0-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-12">12</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">feature_space</span> <span class="o">=</span> <span class="n">feature_space</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">action_space</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">reset_weights</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">reset_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_space</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>        
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Initialize</strong> We start by initializing the necessary things; we need step size $\alpha$ also $\gamma$ and $\epsilon$. Other then these we need to initialize our weight vector. We will have a weight vector that is for each action concatenated after one another. So if we assume that we have 4 observations lets say [1 0 1 0], meaning weights 0 and 2 are active, and if want to update the weights for action 0, we will have [<strong>1 0 1 0</strong> 0 0 0 0 0 0 0 0] if we had 3 possible actions in total. After when we are using $\epsilon$-greedy this will make more sense.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-1">1</a>
</span><span class="lnt" id="hl-1-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-2">2</a>
</span><span class="lnt" id="hl-1-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-3">3</a>
</span><span class="lnt" id="hl-1-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-4">4</a>
</span><span class="lnt" id="hl-1-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-5">5</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Let&rsquo;s move</strong> next thing is to take a step, meaning we will pick the action according to our action-values at hand. We take the observations as input, this will come from the environment, and assuming we get an array of the probabilities for each action given the observations from <code> _act(obs)</code>. Then all we have to do is to roll the die and decide if we will choose a random action or we will choose the action that has the most value for the current time, and thats exactly what we do here ($\epsilon$-greedy action selection).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-1">1</a>
</span><span class="lnt" id="hl-2-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-2">2</a>
</span><span class="lnt" id="hl-2-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-3">3</a>
</span><span class="lnt" id="hl-2-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-4">4</a>
</span><span class="lnt" id="hl-2-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-5">5</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_vals</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_hat</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">q_vals</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Best $\hat{q}$-value</strong> now we need to fill the function <code>_act(obs)</code>. Which basically will call $\hat{q}(s, a, w)$ for each action and store them in an array and return it.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-1">1</a>
</span><span class="lnt" id="hl-3-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">q_hat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Continuing from there we have the $\hat{q}(s,a,w)$ to implement. Which is just writing down the linear formula since we are implementing it linearly. Therefor $\hat{q}(s,a,w) = w^Tx(s, a)$ where $x(s,a)$ is the state action representation. In our case as I already mention this will just be the one hot vector, all the observations are added after one another for each action.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-1">1</a>
</span><span class="lnt" id="hl-4-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-2">2</a>
</span><span class="lnt" id="hl-4-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-3">3</a>
</span><span class="lnt" id="hl-4-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-4">4</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">one_hot</span><span class="p">[</span><span class="n">action</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_space</span><span class="p">:(</span><span class="n">action</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_space</span><span class="p">]</span> <span class="o">=</span> <span class="n">obs</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">one_hot</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Finally</strong> $x(s, a)$ - as I already mentioned twice ð we create the $x$ in a vector that everything 0 other than the active action.</p>
<p>That was the last thing for us to be able to choose the action for a given state. So let&rsquo;s have a broader respective and assume that we are using the <code>step(obs)</code> here is how it would be like:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-5-1">1</a>
</span><span class="lnt" id="hl-5-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-5-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now we see what is left ? Update&hellip; ð¤¦ââï¸ Yeah without update there is no change basically. Which will also be the one differs for the $n$. Let&rsquo;s remember the formula;</p>
<p>$$
w_{t+1} = w_t + \alpha[R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^n\hat{q}(S_{t+n},A_{t+n},w_{t}) - \hat{q}(S_{t},A_{t},w_{t})] \nabla\hat{q}(S_t, A_t, w_t)
$$</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-6-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-1"> 1</a>
</span><span class="lnt" id="hl-6-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-2"> 2</a>
</span><span class="lnt" id="hl-6-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-3"> 3</a>
</span><span class="lnt" id="hl-6-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-4"> 4</a>
</span><span class="lnt" id="hl-6-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-5"> 5</a>
</span><span class="lnt" id="hl-6-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-6"> 6</a>
</span><span class="lnt" id="hl-6-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-7"> 7</a>
</span><span class="lnt" id="hl-6-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-8"> 8</a>
</span><span class="lnt" id="hl-6-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-9"> 9</a>
</span><span class="lnt" id="hl-6-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-10">10</a>
</span><span class="lnt" id="hl-6-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-11">11</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observations</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">observations</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewards</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">actions</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">G</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">r</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span><span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])])</span>
</span></span><span class="line"><span class="cl">            <span class="n">G</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">*</span> \
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_grad_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>There is a bit of a change here, from the pseudocode I provided. Since we want a full seperation between the agent-environment-experiment we need a class system for the algorithms therefor we won&rsquo;t be following what is on the pseudocode.</p>
<p><strong>Update</strong> what happens here is actually not that different, since we only need $n+1$ elements to make the update happen we won&rsquo;t keep the rest of the trajectory. Whenever we use n numbered trajectory the first element becomes useless for the next update. Therefor we remove the first element from the trajectory and use the rest to make our update.</p>
<p><strong>Terminal</strong> we also have a terminal state, and as can be seen in the pseudocode there are some differences that should be changed for the updates when we reach the terminal state. Logical enough, we do not have n+1 element left to complete the calculation we were doing therefor we will just use the rewards rather than $\hat{q}(s,a,w)$ . Therefor we need another function to handle this, which we call <code>end()</code> in our structure;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-7-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-1">1</a>
</span><span class="lnt" id="hl-7-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-2">2</a>
</span><span class="lnt" id="hl-7-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-3">3</a>
</span><span class="lnt" id="hl-7-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-4">4</a>
</span><span class="lnt" id="hl-7-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-5">5</a>
</span><span class="lnt" id="hl-7-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-6">6</a>
</span><span class="lnt" id="hl-7-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-7">7</a>
</span><span class="lnt" id="hl-7-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-8">8</a>
</span><span class="lnt" id="hl-7-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-9">9</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observations</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">observations</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewards</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">actions</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">G</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">r</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span><span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">*</span> \
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_grad_q_hat</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Here as we can see we are not doing something too different. It is just that we are using the last elements we have left and we will remove all the elements from the trajectory while making the last updates to our weights.</p>
<p>Yeah and we are almost done, exept that I didn&rsquo;t show the <code>grad_q_hat()</code> yet, which basically gives the $\nabla\hat{q}(s,a,w)$.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-8-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-1">1</a>
</span><span class="lnt" id="hl-8-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">grad_q_hat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Surprise.. Yeah since we are using linear functions, $\nabla w^Tx(s, a) = x(s,a)$. That&rsquo;s all.</p>
<p>Let&rsquo;s see how would be the experiment part and run the code to get some results then.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-9-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-1"> 1</a>
</span><span class="lnt" id="hl-9-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-2"> 2</a>
</span><span class="lnt" id="hl-9-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-3"> 3</a>
</span><span class="lnt" id="hl-9-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-4"> 4</a>
</span><span class="lnt" id="hl-9-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-5"> 5</a>
</span><span class="lnt" id="hl-9-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-6"> 6</a>
</span><span class="lnt" id="hl-9-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-7"> 7</a>
</span><span class="lnt" id="hl-9-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-8"> 8</a>
</span><span class="lnt" id="hl-9-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-9"> 9</a>
</span><span class="lnt" id="hl-9-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-10">10</a>
</span><span class="lnt" id="hl-9-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-11">11</a>
</span><span class="lnt" id="hl-9-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-12">12</a>
</span><span class="lnt" id="hl-9-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-13">13</a>
</span><span class="lnt" id="hl-9-14"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-14">14</a>
</span><span class="lnt" id="hl-9-15"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-15">15</a>
</span><span class="lnt" id="hl-9-16"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-16">16</a>
</span><span class="lnt" id="hl-9-17"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-17">17</a>
</span><span class="lnt" id="hl-9-18"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-18">18</a>
</span><span class="lnt" id="hl-9-19"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-19">19</a>
</span><span class="lnt" id="hl-9-20"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-20">20</a>
</span><span class="lnt" id="hl-9-21"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-21">21</a>
</span><span class="lnt" id="hl-9-22"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-22">22</a>
</span><span class="lnt" id="hl-9-23"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-23">23</a>
</span><span class="lnt" id="hl-9-24"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-24">24</a>
</span><span class="lnt" id="hl-9-25"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-25">25</a>
</span><span class="lnt" id="hl-9-26"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-26">26</a>
</span><span class="lnt" id="hl-9-27"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-27">27</a>
</span><span class="lnt" id="hl-9-28"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-28">28</a>
</span><span class="lnt" id="hl-9-29"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-29">29</a>
</span><span class="lnt" id="hl-9-30"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-30">30</a>
</span><span class="lnt" id="hl-9-31"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-31">31</a>
</span><span class="lnt" id="hl-9-32"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-32">32</a>
</span><span class="lnt" id="hl-9-33"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-33">33</a>
</span><span class="lnt" id="hl-9-34"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-34">34</a>
</span><span class="lnt" id="hl-9-35"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-35">35</a>
</span><span class="lnt" id="hl-9-36"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-36">36</a>
</span><span class="lnt" id="hl-9-37"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-37">37</a>
</span><span class="lnt" id="hl-9-38"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-38">38</a>
</span><span class="lnt" id="hl-9-39"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-39">39</a>
</span><span class="lnt" id="hl-9-40"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-40">40</a>
</span><span class="lnt" id="hl-9-41"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-41">41</a>
</span><span class="lnt" id="hl-9-42"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-42">42</a>
</span><span class="lnt" id="hl-9-43"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-43">43</a>
</span><span class="lnt" id="hl-9-44"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-44">44</a>
</span><span class="lnt" id="hl-9-45"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-45">45</a>
</span><span class="lnt" id="hl-9-46"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-46">46</a>
</span><span class="lnt" id="hl-9-47"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-47">47</a>
</span><span class="lnt" id="hl-9-48"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-48">48</a>
</span><span class="lnt" id="hl-9-49"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-49">49</a>
</span><span class="lnt" id="hl-9-50"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-50">50</a>
</span><span class="lnt" id="hl-9-51"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-51">51</a>
</span><span class="lnt" id="hl-9-52"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-52">52</a>
</span><span class="lnt" id="hl-9-53"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-53">53</a>
</span><span class="lnt" id="hl-9-54"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-54">54</a>
</span><span class="lnt" id="hl-9-55"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-55">55</a>
</span><span class="lnt" id="hl-9-56"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-56">56</a>
</span><span class="lnt" id="hl-9-57"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-57">57</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;num_of_episodes&#39;</span> <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;max_steps&#39;</span> <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;alpha&#39;</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">14</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;gamma&#39;</span> <span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Creating the tilings</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;grid_size&#39;</span> <span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;tile_size&#39;</span> <span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;num_of_tiles&#39;</span> <span class="p">:</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># environment</span>
</span></span><span class="line"><span class="cl"><span class="n">env</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">(</span><span class="n">portal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">action_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># tile coding</span>
</span></span><span class="line"><span class="cl"><span class="n">tilings</span> <span class="o">=</span> <span class="n">tile_coding</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;num_of_tiles&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;tile_size&#39;</span><span class="p">],</span> <span class="n">action_space</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">state_space</span> <span class="o">=</span> <span class="n">tilings</span><span class="o">.</span><span class="n">num_of_tilings</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Keep stats for final print and data</span>
</span></span><span class="line"><span class="cl"><span class="n">episode_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;num_of_episodes&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Agent created</span>
</span></span><span class="line"><span class="cl"><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl"><span class="n">agent</span> <span class="o">=</span> <span class="n">SG_SARSA</span><span class="p">(</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;num_of_episodes&#39;</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">observations</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">obs</span> <span class="o">=</span> <span class="n">tilings</span><span class="o">.</span><span class="n">active_tiles</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">())</span> <span class="c1"># a x d</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">observations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">obs</span> <span class="o">=</span> <span class="n">tilings</span><span class="o">.</span><span class="n">active_tiles</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">agent</span><span class="o">.</span><span class="n">end</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="n">episode_rewards</span><span class="p">[</span><span class="n">ep</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;EP: </span><span class="si">{}</span><span class="s2"> -------- Return: </span><span class="si">{}</span><span class="s2">      &#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">score</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&#34;</span><span class="se">\r</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>I used tile coding and the grid world environment in our library. If you want you can modify a little to use another state representation or Rich Sutton&rsquo;s tile coding library, or for environment gym.</p>
<p>Anyways, what we do is pretty simple if you read through, and you can ask for clarification on any point if looks weird.</p>
<p>Main point here are the agent functions and how we use them, all three are used as we said, on each step we have the <code>agent.step()</code>, for each step we have the <code>update()</code> called except the terminal state. Which we will call <code>end()</code> instead.</p>
<p>I will give only one graph as result as usual, here is 100 runs on the stochastic grid world environment.</p>
<p><img loading="lazy" src="/posts/blog_posts/semi_gradient_control/images/sg_sarsa_figure.jpg" type="" alt=""  /></p>
<p>If you liked this post follow <a href="https://github.com/BedirT/BetterRL">BetterRL</a>, and keep a like down below. I have a blog series on RL algorithms that you can <a href="bedirt.github.io">check out</a>. Also you can check the repo where I share raw python RL code for both environments and algorithms. Any comments are appreciated!</p>
<p><a href="https://github.com/BedirT/BetterRL/blob/master/value_based/Semi_Gradient_SARSA.py">For full code</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Counting Sort</title>
      <link>https://bedirtapkan.com/posts/blog_posts/counting_sort/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/counting_sort/</guid>
      <description>One of the most efficient sorting algorithms</description>
      <content:encoded><![CDATA[<p>Counting sort is a nice in-place sorting algorithm that we can use for sorting instantly. (This is mostly used for competitive programming.) What I meant by this is, we can use counting sort when we are getting the input. This will not get any additional cost for us, and really good technique for using in-place sorting. I&rsquo;ll get there after explaining the algorithm.</p>
<!-- raw HTML omitted -->
<h3 id="complexity-">Complexity ?</h3>
<hr>
<p>Let me tell you the complexity if you are wondering, but I will explain &ldquo;why&rdquo; after the algorithm itself. <strong>O(n+k)</strong> : n , is size of the array that we will sort and k is the maximum element we have.</p>
<h3 id="algorithm">Algorithm</h3>
<hr>
<p>OK. Let&rsquo;s think of an array. For the sake of simplicity, let&rsquo;s make it short&hellip;</p>
<pre><code>      myArray = [2, 3, 7, 4, 3, 9]
</code></pre>
<p>We have an array -unsorted- , minimum number is <strong>2</strong>, maximum is <strong>9</strong> and we have <strong>6</strong> elements in the array. Alright so let&rsquo;s think of one more array. Which is from 0 (array starting point) to our maximum number (9) and all the values are initially <strong>0</strong>.</p>
<pre><code>      INDEXES    0  1  2  3  4  5  6  7  8  9
weAreCounting = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>
<p><strong>ATTENTION!</strong> We are going to iterate over first array one by one, and we will increment the values that we have (i.e. for first step it is myArray[0] which is 2 so go to weAreCounting array and increment [2] by 1):</p>
<pre><code>for i in myArray:
    weAreCounting[i] += 1
</code></pre>
<p>NOW WHAT WE HAVE AT THE END OF THIS LOOP?</p>
<pre><code>      INDEXES    0  1  2  3  4  5  6  7  8  9
weAreCounting = [0, 0, 1, 2, 1, 0, 0, 1, 0, 1]
</code></pre>
<p>So weAreCounting array basically shows us how many of these numbers we do have (i.e. we have 2 threes so weAreCounting[3] = 2).</p>
<p>This part is how you have all the items counted. Now we can use this as sorted array (with iterating over it) or we can have our new array that will have the sorted array directly. For the second version:</p>
<p>My logic will be to use same array before (myArray) so that it will be more efficient (space-wise).</p>
<p>So I iterate thorough weAreCounting and if the number is bigger than 0 I will add it into myArray. That is all of the logic.</p>
<h3 id="code">Code</h3>
<hr>
<p>Here is the c++ code, as simplified as possible. ENJOY!</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-1"> 1</a>
</span><span class="lnt" id="hl-0-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-2"> 2</a>
</span><span class="lnt" id="hl-0-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-3"> 3</a>
</span><span class="lnt" id="hl-0-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-4"> 4</a>
</span><span class="lnt" id="hl-0-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-5"> 5</a>
</span><span class="lnt" id="hl-0-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-6"> 6</a>
</span><span class="lnt" id="hl-0-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-7"> 7</a>
</span><span class="lnt" id="hl-0-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-8"> 8</a>
</span><span class="lnt" id="hl-0-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-9"> 9</a>
</span><span class="lnt" id="hl-0-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-10">10</a>
</span><span class="lnt" id="hl-0-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-11">11</a>
</span><span class="lnt" id="hl-0-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-12">12</a>
</span><span class="lnt" id="hl-0-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-13">13</a>
</span><span class="lnt" id="hl-0-14"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-14">14</a>
</span><span class="lnt" id="hl-0-15"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-15">15</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">countingSort</span> <span class="p">(</span><span class="kt">int</span> <span class="o">*</span> <span class="n">arr</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">countingArray</span><span class="p">[</span><span class="n">MAX_NUM</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ARRAY_SIZE</span> <span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">countingArray</span><span class="p">[</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">output_Index</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">MAX_NUM</span> <span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="p">(</span> <span class="n">countingArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">--</span> <span class="p">)</span> <span class="c1">// Process will continue until the elements reach to 0
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">arr</span><span class="p">[</span><span class="n">output_Index</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span> <span class="c1">// PS: Incrementing will be after the line_process
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// Instead of these two lines we could use memset function too...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>And here is python3.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-1"> 1</a>
</span><span class="lnt" id="hl-1-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-2"> 2</a>
</span><span class="lnt" id="hl-1-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-3"> 3</a>
</span><span class="lnt" id="hl-1-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-4"> 4</a>
</span><span class="lnt" id="hl-1-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-5"> 5</a>
</span><span class="lnt" id="hl-1-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-6"> 6</a>
</span><span class="lnt" id="hl-1-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-7"> 7</a>
</span><span class="lnt" id="hl-1-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-8"> 8</a>
</span><span class="lnt" id="hl-1-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-9"> 9</a>
</span><span class="lnt" id="hl-1-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-10">10</a>
</span><span class="lnt" id="hl-1-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-11">11</a>
</span><span class="lnt" id="hl-1-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-12">12</a>
</span><span class="lnt" id="hl-1-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-13">13</a>
</span><span class="lnt" id="hl-1-14"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-14">14</a>
</span><span class="lnt" id="hl-1-15"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-15">15</a>
</span><span class="lnt" id="hl-1-16"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-16">16</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">countingSort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">countingArray</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">countingArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span><span class="p">(</span><span class="n">countingArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
</span></span><span class="line"><span class="cl">            <span class="n">countingArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">arr</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">countingSort</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span> <span class="p">,</span><span class="mi">5</span> <span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>If you check the complexity in the code, you will see that we have two loops, one is doing n operation (going through the array that we will sort). Second loop is doing k operation (which is the maximum number that we have in the array that we will sort). So time complexity will be O(n+k).</p>
<p>The space complexity: we have two arrays (we could have 3 but we decrease it to 3 because we used the one at the beginig two times. Since at the end we will not going to need that one.) one is size of n and one is size of k so our complexity will be O(n+k).</p>
<p>Now I suggest you to go to the link below, and try to solve the questions in Week 3/ Counting Sort section. So that you will have full understanding about this question. If you like the concept of it you can star or watch out repository as well. Have a great one!</p>
<p><a href="https://github.com/NAU-ACM/ACM-ICPC-Preparation">You can also find other algorithms explained and full code samples about this one here</a> (Check Week 3)</p>]]></content:encoded>
    </item>
    
    <item>
      <title>Factorizing a Number</title>
      <link>https://bedirtapkan.com/posts/blog_posts/prime_factorization/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/prime_factorization/</guid>
      <description>Prime numbers are really important, how do we find them though?</description>
      <content:encoded><![CDATA[<p><strong>Before starting-</strong> What is Prime Factorization ? What is a Prime number ? If you are curious about these please checkout this link before proceed because I will not explain them here :)</p>
<p><a href="https://en.wikipedia.org/wiki/Prime_number">Wikipedia - Prime Numbers</a></p>
<p>Since we all know what a prime number and composite number is, let&rsquo;s look at our realllly simple algorithm. Actually there is nothing fancy here, we are just using simple Sieve of Eratoshenes(Hardest name to pronounce, I checked online if I am right) algorithm. By the way that topic also pre-requised for this post, but fortunetely we already have a tutorial-explanation for it. If you don&rsquo;t know or confused about it in some ways please check the links below:</p>
<p><a href="https://github.com/BedirT/ACM-ICPC-Preparation/tree/master/Week01">Some sources to learn about Sieve of Eretosthenes</a></p>
<p>Since &ldquo;WE&rdquo; covered everything required, let me involve in this learning process too&hellip; Prime factorization: This is highly important topic. All your passwords , your bank accounts and stuff are protected by these numbers. Anyway that is why we actually have couple algorithms about Prime Factorization. There is a good answer on <a href="quora.com">quora.com</a> about the prime number algorithms:</p>
<blockquote>
<p>Different algorithms get used based on how large the number is. It goes something like this:</p>
<p>Small Numbers : Use simple sieve algorithms to create list of primes and do plain factorization. Works blazingly fast for small numbers.</p>
<p>Big Numbers : Use Pollard&rsquo;s rho algorithm, Shanks&rsquo; square forms factorization (Thanks to Dana Jacobsen for the pointer)</p>
<p>Less Than  10^25  : Use Lenstra elliptic curve factorization</p>
<p>Less Than  10^100  : Use Quadratic sieve</p>
<p>More Than  10^100  : Use General number field sieve</p>
<p>Currently, in the very large integer factorization arena, GNFS is the leader. It was the winner of the RSA factoring challenge of the 232 digit number</p>
<p><em>Arun Iyer</em> <a href="https://www.quora.com/Which-is-the-fastest-prime-factorization-algorithm-to-date">quora.com</a></p>
</blockquote>
<p>OK cool, we have a lot of options, although you can see that these numbers are gigantic. $10^{25}$ ?? This was the smallest one mentioned above by the way. So we don&rsquo;t really care about them, they are exist because like I said before, these numbers are extremely powerful so people need biiig ones. Since our languages supports (for C++) until $10^{19}$ , and our tutorials are for ACM-ICPC kind programming contests, considering that these contests have time limit and %100 sure that if $10^{25}$ will given&hellip; you probably should search for some trick in question, because we cannot compete that many operations on time.</p>
<h2 id="finally-the-algorithm">Finally the Algorithm</h2>
<p>Anyway after all explanation lets talk about our &ldquo;small&rdquo; algorithm. It really is nothing much than using Sieve algorithm. We are just going to optimize it a little bit. Let&rsquo;s say we already runned our sieve function:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">sieve</span><span class="p">(</span><span class="mi">10001</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now we have an array or vector , I don&rsquo;t know how you implemented so I will go with mine -&gt; you can check it out:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-1"> 1</a>
</span><span class="lnt" id="hl-1-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-2"> 2</a>
</span><span class="lnt" id="hl-1-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-3"> 3</a>
</span><span class="lnt" id="hl-1-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-4"> 4</a>
</span><span class="lnt" id="hl-1-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-5"> 5</a>
</span><span class="lnt" id="hl-1-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-6"> 6</a>
</span><span class="lnt" id="hl-1-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-7"> 7</a>
</span><span class="lnt" id="hl-1-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-8"> 8</a>
</span><span class="lnt" id="hl-1-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-9"> 9</a>
</span><span class="lnt" id="hl-1-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-10">10</a>
</span><span class="lnt" id="hl-1-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-11">11</a>
</span><span class="lnt" id="hl-1-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-12">12</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">primes</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">sieve</span><span class="p">(</span><span class="kt">int</span> <span class="n">size</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">bitset</span><span class="o">&lt;</span><span class="mi">10000010</span><span class="o">&gt;</span> <span class="n">was</span><span class="p">;</span> 	<span class="c1">// You can also use boolean array or vector, but this is optimized for bool (C++ is best :) )
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">was</span><span class="p">.</span><span class="n">set</span><span class="p">();</span>        		<span class="c1">// Initilizing all bitset to true
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">was</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">was</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>	<span class="c1">// Except 0 and 1 of course 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">size</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">  		<span class="k">if</span> <span class="p">(</span><span class="n">was</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  			<span class="n">primes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	    		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;=</span> <span class="n">size</span><span class="p">;</span> <span class="n">j</span> <span class="o">+=</span> <span class="n">i</span><span class="p">)</span> <span class="n">was</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  		<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We have a vector named primes and it has all the primes from begining(2) to size.</p>
<p>$$primes -&gt; [ 2 , 3 , 5 , 7 , 11 , 13 , 19 , 21 , &hellip; ]$$</p>
<p>What will we do is we will use basic logic and check every prime number and if it can divide our number <strong>N</strong>. If it can divide , we will just put it into our new vector (If you don&rsquo;t know vector you still can use list or array, depends on the language). If we can divide we will divide it, with this way we will decrement our operations. So let&rsquo;s say we have <strong>18</strong> as our <strong>N</strong>. We start with first element in the <strong><em>primes</em></strong> which is <strong>2</strong>.</p>
<ul>
<li>Is <strong>2</strong> dividing <strong>N = 18</strong> ?</li>
</ul>
<p>Yes obviously so:</p>
<ul>
<li>Put <strong>2</strong> into our <strong>Factors</strong> vector;</li>
</ul>
<p>So -&gt;</p>
<p>$$Factors -&gt; [ 2 ]$$</p>
<p>And we will divide our N by 2:</p>
<ul>
<li><strong>N = 18/2 = 9</strong></li>
</ul>
<p>Continue to check if 2 is dividing N which is not becuese <strong>N = 9</strong>. So lets pass 2 and go to 3:</p>
<ul>
<li>Is <strong>3</strong> dividing <strong>N = 9</strong> ? Yes</li>
<li>Put <strong>3</strong> into our <strong>Factors</strong> vector;</li>
</ul>
<p>$$Factors -&gt; [ 2 , 3 ]$$</p>
<ul>
<li><strong>N = 9/3 = 3</strong></li>
<li>Is <strong>3</strong> dividing <strong>N = 3</strong> ? Yes</li>
<li>Put <strong>3</strong> into our <strong>Factors</strong> vector;</li>
</ul>
<p>$$Factors -&gt; [ 2 , 3 , 3 ]$$</p>
<ul>
<li><strong>N = 3/3 = 1</strong></li>
<li>Is <strong>3</strong> dividing <strong>N = 1</strong> ? Nope</li>
<li>Proceed to <strong>5</strong>.</li>
</ul>
<p><strong>5</strong> ? Yes we will stop here. This is the next optimization, at most we will go until $p^2 \leq N$ (and p is my prime number that I am checking). This is what determines my complexity in this method. So I have $\sqrt{N}$ here. This is also my number that will go into O notation -&gt; O($\sqrt{N}$). (Mathematically this complexity is represented with $O(\pi(\sqrt{N})) = O(\sqrt{N}\times lnN)$)You can further check the code C++ implementation. I commented it so you can see what is going on in each step. After understanding the code I highly recommend you to solve questions about this topic, we have our list for this question as well, check the link at the bottom.</p>
<h2 id="implementation-c">Implementation C++</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-1"> 1</a>
</span><span class="lnt" id="hl-2-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-2"> 2</a>
</span><span class="lnt" id="hl-2-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-3"> 3</a>
</span><span class="lnt" id="hl-2-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-4"> 4</a>
</span><span class="lnt" id="hl-2-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-5"> 5</a>
</span><span class="lnt" id="hl-2-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-6"> 6</a>
</span><span class="lnt" id="hl-2-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-7"> 7</a>
</span><span class="lnt" id="hl-2-8"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-8"> 8</a>
</span><span class="lnt" id="hl-2-9"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-9"> 9</a>
</span><span class="lnt" id="hl-2-10"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-10">10</a>
</span><span class="lnt" id="hl-2-11"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-11">11</a>
</span><span class="lnt" id="hl-2-12"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-12">12</a>
</span><span class="lnt" id="hl-2-13"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-13">13</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">primeFactors</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">	<span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">vc</span><span class="p">;</span> <span class="c1">// An empty vector for us to fill with our numbers factors.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">primes</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span> <span class="c1">// f standing for FACTOR - idx is index that we will increment 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">while</span><span class="p">(</span><span class="n">N</span> <span class="o">!=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">N</span> <span class="o">&gt;=</span> <span class="n">f</span> <span class="o">*</span> <span class="n">f</span><span class="p">){</span>  <span class="c1">// f * f ... This is the part with sqrt(N) so the loop continues until our factor is bigger than sqrt(N)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">while</span><span class="p">(</span><span class="n">N</span> <span class="o">%</span> <span class="n">f</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span> 	<span class="c1">// I will continuously check if N is divisible by this prime, until it become wrong.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="n">N</span> <span class="o">/=</span> <span class="n">f</span><span class="p">;</span> 			<span class="c1">// Dividing N to my prime.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="n">vc</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">f</span><span class="p">);</span> 	<span class="c1">// adding that prime to my vector.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span><span class="p">(</span><span class="n">N</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="n">vc</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">N</span><span class="p">);</span> 	<span class="c1">// This case is for prime numbers itself, if the number is prime than we should add it to our vector. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>							<span class="c1">// If some value, after our loop is still not equals to 1 than it is a prime itself. (because of sqrt(N))
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">return</span> <span class="n">vc</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We have a well designed Curriculum on Github, also the questions about this algorithm are there too, check it out here</p>
<p><a href="https://github.com/BedirT/ACM-ICPC-Preparation">ACM-ICPC Curriculum</a></p>]]></content:encoded>
    </item>
    
    <item>
      <title>Kadane&#39;s Algorithm</title>
      <link>https://bedirtapkan.com/posts/blog_posts/kadane/</link>
      <pubDate>Sun, 18 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/kadane/</guid>
      <description>How to find sum of maximum of the subarrays</description>
      <content:encoded><![CDATA[<p>What we will learn today is, how to find the , in optimal solution. First, let&rsquo;s clarify the goal a bit.</p>
<p>What is subarray? Subarray is an array that is included in the bigger array. So if we have an array that has 7 elements in it. What we have is elements that have indexes of: 0, 1, 2, 3, 4, 5, 6 . A subarray is smaller array inside of this big array. So for example 1, 2, 3  or 4, 5 are subarrays. But 1, 3 is not a subarray because the subarray should be contiguous. So our task is to find the largest contiguous array in our big array.</p>
<p><img loading="lazy" src="/posts/blog_posts/kadane/images/not_subarray.jpg" type="" alt=""  /></p>
<p>Since we clarify our objective let&rsquo;s look at the solutions we have. First let&rsquo;s see what will be the brute force solution since that will be the first one which comes to mind. What we would do is, we would start from 0 index and hold it, check every elements before that index and keep the largest one. So if we have [3, 5, 7, 9] in the array. We would first check the 3. We would see that it is the largest subarray since there is none other than that. And than we would check the index 1 -&gt; 5, we have 5 and 5 + 3 = 8. The bigger one is 8 so we keep 8. Than we go for 2nd index -&gt; 7. We have 7, 7+5, 7+5+3 so that biggest one will be 7+5+3 which is 15, we keep it. Then next one : index 3 -&gt; 9. We have 9, 9+7, 9+7+5, 9+7+5+3. Largest one will be 9+7+5+3 = 24. So we compare the ones we found as sum of subarrays and the greatest one will be 24, the last one we checked, that&rsquo;s because we have no negative elements in the array. Anyways that would be the brute force solution and still a smart one. But the time complexity would be O(n^2). Since we take the index and check every others that we can combine with this index. Let me visualize this one:</p>
<p><img loading="lazy" src="/posts/blog_posts/kadane/images/bruteForce.jpg" type="" alt=""  /></p>
<p>OK, we got this part. So we got the question, now what is the optimal solution for this problem. What is this guy , <a href="https://en.wikipedia.org/wiki/Joseph_Born_Kadane">Kadane</a> , found. Here is the algorithm then. This algorithm is dynamic, which means we will approach the result using the ones we find before. OK, this guy teaches us a way that has complexity O(n), linear time.</p>
<p>Let&rsquo;s go with an example so it will be more clear. Our array is [5, -2, -4, 4, 4]. Kadane says that in each iteration we have only two options to get the max subarray:</p>
<ol>
<li>It can be only itself</li>
<li>It can be itself combined with the maximum subarray that previous index has.</li>
</ol>
<p>Man, this is a smart solution. OK, what he says is let&rsquo;s say we calculated the sum of maximum subarrays until index 1 which has value -2. For the sake of understanding let&rsquo;s calculate with brute force. We have -2 and -2+5. The greater one is -2+5 = 3. So let&rsquo;s proceed. This time let&rsquo;s use Kadane&rsquo;s Algorithm for calculating the 3rd step. What are the options:</p>
<ol>
<li>It can be only itself (We have -4 as the 2nd indexed value so = <strong>-4</strong>)</li>
<li>It can be itself combined with the maximum subarray that previous index has. (We have 3 as 1st index&rsquo;s max Subarray sum, so = <strong>3-4</strong> = <strong>-1</strong>)</li>
</ol>
<p>So we have a -1 and -4 &hellip; -1 indeed. But is it always the second option then? Let&rsquo;s see with another part of our array. Let&rsquo;s proceed one more step. Don&rsquo;t forget that we have -1 as our current max. 3rd index -&gt;</p>
<ol>
<li>It can be only itself (We have 4 as the value = <strong>4</strong>)</li>
<li>It can be itself combined with the maximum subarray that previous index has. (We have -1 as the previous index&rsquo;s maximum -&gt; <strong>-1+4=3</strong>)</li>
</ol>
<p>Now we approached the first option, 4 &gt; 3 so we will keep 4 instead of 3. And repeat this until the end &hellip; Really that&rsquo;s all.</p>
<p><img loading="lazy" src="/posts/blog_posts/kadane/images/kadaneTable.jpg" type="" alt=""  /></p>
<p>Now that we understand the logic. Let&rsquo;s proceed to the code. I will give pseudocode here.</p>
<pre><code>kadane(Array){

    generalMaximum = currentMaximum = Array[0]

    for (i = 1 until n) {
        currentMaximum = maximum of(Array[i], currentMaximum + Array[i]);
        if(currentMaximum &gt;= generalMaximum) generalMaximum = currentMaximum;
    }

    return generalMaximum;

}
</code></pre>
<p>If you are interested on learning or practicing more algorithms, you can visit our curriculum from github <a href="https://github.com/NAU-ACM/ACM-ICPC-Preparation">ACM-ICPC Preparation</a>. There are also questions and source code&rsquo;s about this topic. ENJOY!</p>]]></content:encoded>
    </item>
    
    <item>
      <title>Machine Learning Basics</title>
      <link>https://bedirtapkan.com/posts/blog_posts/ml_basics/</link>
      <pubDate>Sat, 27 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://bedirtapkan.com/posts/blog_posts/ml_basics/</guid>
      <description>An introduction to Supervised vs Unsupervised Learning</description>
      <content:encoded><![CDATA[<p>Reaaally introductory post for Machine Learning basics. What is Supervised and Unsupervised learning?</p>
<h2 id="supervised-learning">Supervised Learning</h2>
<p><strong>Supervised learning</strong> is the most common machine learning problem. In Supervised Learning we already know what the correct output should be like.</p>
<p>There are two categories for supervised learning, first one is <strong>&ldquo;regression problem&rdquo;</strong> and the second one is <strong>&ldquo;classification problem&rdquo;</strong>. I will explain both with examples.</p>
<h2 id="regression-problems">Regression Problems</h2>
<p>In regression we have an output that is continous and we are trying to predict what will be the correct answer/output/label for our input. Lets see an example to understand the concept better.</p>
<h3 id="examples">Examples:</h3>
<p>Let&rsquo;s say we have a friend who has a chocalate company. He has a lot of money and he wants to make his product sell as many as Snickers. OK. But his chocalates are not famous as Snickers. Now , what he should do is, take a look at the competitor. There is a chart which has two dimensions. One is the price. Another is the popularity. Now that since we have continous output for the prices. We will predict the one that we are looking for. (I will just give the popularities according to myself.)</p>
<p><img loading="lazy" src="/posts/blog_posts/ml_basics/images/graphs1.png" type="" alt=""  /></p>
<p>Now, looking at this output. What should we do is, putting a straight or polinomial line to the outputs.</p>
<p><img loading="lazy" src="/posts/blog_posts/ml_basics/images/graphs2.png" type="" alt=""  /></p>
<p><img loading="lazy" src="/posts/blog_posts/ml_basics/images/graphs3.png" type="" alt=""  /></p>
<p>Then we will have our line that will help us to predict the price. According to the surveys, our chocalates have 8 point for the popularity. So what will be the best price according to the survey and the industry&hellip;</p>
<p><img loading="lazy" src="/posts/blog_posts/ml_basics/images/graphs4.png" type="" alt=""  /></p>
<p>It seems something like 70Â¢ &hellip;</p>
<p>This is the regression problem &hellip;</p>
<h2 id="classification-problems">Classification Problems</h2>
<p>In classification, the simplest one, binary classification, we have two options, either true or false. We also can say that we will predict the result in a binary map. Let&rsquo;s check an example.</p>
<h3 id="examples-1">Examples:</h3>
<p>Let&rsquo;s give an absurd example so that it will be more permament. So we have a friend who just ate 5 kilos of Nutella and he is 24 years old. We want to predict if he will get sick or not. And we have a dataset that have people&rsquo;s ages that ate 5 kilos of Nutella and got the sick or not !!</p>
<p><img loading="lazy" src="/posts/blog_posts/ml_basics/images/graphs5.png" type="" alt=""  /></p>
<p>So according to this graph our friend will get sick or not. It is a binary example. There is just two probabalities. This is a classification problem. Let&rsquo;s see the expected result &hellip;</p>
<p><img loading="lazy" src="/posts/blog_posts/ml_basics/images/graphs6.png" type="" alt=""  /></p>
<p>(He will probably get sick, according to our prediction.)</p>
<h2 id="unsupervised-learning">Unsupervised Learning</h2>
<p>The <strong>unsupervised learning</strong> is the second most common machine learning problem. In unsupervised learning we don&rsquo;t know the result for each input. We will obtain a structure form the data. We do not know what are the exact effects of our inputs. We will use <strong>clustering</strong> for this.</p>
<h2 id="clustering-problem">Clustering Problem</h2>
<p>We basically will seperate the data according to variables.</p>
<p><img loading="lazy" src="/posts/blog_posts/ml_basics/images/graphs7.png" type="" alt=""  /></p>
<p>Let&rsquo;s say you got hundred different composition classes&rsquo; final essays. They all have different topics. What clustering do is, classifying all the essays according to their topics. So that if we use clustering, all these classes&rsquo; articles will be separated. This is just one variable (topic). If you want, you can add more variables to make the groups more specific. In this case we can add words count for example.</p>]]></content:encoded>
    </item>
    
  </channel>
</rss>
