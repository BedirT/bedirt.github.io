<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Lecture Notes - Makemore Part 3: Activations &amp; Gradients, BatchNorm | Bedir Tapkan</title>
<meta name="keywords" content="Machine Learning, Deep Learning">
<meta name="description" content="My lecture notes for Makemore Part 3 from Andrej Karpathy">
<meta name="author" content="Bedir Tapkan">
<link rel="canonical" href="https://bedirtapkan.com/posts/blog_posts/makemore3/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.34f610da9fa2cd806ab9ff3f025f84f8f50c190cfbe3eb63b50b7d092bd7afdf.css" integrity="sha256-NPYQ2p&#43;izYBquf8/Al&#43;E&#43;PUMGQz74&#43;tjtQt9CSvXr98=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bedirtapkan.com/favicon.ico">
<link rel="apple-touch-icon" href="https://bedirtapkan.com/apple-touch-icon.png">
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Lecture Notes - Makemore Part 3: Activations &amp; Gradients, BatchNorm | Bedir Tapkan" />
<meta name="twitter:description" content="My lecture notes for Makemore Part 3 from Andrej Karpathy" />
<meta property="og:title" content="Lecture Notes - Makemore Part 3: Activations &amp; Gradients, BatchNorm | Bedir Tapkan" />
<meta property="og:description" content="My lecture notes for Makemore Part 3 from Andrej Karpathy" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bedirtapkan.com/posts/blog_posts/makemore3/" />
    <meta property="og:image" content="https://bedirtapkan.com/logo.png"/>
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2022-10-26T00:00:00&#43;00:00" />
  <meta property="article:modified_time" content="2022-10-26T00:00:00&#43;00:00" /><meta property="og:site_name" content="Bedir Tapkan" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://bedirtapkan.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Lecture Notes - Makemore Part 3: Activations \u0026 Gradients, BatchNorm",
      "item": "https://bedirtapkan.com/posts/blog_posts/makemore3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Lecture Notes - Makemore Part 3: Activations \u0026 Gradients, BatchNorm | Bedir Tapkan",
  "name": "Lecture Notes - Makemore Part 3: Activations \u0026 Gradients, BatchNorm",
  "description": "My lecture notes for Makemore Part 3 from Andrej Karpathy",
  "keywords": [
    "Machine Learning", "Deep Learning"
  ],
  "wordCount" : "1891",
  "inLanguage": "en",
  "datePublished": "2022-10-26T00:00:00Z",
  "dateModified": "2022-10-26T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Bedir Tapkan"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bedirtapkan.com/posts/blog_posts/makemore3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Bedir Tapkan",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bedirtapkan.com/favicon.ico"
    }
  }
}
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js" integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
onload="renderMathInElement(document.body, 
{
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '\\[', right: '\\]', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false}
            ]
        }
);"></script>

<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary-bg: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list-page {
                background: var(--theme);
            }

            .list-page:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list-page:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'auto';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bedirtapkan.com" accesskey="h" title="Bedir Tapkan (Alt + H)">Bedir Tapkan</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bedirtapkan.com/about/" title="About"
                >About
                </a>
            </li>
            <li>
                <a href="https://bedirtapkan.com/projects/" title="Projects"
                >Projects
                </a>
            </li>
            <li>
                <a href="https://bedirtapkan.com/archives/" title="Archives"
                >Archives
                </a>
            </li>
            <li>
                <a href="https://bedirtapkan.com/search/" title="Search (Alt &#43; /)"data-no-instant accesskey=/
                >Search
                </a>
            </li>
            <li>
                <a href="https://bedirtapkan.com/design/" title="3D Projects"
                >3D Projects
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://bedirtapkan.com">Home</a>&nbsp;»&nbsp;<a href="https://bedirtapkan.com/posts/">Posts</a></div><h1 class="post-title">Lecture Notes - Makemore Part 3: Activations &amp; Gradients, BatchNorm</h1>
    <div class="post-description">My lecture notes for Makemore Part 3 from Andrej Karpathy</div>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>October 26, 2022</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select: text;"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z" style="user-select: text;"></path><line x1="7" y1="7" x2="7" y2="7" style="user-select: text;"></line></svg>
  <span class="post-tags"><a href="https://bedirtapkan.com/tags/machine-learning/">Machine Learning</a><a href="https://bedirtapkan.com/tags/deep-learning/">Deep Learning</a></span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select: text;"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" style="user-select: text;"></path><polyline points="14 2 14 8 20 8" style="user-select: text;"></polyline><line x1="16" y1="13" x2="8" y2="13" style="user-select: text;"></line><line x1="16" y1="17" x2="8" y2="17" style="user-select: text;"></line><polyline points="10 9 9 9 8 9" style="user-select: text;"></polyline></svg>
  <span>1891 words</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>9 min</span></span>

      
      
    </div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#initialization">Initialization</a></li>
    <li><a href="#kaiming-init">Kaiming Init</a></li>
    <li><a href="#batch-normalization">Batch Normalization</a></li>
    <li><a href="#diagnostic-tools">Diagnostic Tools</a>
      <ul>
        <li><a href="#activation-distribution">Activation Distribution</a></li>
        <li><a href="#gradient-distribution">Gradient Distribution</a></li>
        <li><a href="#weight-gradient-distribution-on-parameters">Weight-Gradient Distribution on Parameters</a></li>
        <li><a href="#update-ratio">Update Ratio</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><!-- raw HTML omitted -->
<p>This post includes my notes from the lecture “Makemore Part 3:  Activations &amp; Gradients, BatchNorm” by Andrej Karpathy.</p>
<h2 id="initialization">Initialization<a hidden class="anchor" aria-hidden="true" href="#initialization">¶</a></h2>
<p>Fixing the initial Loss:</p>
<ul>
<li>Initial loss must be arranged (the value depends on the question), in our case its a uniform probability.</li>
<li>When initializing make sure the numbers do not take extreme values (<code>* .01</code>)</li>
<li>Do not initialize to 0</li>
<li>Having <code>0</code> and <code>1</code> in softmax (a lot of them) is really bad, since the gradient will be 0 (vanishing gradient). This is called <strong>saturated tanh</strong>.</li>
<li><strong>So in summary:</strong> What we did is basically just making sure initially we give the network random values such that it is varied, and not making gradients 0 (no dead neurons). In the case of tanh, when we use softmax to squash the values, if the initial values were too broad, we will get a lot of vanishing gradients due to values ending up above <code>1</code> or below <code>-1</code>. So we first reduce the initial values and then use softmax on them (and continue training process).</li>
</ul>
<h2 id="kaiming-init">Kaiming Init<a hidden class="anchor" aria-hidden="true" href="#kaiming-init">¶</a></h2>
<p>Okay we know how to fix initialization now, but how much should we reduce these numbers? Meaning what is the value we should scale the layers with. Here comes <strong>Kaiming init</strong>.</p>
<ul>
<li>
<p>Here are two plots, left is for <code>x</code> and right is for <code>y</code> (pre activation, <code>x @ w</code>) layer values. We see that even though <code>x</code> and <code>w</code> are uniform gaussian with unit mean and standard deviation, the result of their dot product, y, has a non-unit standard deviation (still gaussian).</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph.png" type="" alt="Graph"  /></p>
</li>
<li>
<p>We don’t want this in a neural network, we want the nn to have relatively simple activations, so we want unit gaussian throughout the network.</p>
</li>
<li>
<p>To keep std of <code>y</code> unit, we need to scale <code>w</code> down, as shown in the figure below (<code>w</code> scaled by <code>0.2</code>), but with what exactly?</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph1.png" type="" alt="Graph"  /></p>
</li>
<li>
<p>Mathematically this scale is equal to the square root of fan-in (number of input dimensions, e.g <code>10</code> for a tensor of <code>(10, 1000)</code>).</p>
</li>
<li>
<p>Depending on the activation function used, this value needs to be scaled by a <strong>gain</strong>. This gain is $\frac{5}{3}$ for tanh and 1 for linear, and $\sqrt{2}$ for relu. These values are due to shrinking and clamping the values (on relu and tanh).</p>
</li>
<li>
<p>Kaiming init is implemented in pytorch as <code>torch.nn.init.kaiming_normal_</code>.</p>
</li>
<li>
<p>Since the development of more sophisticated techniques in neural networks the importance of accurately initializing weights became unnecessary. To name some; residual connections, some normalizations (batch normalization etc.), optimizers (adam, rmsprop).</p>
</li>
<li>
<p>In practice, just normalizing by square root of fan-in is enough.</p>
</li>
</ul>
<p>Now that we see how to initialize the network, and mentioned some methods that makes this process more relaxed, let’s talk about one of these innovations; <strong>batch normalization</strong></p>
<h2 id="batch-normalization">Batch Normalization<a hidden class="anchor" aria-hidden="true" href="#batch-normalization">¶</a></h2>
<ul>
<li>
<p>We mentioned while training the network that we want balance in the pre-activation values, we don’t want them to be zero, or too small so that tanh actually does something, and we don’t want them to be too large because then tanh is saturated.</p>
</li>
<li>
<p>So we want roughly a uniform gaussian at initialization.</p>
</li>
<li>
<p>Batch Normalization basically says, <em>why don’t we just take the hidden states and normalize them to be gaussian.</em></p>
</li>
<li>
<p>Right before the activation, we standardize the weights to be unit gaussian. We will do this by getting the mean and std of the batch, and scaling the values. Since all these operations are easily differentiable there will be no issues during the backprop phase.</p>
</li>
<li>
<p>For our example;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-1">1</a>
</span><span class="lnt" id="hl-0-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-2">2</a>
</span><span class="lnt" id="hl-0-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-3">3</a>
</span><span class="lnt" id="hl-0-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-0-4">4</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Forward pass</span>
</span></span><span class="line"><span class="cl"><span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Will have the batch norm before the activation is introduced. For this we need to calculate the mean and standard deviation of the batch;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-1">1</a>
</span><span class="lnt" id="hl-1-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-1-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">hstd</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Here we use <code>0</code> for the dimension since the shape of <code>preact</code> is <code>[num_samples, num_hidden_layers]</code> and we want the mean and std for all the samples for the weight connecting to one hidden layer. So the dimensions of <code>hmean</code> and <code>hstd</code> will be <code>[1, num_hidden_layers]</code>. So in the end we update our hpreact to;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-2-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hpreact_bn</span> <span class="o">=</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">hmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">hstd</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>If we leave it at that, we now have the weights forced to be unit gaussian at every step of the training. We want this to be the case only at the initialization. In general case we want the neural network to be able to move the distribution and scale it. So we introduce one more component called <strong>scale and shift</strong>.</p>
</li>
<li>
<p>These will be two new parameter set we add on our list that we start the scale with <code>1</code> and shift with <code>0</code>. We then backpropagate through these values and give the network the freedom to shift and scale the distribution;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-1">1</a>
</span><span class="lnt" id="hl-3-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-3-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">bnorm_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">bnorm_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We then update our <code>hpreact_bn</code> :</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-4-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hpreact_bn</span> <span class="o">=</span> <span class="p">((</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">hmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">hstd</span><span class="p">)</span> <span class="o">*</span> <span class="n">bnorm_scale</span> <span class="o">+</span> <span class="n">bnorm_bias</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>We also add the new parameters in our parameters, to update while optimize the network:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-5-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bnorm_scale</span><span class="p">,</span> <span class="n">bnorm_bias</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>It’s common to use batch norm throughout the neural network to be able to have a more relaxed initializations.</p>
</li>
<li>
<p>When introduced batch norm, we make the results of the forward and backward pass of any one input dependent on the batches. Meaning the result of a single sample is now not just dependent on itself but the batch it came with as well. Surprisingly, this is unexpectedly proven to be a good thing, acting as a regularizer.</p>
</li>
<li>
<p>This coupling effect is not always desired, which is why some scholars looked into other non-coupling regularizers such as Linear normalization.</p>
</li>
<li>
<p>One thing that still needs adjustment is how to use batch norm in testing phase. We trained the network on batches using batch mean and std but when the model is deployed, we want to use a single sample and get the result based on that. First method for accomplishing this is to calculate the exact mean and std on the complete dataset after training, like:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-6-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-1">1</a>
</span><span class="lnt" id="hl-6-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-2">2</a>
</span><span class="lnt" id="hl-6-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-3">3</a>
</span><span class="lnt" id="hl-6-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-4">4</a>
</span><span class="lnt" id="hl-6-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-5">5</a>
</span><span class="lnt" id="hl-6-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-6-6">6</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x_train</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
</span></span><span class="line"><span class="cl">    <span class="n">bn_mean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">bn_std</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>And using <code>bn_mean</code> and <code>bn_std</code> instead of <code>hmean</code> and <code>hstd</code> from the training loop.</p>
<p>We can further eliminate this step using a running mean and std. For this purpose we introduce two new parameters:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-7-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-1">1</a>
</span><span class="lnt" id="hl-7-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-7-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">running_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden_neurons</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Then in the main training loop we update these values slowly. This will give us a close estimate.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-8-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-1">1</a>
</span><span class="lnt" id="hl-8-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-2">2</a>
</span><span class="lnt" id="hl-8-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-3">3</a>
</span><span class="lnt" id="hl-8-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-8-4">4</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Updating running mean and std</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">running_mean</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">running_mean</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">hmean</span>
</span></span><span class="line"><span class="cl">    <span class="n">running_std</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">running_std</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">hstd</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>There is a minor addition of $\epsilon$ on the paper to the denominator of the batch normalization. The reason is to avoid division by zero. We did not make use of this epsilon since it is highly unlikely that we get a zero std in our question.</p>
</li>
<li>
<p>Last fix we need to do is on bias. When we introduced the batch norm we made the bias <code>b1</code> useless. This is due to the subtracting the mean after applying the bias. Since the mean includes bias in it, we are practically adding and removing the same value, hence doing an unnecessary operation. In the case of batch norm, we do not need to use explicit bias for that layer, instead the batch norm bias, or <code>bnorm_bias</code> will handle the shifting of the values.</p>
</li>
<li>
<p>Use batchnorm carefully. It is really easy to make mistakes, mainly due to coupling. More recent networks usually prefer using layer normalization or group normalization. Batchnorm was very influential around 2015, since it introduced a reliable training for deeper networks because batchnorm was effective on controlling the statistics of the activations.</p>
</li>
</ul>
<h2 id="diagnostic-tools">Diagnostic Tools<a hidden class="anchor" aria-hidden="true" href="#diagnostic-tools">¶</a></h2>
<p>Training neural networks without the use of tools that makes initialization more relaxed, such as adam or batch normalization, is excruciating. Here we introduce multitudes of techniques to evaluate the correctness of the neural network.</p>
<h3 id="activation-distribution">Activation Distribution<a hidden class="anchor" aria-hidden="true" href="#activation-distribution">¶</a></h3>
<ul>
<li>
<p>First of, activation distribution throughout the layers. We are using a somehow deep network to be able to see the effects, with 5 layers. Each linear layer is followed by a <code>tanh</code>. As we saw before, <code>tanh</code> kaiming scale is $\frac{5}{3}$. Here is how the activations look like when we have it right:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph2.png" type="" alt="Graph"  /></p>
<p>We see that the layers have somehow similar activations throughout, saturation is around 5% which is what we wanted. If we change the scaling value to $1$ instead:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph3.png" type="" alt="Graph"  /></p>
<p>We get an unbalanced activations with 0 saturation. To see even more clear, let’s set the value to $0.5$:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph4.png" type="" alt="Graph"  /></p>
</li>
</ul>
<h3 id="gradient-distribution">Gradient Distribution<a hidden class="anchor" aria-hidden="true" href="#gradient-distribution">¶</a></h3>
<ul>
<li>
<p>The next test is on gradients. Same as before we want the gradients throughout the layers to be similar. Here is the gradient distribution when we actually use $5/3$ as our scaling value:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph5.png" type="" alt="Graph"  /></p>
<p>As opposed to $$ $3$:</p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph6.png" type="" alt="Graph"  /></p>
<p>We can see here that the gradients are shrinking.</p>
</li>
</ul>
<h3 id="weight-gradient-distribution-on-parameters">Weight-Gradient Distribution on Parameters<a hidden class="anchor" aria-hidden="true" href="#weight-gradient-distribution-on-parameters">¶</a></h3>
<p>What are we checking:</p>
<ul>
<li>The std should be similar across layers</li>
<li>The mean should be similar across layers</li>
<li>The grad:data ratio should be similar across layers</li>
</ul>
<p>Grad:data ratio gives us an intuition of what is the scale of the gradient compared to the actual values. This is important because we will be taking a step update of the form <code>w = w - lr * grad</code>. If the gradient is too large compared to the actual values, we will be overshooting the minimum. If the gradient is too small compared to the actual values, we will be taking too many steps to reach the minimum.</p>
<p>The std of the gradient is a measure of how much the gradient changes across the weights. If the std for a layer is too different from the std of the other layers, this will be an issue because this layer will be learning at a different rate than the other layers.</p>
<p>This is for initialization phase. If we let the network train for a while, it will fix this issue itself. Nevertheless, this is an issue especially if we are using a simple optimizer like SGD. If we are using an optimizer like Adam, this issue will be fixed automatically.</p>
<p>Here are examples;</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-9-1"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-1">1</a>
</span><span class="lnt" id="hl-9-2"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-2">2</a>
</span><span class="lnt" id="hl-9-3"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-3">3</a>
</span><span class="lnt" id="hl-9-4"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-4">4</a>
</span><span class="lnt" id="hl-9-5"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-5">5</a>
</span><span class="lnt" id="hl-9-6"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-6">6</a>
</span><span class="lnt" id="hl-9-7"><a style="outline: none; text-decoration:none; color:inherit" href="#hl-9-7">7</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">33</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">+</span><span class="mf">0.000207</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">3.741454e-03</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">3.559389e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">+</span><span class="mf">0.000027</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">2.011833e-03</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">4.706446e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000008</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">1.438244e-03</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">4.848074e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000005</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">9.275978e-04</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">3.078747e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000007</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">7.061330e-04</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">2.373874e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">-</span><span class="mf">0.000000</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">5.087151e-04</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">1.693161e-03</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">33</span><span class="p">])</span> <span class="o">-</span> <span class="n">mean</span> <span class="o">+</span><span class="mf">0.000000</span> <span class="o">-</span> <span class="n">std</span> <span class="mf">2.043289e-02</span> <span class="o">-</span> <span class="n">grad</span><span class="p">:</span><span class="n">data</span> <span class="n">ratio</span> <span class="mf">2.027916e+00</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph7.png" type="" alt="Graph"  /></p>
<p>We can see that the ratio of the last layer is way too large, as well as its standard deviation. Which is why the pink line on the graph is too wide.</p>
<h3 id="update-ratio">Update Ratio<a hidden class="anchor" aria-hidden="true" href="#update-ratio">¶</a></h3>
<p>We calculate the update std’s ratio with real value, and this gives us a measure for learning rate. Roughly the layers are all should be around <code>-3</code></p>
<p><img loading="lazy" src="/posts/blog_posts/makemore3/images/graph8.png" type="" alt="Graph"  /></p>
<p>The formula is for each epoch: <code>[(lr * p.grad.std() / p.data.std()).log().item() for p in params]</code>.</p>


  </div>

  <footer class="post-footer">
<nav class="paginav">
  <a class="next" href="https://bedirtapkan.com/posts/blog_posts/average_reward/">
    <span class="title">Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12" style="user-select: text;"></line><polyline points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg>
    </span>
    <br>
    <span>Average Reward, Continuing Tasks and Discounting</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
  <span>© 2022</span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied!';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

</body>

</html>
