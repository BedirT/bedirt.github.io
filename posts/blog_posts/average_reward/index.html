<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Average Reward, Continuing Tasks and Discounting | Bedir Tapkan</title><meta name=keywords content="Reinforcement Learning,Control,SARSA,BetterRL"><meta name=description content="Prime numbers are really important, how do we find them though?"><meta name=author content="Bedir Tapkan"><link rel=canonical href=https://bedirtapkan.com/posts/blog_posts/average_reward/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ea30473f8638b6ede78b12b5b77671ac6ac713c2748b939941d6000e5fbd3531.css integrity="sha256-6jBHP4Y4tu3nixK1t3ZxrGrHE8J0i5OZQdYADl+9NTE=" rel="preload stylesheet" as=style><link rel=icon href=https://bedirtapkan.com/favicon.ico><link rel=apple-touch-icon href=https://bedirtapkan.com/apple-touch-icon.png><meta name=twitter:title content="Average Reward, Continuing Tasks and Discounting | Bedir Tapkan"><meta name=twitter:description content="Prime numbers are really important, how do we find them though?"><meta property="og:title" content="Average Reward, Continuing Tasks and Discounting | Bedir Tapkan"><meta property="og:description" content="Prime numbers are really important, how do we find them though?"><meta property="og:type" content="article"><meta property="og:url" content="https://bedirtapkan.com/posts/blog_posts/average_reward/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-03-11T00:00:00+00:00"><meta property="article:modified_time" content="2020-03-11T00:00:00+00:00"><meta property="og:site_name" content="Bedir Tapkan"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bedirtapkan.com/posts/"},{"@type":"ListItem","position":3,"name":"Average Reward, Continuing Tasks and Discounting","item":"https://bedirtapkan.com/posts/blog_posts/average_reward/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Average Reward, Continuing Tasks and Discounting | Bedir Tapkan","name":"Average Reward, Continuing Tasks and Discounting","description":"Prime numbers are really important, how do we find them though?","keywords":["Reinforcement Learning","Control","SARSA","BetterRL"],"wordCount":"1321","inLanguage":"en","datePublished":"2020-03-11T00:00:00Z","dateModified":"2020-03-11T00:00:00Z","author":{"@type":"Person","name":"Bedir Tapkan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bedirtapkan.com/posts/blog_posts/average_reward/"},"publisher":{"@type":"Organization","name":"Bedir Tapkan","logo":{"@type":"ImageObject","url":"https://bedirtapkan.com/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://bedirtapkan.com accesskey=h title="Bedir Tapkan (Alt + H)">Bedir Tapkan</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://bedirtapkan.com/about/ title=About>About</a></li><li><a href=https://bedirtapkan.com/projects/ title=Projects>Projects</a></li><li><a href=https://bedirtapkan.com/archives/ title=Archives>Archives</a></li><li><a href=https://bedirtapkan.com/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=https://bedirtapkan.com/design/ title="3D Projects">3D Projects</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://bedirtapkan.com>Home</a>&nbsp;Â»&nbsp;<a href=https://bedirtapkan.com/posts/>Posts</a></div><h1 class=post-title>Average Reward, Continuing Tasks and Discounting</h1><div class=post-description>Prime numbers are really important, how do we find them though?</div><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>March 11, 2020</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=https://bedirtapkan.com/tags/reinforcement-learning/>Reinforcement Learning</a><a href=https://bedirtapkan.com/tags/control/>Control</a><a href=https://bedirtapkan.com/tags/sarsa/>SARSA</a><a href=https://bedirtapkan.com/tags/betterrl/>BetterRL</a></span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>1321 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>7 min</span></span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#prerequisites>Prerequisites</a></li></ul></li><li><a href=#what-is-continuous>What is continuous?</a></li><li><a href=#why-is-discounting-not-applicable-for-continuing-tasks>Why is discounting not applicable for continuing tasks?</a></li><li><a href=#the-remedy-average-reward>The Remedy: Average Reward</a><ul><li><a href=#quick-math>Quick Math</a></li></ul></li><li><a href=#differential-semi-gradient-sarsa>Differential Semi-Gradient SARSA</a></li></ul></nav></div></details></div><div class=post-content><h3 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>Â¶</a></h3><ul><li>Intro to Linear Methods</li><li>Semi-Gradient Prediction</li><li><a href=https://bedirt.github.io/reinforcement%20learning/control%20methods/2020/03/10/Semi-Gradient-Control.html>Semi-Gradient SARSA</a></li></ul><h2 id=what-is-continuous>What is continuous?<a hidden class=anchor aria-hidden=true href=#what-is-continuous>Â¶</a></h2><p>Let&rsquo;s first describe the main task we will be handling; continuity. Continuous problems are tasks that has no specific terminal state, therefor will go on forever. As simple as it sounds, it is not a piece of cake to tackle the issues it brings with itself. Some examples could be the stock-market, where there is no end and you keep getting data. Or as book suggests access-control queuing task (Example 10.2).</p><p>I will follow a simple format so that we all can stay on the same page and everything is clear cut:</p><ul><li>Why is discounting not applicable for continuing tasks?</li><li>The Remedy: Average Reward<ul><li>Logic behind it</li><li>Why is it true: Math</li></ul></li><li>Differential Semi-Gradient SARSA</li></ul><p>So let&rsquo;s start.</p><h2 id=why-is-discounting-not-applicable-for-continuing-tasks>Why is discounting not applicable for continuing tasks?<a hidden class=anchor aria-hidden=true href=#why-is-discounting-not-applicable-for-continuing-tasks>Â¶</a></h2><p>First of all, we should know that discounting works well for tabular cases. The issue we will be talking about rises when we start to use approximations.</p><p>We have a sequence of episodes that has no beginning or end, and no way to clearly distinguish them. As the book suggests, we have the feature vectors to maybe have a use of, but then the issue of <strong>clearly seperable</strong> arises. We might have two feature vectors that has no to little difference between them, which won&rsquo;t be possible to be able to distinguish.</p><p>Since we have no start point or end point, and since there is no clear line in between episodes, using discounting is not possible. Well it actually is possible. But it is not needed. Actually using $\gamma = 0$ will give the same results as any other one. That&rsquo;s because the discounted rewards are proportional to average reward. That&rsquo;s why instead we will only use average reward. Here I will put the proof that both will results in the same order (discounted and without discounting):</p><p><img loading=lazy src=/posts/blog_posts/average_reward/images/discounting_proof.png type alt></p><p>The main issue with discounting in the approximation cases is that, since we have states depending on the same features, we do not have the <strong>policy improvement theorem</strong> anymore. Which was stating that we can get the optimal policy, just by changing all the action selections to the optimal ones for each state. Since we could choose the probabilities for one state without effecting the others it was pretty easy to handle. Now that we lost that property there is no guaranteed improvement over policy.</p><p>As Rich puts it <em>&ldquo;This is an area with multiple open theoretical questions&rdquo;</em>. If you are interested.</p><h2 id=the-remedy-average-reward>The Remedy: Average Reward<a hidden class=anchor aria-hidden=true href=#the-remedy-average-reward>Â¶</a></h2><p>Average reward is a pretty popular technique used in dynamic programming. Later on included into the Reinforcement Learning setting. We use average reward for approximated continual setting as we discussed above. Without discounting means that we care about each reward equally without thinking of if it occurs in far future etc.</p><p>We denote it as $r(\pi)$. Not much detail but for the intuition part I will give the main definition for it:
$$
r(\pi) \doteq \sum_{s}\mu_\pi\sum_{a}\pi(a|s)\sum_{r, s&rsquo;}p(r, s&rsquo;|s, a) r
$$
Basically we consider the best policy as the policy which has the most $r(\pi)$. For average reward we define returns as the difference between the $r(\pi)$ and the reward received at that point, this is called the differential return:
$$
G_t = R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + \ldots
$$
I believe differential return holds almost all the properties normal returns had. Only change we will do is to replace the reward with the difference i.e. $R_{t+1} - r(\pi)$. This goes for TD errors, bellman equations etc.</p><h3 id=quick-math>Quick Math<a hidden class=anchor aria-hidden=true href=#quick-math>Â¶</a></h3><p>So we already saw the formula for $r(\pi)$ but we didn&rsquo;t actually see how it came to existence or what all those things mean.
$$
r(\pi) \doteq \lim_{h\rightarrow\infty} \frac{1}{h} \sum_{t=1}^{h}\mathbb{E}[R_t|S_0, A_{0:t-1} \sim \pi]
$$
Let&rsquo;s explain what&rsquo;s happening here. We are assuming we have $h$ number of rewards, we are summing expected value of all the rewards given the first state and the action trajectory following the policy $\pi$, and we are dividing it to $h$ to get to the average of these rewards. So we simply had $h$ many rewards and we got the average. Then;
$$
= \lim_{t\rightarrow\infty} \mathbb{E}[R_t|S_0, A_{0:t-1} \sim \pi]
$$
Since I have the expectation inside the summation, we can actually simplify the summation with the division. We do have to put $t\rightarrow\infty$ to ccorrect the formula, as we will have number of samples approaching infinity. Next jump on the book seems fuzzy, but when you open it up it is extremely easy to see how it happens.</p><p>So if we have a randomness over something, what we want to do is to get the expectation of it. If we get the expectation that means we can formulate it, therefor no more randomness. In an MDP we have three kind of randomness possibly can happen.</p><ul><li>States are random</li><li>Actions are random</li><li>Dynamics are random</li></ul><p>What does this mean? It means we can be in a state, and we don&rsquo;t know what state that might be, and from there we will take an action, but we don&rsquo;t know for sure which action will that be. And the last one is that we take that action but since we don&rsquo;t know the dynamics of the environment (if stochastic even if we do know) we don&rsquo;t know which state we will end up in. So actually this formula goes like;
$$
\mathbb{E}[\mathbb{E} [ \mathbb{E}[R_t|S_t, A_t]]]
$$
Where the inner most is for the states and in the middle its the actions, the last one is the dynamics. So we know from bellman equations how to write this down;
$$
\mathbb{E}[R_t] = \sum_{s&rsquo;,r}p(s&rsquo;,r|s, a) r
$$
This is the expected reward is it not ? Now lets add the action selection on top:
$$
\mathbb{E}[R_t|A_t] = \sum_{a}\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s, a) r
$$
One last thing left is the state selection. We are using $\mu_\pi(s)$ to specify state distribution given the state (which the book covered earlier - Chapter 9). So the last piece of the puzzle;
$$
\mathbb{E}[R_t|A_t, S_t] = \sum_{s}\mu_\pi(s)\sum_{a}\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s, a) r
$$
That&rsquo;s all, we therefor have the average reward formula covered.</p><p>In practice we will be using moving mean to calculate average reward.</p><h2 id=differential-semi-gradient-sarsa>Differential Semi-Gradient SARSA<a hidden class=anchor aria-hidden=true href=#differential-semi-gradient-sarsa>Â¶</a></h2><p>Well, I don&rsquo;t really have much to add. If you read the <a href=https://bedirt.github.io/reinforcement%20learning/control%20methods/2020/03/10/Semi-Gradient-Control.html>Semi-Gradient SARSA</a> post, this is mostly just changing the update rule for the continuous setting. That will be the change for $G_{t:t+n}$.</p><p>$$G_{t:t+n}=R_{t+1}-\bar{R}_{t+1}$$</p><p>$$+R_{t+2}-\bar{R}_{t+2}$$</p><p>$$+\ldots+ R_{t+n}-\bar{R}_{t+n}$$</p><p>$$+\hat{q}(S_{t+n},A_{t+n},w_{t+n-1})$$</p><p>The TD error then will be like:</p><p>$$
\delta_t = G_{t:t+n} - \hat{q}(S_t, A_t, w)
$$</p><p>and we will use another step size parameter $\beta$ to update the average reward value. Here is the pseudocode:</p><p><img loading=lazy src=/posts/blog_posts/average_reward/images/sarsa_differential_pseudo.png type alt></p><p>And here is my implementation of it, which does not require much explanation I assume:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-12>12</a>
</span><span class=lnt id=hl-0-13><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-13>13</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>observations</span><span class=p>,</span> <span class=n>actions</span><span class=p>,</span> <span class=n>rewards</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  	<span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>observations</span><span class=p>)</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>n</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>observations</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>actions</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>rewards</span><span class=p>)</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>n</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>G</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>([(</span><span class=n>r</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>avg_rew</span><span class=p>)</span> <span class=k>for</span> <span class=n>r</span> <span class=ow>in</span> <span class=n>rewards</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>        <span class=n>G</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_q_hat</span><span class=p>(</span><span class=n>observations</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>actions</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>delta</span> <span class=o>=</span> <span class=n>G</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>_q_hat</span><span class=p>(</span><span class=n>observations</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>actions</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>avg_rew</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>delta</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>w</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>delta</span> <span class=o>*</span> 
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>_grad_q_hat</span><span class=p>(</span><span class=n>observations</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>actions</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>It is basically almost the same with the previous version. We are first checking if we have more elements than $n$ which means we need to remove the first elements from the storage. Then we have a check which sees if we have enough elements, because we won&rsquo;t be making any updates if there is not at least $n$ elements in the trajectory. The rest is the same update as in the pseudocode.</p><p>Again we run an experiment using the same settings as before which results in a high varience learning, thought it does learn which is the point here right now ð.</p><p><img loading=lazy src=/posts/blog_posts/average_reward/images/sarsa_differential_figure.jpg type alt></p><p>I have a blog series on RL algorithms that you can <a href=https://bedirt.github.io/tags/#betterrl-series>check out</a>. Also you can check <a href=https://github.com/BedirT/BetterRL>BetterRL</a> where I share raw python RL code for both environments and algorithms. Any comments are appreciated!</p><p><a href=https://github.com/BedirT/BetterRL/blob/master/value_based/Semi_Gradient_differential_SARSA.py>For full code</a></p></div><footer class=post-footer><nav class=paginav><a class=next href=https://bedirtapkan.com/posts/blog_posts/semi_gradient_control/><span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>Semi-Gradient Control Methods</span></a></nav></footer></article></main><footer class=footer><span>Â© 2022</span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a></span>
<span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script>
<script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a=""=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>