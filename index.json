[{"content":"Github Link\nTools Used: Python, C++, Tensorflow, Open-Spiel, Pandas, Numpy, Matplotlib, PyGame, PyDot, Tkinter\nIntroduction Dark Hex is an imperfect information version of the Hex. Dark Hex is a phantom game, where a player have a chance to play concecutively. Due to this property Dark Hex is an extremely huge game. Which makes it really hard to train agents, and develop algorithms for.\nIn this project I implemented certain tools to help generate new strategies, and come up with better players eventually. The project includes my thesis work, along side all the results and experiments I have done. We have the best known players implemented (that we developed) as well as the methods we used to train them.\nWe base the tools on DeepMind\u0026rsquo;s Open-Spiel library, most of the game specific functions are used from there. I am yet to finish documentation for the project, but I will be adding it soon.\nWe heavily rely on MCCFR and NFSP algorithms for the training phase.\nTools MCCFR implementation for training agents in Dark Hex. PolGen: A UI-based tool to generate complete policies for the agents. When we are creating new strategies/policies, we need to make sure we covered all the cases possible. PolGen helps us do that, and makes it even easier with extra features we added. Tree Generator: A tool to generate the game tree for two players. This is a crucial part of evaluating and understanding the agent behaviour. We can use this tool to generate the game tree for any given state, and see the probabilities of the agent choosing a certain action. SimPly/SimPly+: The algorithms we used to train our agents. The details of the algorithm can be found in the thesis. More tools are on the previous version. I am still cleaning up and organizing the code, and will be adding them soon. ","permalink":"https://bedirtapkan.com/projects/dark_hex/","summary":"Github Link\nTools Used: Python, C++, Tensorflow, Open-Spiel, Pandas, Numpy, Matplotlib, PyGame, PyDot, Tkinter\nIntroduction Dark Hex is an imperfect information version of the Hex. Dark Hex is a phantom game, where a player have a chance to play concecutively. Due to this property Dark Hex is an extremely huge game. Which makes it really hard to train agents, and develop algorithms for.\nIn this project I implemented certain tools to help generate new strategies, and come up with better players eventually.","title":"Dark Hex"},{"content":"Prerequisites Intro to Linear Methods Semi-Gradient Prediction Semi-Gradient SARSA What is continuous? Let\u0026rsquo;s first describe the main task we will be handling; continuity. Continuous problems are tasks that has no specific terminal state, therefor will go on forever. As simple as it sounds, it is not a piece of cake to tackle the issues it brings with itself. Some examples could be the stock-market, where there is no end and you keep getting data. Or as book suggests access-control queuing task (Example 10.2).\nI will follow a simple format so that we all can stay on the same page and everything is clear cut:\nWhy is discounting not applicable for continuing tasks? The Remedy: Average Reward Logic behind it Why is it true: Math Differential Semi-Gradient SARSA So let\u0026rsquo;s start.\nWhy is discounting not applicable for continuing tasks? First of all, we should know that discounting works well for tabular cases. The issue we will be talking about rises when we start to use approximations.\nWe have a sequence of episodes that has no beginning or end, and no way to clearly distinguish them. As the book suggests, we have the feature vectors to maybe have a use of, but then the issue of clearly seperable arises. We might have two feature vectors that has no to little difference between them, which won\u0026rsquo;t be possible to be able to distinguish.\nSince we have no start point or end point, and since there is no clear line in between episodes, using discounting is not possible. Well it actually is possible. But it is not needed. Actually using $\\gamma = 0$ will give the same results as any other one. That\u0026rsquo;s because the discounted rewards are proportional to average reward. That\u0026rsquo;s why instead we will only use average reward. Here I will put the proof that both will results in the same order (discounted and without discounting):\nThe main issue with discounting in the approximation cases is that, since we have states depending on the same features, we do not have the policy improvement theorem anymore. Which was stating that we can get the optimal policy, just by changing all the action selections to the optimal ones for each state. Since we could choose the probabilities for one state without effecting the others it was pretty easy to handle. Now that we lost that property there is no guaranteed improvement over policy.\nAs Rich puts it \u0026ldquo;This is an area with multiple open theoretical questions\u0026rdquo;. If you are interested.\nThe Remedy: Average Reward Average reward is a pretty popular technique used in dynamic programming. Later on included into the Reinforcement Learning setting. We use average reward for approximated continual setting as we discussed above. Without discounting means that we care about each reward equally without thinking of if it occurs in far future etc.\nWe denote it as $r(\\pi)$. Not much detail but for the intuition part I will give the main definition for it: $$ r(\\pi) \\doteq \\sum_{s}\\mu_\\pi\\sum_{a}\\pi(a|s)\\sum_{r, s\u0026rsquo;}p(r, s\u0026rsquo;|s, a) r $$ Basically we consider the best policy as the policy which has the most $r(\\pi)$. For average reward we define returns as the difference between the $r(\\pi)$ and the reward received at that point, this is called the differential return: $$ G_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + \\ldots $$ I believe differential return holds almost all the properties normal returns had. Only change we will do is to replace the reward with the difference i.e. $R_{t+1} - r(\\pi)$. This goes for TD errors, bellman equations etc.\nQuick Math So we already saw the formula for $r(\\pi)$ but we didn\u0026rsquo;t actually see how it came to existence or what all those things mean. $$ r(\\pi) \\doteq \\lim_{h\\rightarrow\\infty} \\frac{1}{h} \\sum_{t=1}^{h}\\mathbb{E}[R_t|S_0, A_{0:t-1} \\sim \\pi] $$ Let\u0026rsquo;s explain what\u0026rsquo;s happening here. We are assuming we have $h$ number of rewards, we are summing expected value of all the rewards given the first state and the action trajectory following the policy $\\pi$, and we are dividing it to $h$ to get to the average of these rewards. So we simply had $h$ many rewards and we got the average. Then; $$ = \\lim_{t\\rightarrow\\infty} \\mathbb{E}[R_t|S_0, A_{0:t-1} \\sim \\pi] $$ Since I have the expectation inside the summation, we can actually simplify the summation with the division. We do have to put $t\\rightarrow\\infty$ to ccorrect the formula, as we will have number of samples approaching infinity. Next jump on the book seems fuzzy, but when you open it up it is extremely easy to see how it happens.\nSo if we have a randomness over something, what we want to do is to get the expectation of it. If we get the expectation that means we can formulate it, therefor no more randomness. In an MDP we have three kind of randomness possibly can happen.\nStates are random Actions are random Dynamics are random What does this mean? It means we can be in a state, and we don\u0026rsquo;t know what state that might be, and from there we will take an action, but we don\u0026rsquo;t know for sure which action will that be. And the last one is that we take that action but since we don\u0026rsquo;t know the dynamics of the environment (if stochastic even if we do know) we don\u0026rsquo;t know which state we will end up in. So actually this formula goes like; $$ \\mathbb{E}[\\mathbb{E} [ \\mathbb{E}[R_t|S_t, A_t]]] $$ Where the inner most is for the states and in the middle its the actions, the last one is the dynamics. So we know from bellman equations how to write this down; $$ \\mathbb{E}[R_t] = \\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s, a) r $$ This is the expected reward is it not ? Now lets add the action selection on top: $$ \\mathbb{E}[R_t|A_t] = \\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s, a) r $$ One last thing left is the state selection. We are using $\\mu_\\pi(s)$ to specify state distribution given the state (which the book covered earlier - Chapter 9). So the last piece of the puzzle; $$ \\mathbb{E}[R_t|A_t, S_t] = \\sum_{s}\\mu_\\pi(s)\\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s, a) r $$ That\u0026rsquo;s all, we therefor have the average reward formula covered.\nIn practice we will be using moving mean to calculate average reward.\nDifferential Semi-Gradient SARSA Well, I don\u0026rsquo;t really have much to add. If you read the Semi-Gradient SARSA post, this is mostly just changing the update rule for the continuous setting. That will be the change for $G_{t:t+n}$.\n$$G_{t:t+n}=R_{t+1}-\\bar{R}_{t+1}$$\n$$+R_{t+2}-\\bar{R}_{t+2}$$\n$$+\\ldots+ R_{t+n}-\\bar{R}_{t+n}$$\n$$+\\hat{q}(S_{t+n},A_{t+n},w_{t+n-1})$$\nThe TD error then will be like:\n$$ \\delta_t = G_{t:t+n} - \\hat{q}(S_t, A_t, w) $$\nand we will use another step size parameter $\\beta$ to update the average reward value. Here is the pseudocode:\nAnd here is my implementation of it, which does not require much explanation I assume:\n1 2 3 4 5 6 7 8 9 10 11 12 13 def update(self, observations, actions, rewards): if len(observations) \u0026gt; self.n: observations.pop(0) rewards.pop(0) actions.pop(0) if len(rewards) == self.n: G = sum([(r - self.avg_rew) for r in rewards[:-1]]) G += self._q_hat(observations[-1], actions[-1]) delta = G - self._q_hat(observations[0], actions[0]) self.avg_rew += self.beta * delta self.w += self.alpha * delta * self._grad_q_hat(observations[0], actions[0]) It is basically almost the same with the previous version. We are first checking if we have more elements than $n$ which means we need to remove the first elements from the storage. Then we have a check which sees if we have enough elements, because we won\u0026rsquo;t be making any updates if there is not at least $n$ elements in the trajectory. The rest is the same update as in the pseudocode.\nAgain we run an experiment using the same settings as before which results in a high varience learning, thought it does learn which is the point here right now 😄.\nI have a blog series on RL algorithms that you can check out. Also you can check BetterRL where I share raw python RL code for both environments and algorithms. Any comments are appreciated!\nFor full code\n","permalink":"https://bedirtapkan.com/posts/blog_posts/average_reward/","summary":"Prerequisites Intro to Linear Methods Semi-Gradient Prediction Semi-Gradient SARSA What is continuous? Let\u0026rsquo;s first describe the main task we will be handling; continuity. Continuous problems are tasks that has no specific terminal state, therefor will go on forever. As simple as it sounds, it is not a piece of cake to tackle the issues it brings with itself. Some examples could be the stock-market, where there is no end and you keep getting data.","title":"Average Reward, Continuing Tasks and Discounting"},{"content":"Prerequisites Semi-Gradient Prediction Intro to Linear Methods If you read the prediction part for the semi gradient methods, it is pretty easy to extend what we know to the control case. We know that control is almost all the time just adding policy improvement over the prediction case. That\u0026rsquo;s exactly the case for us here for semi-gradient control methods as well.\nWe already have describe and understood a formula back in prediction part (if you read it somewhere else that\u0026rsquo;s also fine), and now we want to extend our window a little.\nFor prediction we were using $S_t \\mapsto U_t$ examples, now since we have action-values instead of state-values (because we will pick the best action possible), we will use examples of form $S_t, A_t \\mapsto U_t$ meaning that instead of $v_\\pi(S_t)$ we will be using estimations for $q_\\pi(S_t, A_t)$.\nSo our general update rule would be (following from the formula for prediction);\n$$ w_{t+1} = w_t + \\alpha [U_t - \\hat{q}(S_t, A_t, w_t)] \\nabla\\hat{q}(S_t, A_t, w_t) $$\nAs we always do, you can replace $U_t$ with any approximation method you want, so it could have been a Monte Carlo method (Though I believe this does not count as semi-gradient, because it will be a direct stochastic gradient since it does not use any bootstrapping, but the book says otherwise so I am just passing the information 😄). Therefor we can implement an $n$-step episodic SARSA with an infinite option, which will correspond to Monte-Carlo (We will learn a better method to do this in future posts).\nThe last piece of information to add is the policy improvement part, since we are doing control, we need to update our policy and make it better as we go of course. Which won\u0026rsquo;t be hard cause we will just be using a soft approximation method, I will use the classic $\\epsilon$-greedy policy.\nOne more thing to note, which I think is pretty important, for continuous action spaces, or large discrete action spaces methods for the control part is still not clear. Meaning we don\u0026rsquo;t know what is the best way to approach yet. That is if you think of a large choices of actions, there is no good way to apply a soft approximation technique for the action selection as you can imagine.\nFor the implementation, as usual we will just go linear, as it is the best way to grasp every piece of information. But first I will as usual give the pseudo-code given in the book.\nI only took the pseudocode from chapter 10.2 because we don\u0026rsquo;t really the one before, as it is only the one step version. We are interested in the general version therefor n-step.\nImplementation 1 2 3 4 5 6 7 8 9 10 11 12 def __init__(self, feature_space, action_space, alpha = 0.0001, gamma = 0.99, eps = .1): self.alpha = alpha self.gamma = gamma self.eps = eps self.feature_space = feature_space self.action_space = action_space self.reset_weights() def reset_weights(self): self.w = np.random.rand(self.feature_space * self.action_space) Initialize We start by initializing the necessary things; we need step size $\\alpha$ also $\\gamma$ and $\\epsilon$. Other then these we need to initialize our weight vector. We will have a weight vector that is for each action concatenated after one another. So if we assume that we have 4 observations lets say [1 0 1 0], meaning weights 0 and 2 are active, and if want to update the weights for action 0, we will have [1 0 1 0 0 0 0 0 0 0 0 0] if we had 3 possible actions in total. After when we are using $\\epsilon$-greedy this will make more sense.\n1 2 3 4 5 def step(self, obs): if np.random.sample() \u0026gt; self.eps: return np.argmax(self._act(obs)) else: return np.random.randint(0, self.action_space) Let\u0026rsquo;s move next thing is to take a step, meaning we will pick the action according to our action-values at hand. We take the observations as input, this will come from the environment, and assuming we get an array of the probabilities for each action given the observations from _act(obs). Then all we have to do is to roll the die and decide if we will choose a random action or we will choose the action that has the most value for the current time, and thats exactly what we do here ($\\epsilon$-greedy action selection).\n1 2 3 4 5 def _act(self, obs): q_vals = np.zeros(self.action_space) for a in range(self.action_space): q_vals[a] = self.q_hat(obs, a) return q_vals Best $\\hat{q}$-value now we need to fill the function _act(obs). Which basically will call $\\hat{q}(s, a, w)$ for each action and store them in an array and return it.\n1 2 def q_hat(self, obs, action): return self.w.T.dot(self._x(obs, action)) Continuing from there we have the $\\hat{q}(s,a,w)$ to implement. Which is just writing down the linear formula since we are implementing it linearly. Therefor $\\hat{q}(s,a,w) = w^Tx(s, a)$ where $x(s,a)$ is the state action representation. In our case as I already mention this will just be the one hot vector, all the observations are added after one another for each action.\n1 2 3 4 def _x(self, obs, action): one_hot = np.zeros_like(self.w) one_hot[action * self.feature_space:(action+1) * self.feature_space] = obs return one_hot Finally $x(s, a)$ - as I already mentioned twice 😄 we create the $x$ in a vector that everything 0 other than the active action.\nThat was the last thing for us to be able to choose the action for a given state. So let\u0026rsquo;s have a broader respective and assume that we are using the step(obs) here is how it would be like:\n1 2 action = agent.step(obs) obs, reward, done = env.step(action) Now we see what is left ? Update\u0026hellip; 🤦‍♂️ Yeah without update there is no change basically. Which will also be the one differs for the $n$. Let\u0026rsquo;s remember the formula;\n$$ w_{t+1} = w_t + \\alpha[R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^n\\hat{q}(S_{t+n},A_{t+n},w_{t}) - \\hat{q}(S_{t},A_{t},w_{t})] \\nabla\\hat{q}(S_t, A_t, w_t) $$\n1 2 3 4 5 6 7 8 9 10 11 def update(self, observations, actions, rewards): if len(observations) \u0026gt; self.n+1: observations.pop(0) rewards.pop(0) actions.pop(0) if len(rewards) == self.n+1: G = sum([(self.gamma ** t) * r for t,r in enumerate(rewards[:-1])]) G += (self.gamma ** (self.n)) * self._q_hat(observations[-1], actions[-1]) self.w += self.alpha * (G - self._q_hat(observations[0], actions[0])) * \\ self._grad_q_hat(observations[0], actions[0]) There is a bit of a change here, from the pseudocode I provided. Since we want a full seperation between the agent-environment-experiment we need a class system for the algorithms therefor we won\u0026rsquo;t be following what is on the pseudocode.\nUpdate what happens here is actually not that different, since we only need $n+1$ elements to make the update happen we won\u0026rsquo;t keep the rest of the trajectory. Whenever we use n numbered trajectory the first element becomes useless for the next update. Therefor we remove the first element from the trajectory and use the rest to make our update.\nTerminal we also have a terminal state, and as can be seen in the pseudocode there are some differences that should be changed for the updates when we reach the terminal state. Logical enough, we do not have n+1 element left to complete the calculation we were doing therefor we will just use the rewards rather than $\\hat{q}(s,a,w)$ . Therefor we need another function to handle this, which we call end() in our structure;\n1 2 3 4 5 6 7 8 9 def end(self, observations, actions, rewards): for _ in range(self.n): observations.pop(0) rewards.pop(0) actions.pop(0) G = sum([(self.gamma ** t) * r for t,r in enumerate(rewards)]) self.w += self.alpha * (G - self._q_hat(observations[0], actions[0])) * \\ self._grad_q_hat(observations[0], actions[0]) Here as we can see we are not doing something too different. It is just that we are using the last elements we have left and we will remove all the elements from the trajectory while making the last updates to our weights.\nYeah and we are almost done, exept that I didn\u0026rsquo;t show the grad_q_hat() yet, which basically gives the $\\nabla\\hat{q}(s,a,w)$.\n1 2 def grad_q_hat(self, obs, action): return self._x(obs, action) Surprise.. Yeah since we are using linear functions, $\\nabla w^Tx(s, a) = x(s,a)$. That\u0026rsquo;s all.\nLet\u0026rsquo;s see how would be the experiment part and run the code to get some results then.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 params = { \u0026#39;num_of_episodes\u0026#39; : 1000, \u0026#39;max_steps\u0026#39; : 1000, \u0026#39;alpha\u0026#39; : 2 ** (-14), \u0026#39;gamma\u0026#39; : 0.98, # Creating the tilings \u0026#39;grid_size\u0026#39; : 5, \u0026#39;tile_size\u0026#39; : 4, \u0026#39;num_of_tiles\u0026#39; : 5 } # environment env = grid_world(portal=True) action_space = env.action_space.shape[0] # tile coding tilings = tile_coding(env.grid_size[0], params[\u0026#39;num_of_tiles\u0026#39;], params[\u0026#39;tile_size\u0026#39;], action_space) state_space = tilings.num_of_tilings # Keep stats for final print and data episode_rewards = np.zeros(params[\u0026#39;num_of_episodes\u0026#39;]) # Agent created n = 8 agent = SG_SARSA(state_space, action_space, n, params[\u0026#39;alpha\u0026#39;], params[\u0026#39;gamma\u0026#39;]) np.random.seed(1) for ep in range(params[\u0026#39;num_of_episodes\u0026#39;]): rewards = [] observations = [] actions = [] obs = tilings.active_tiles(env.reset()) # a x d score = 0 for t in range(params[\u0026#39;max_steps\u0026#39;]): action = agent.step(obs) observations.append(obs) obs, reward, done = env.step(action) obs = tilings.active_tiles(obs) rewards.append(reward) actions.append(action) score += reward if done: agent.end(observations, actions, rewards) break else: agent.update(observations, actions, rewards) episode_rewards[ep] = score print(\u0026#34;EP: {} -------- Return: {} \u0026#34;.format(ep, score), end=\u0026#34;\\r\u0026#34;, flush=True) I used tile coding and the grid world environment in our library. If you want you can modify a little to use another state representation or Rich Sutton\u0026rsquo;s tile coding library, or for environment gym.\nAnyways, what we do is pretty simple if you read through, and you can ask for clarification on any point if looks weird.\nMain point here are the agent functions and how we use them, all three are used as we said, on each step we have the agent.step(), for each step we have the update() called except the terminal state. Which we will call end() instead.\nI will give only one graph as result as usual, here is 100 runs on the stochastic grid world environment.\nIf you liked this post follow BetterRL, and keep a like down below. I have a blog series on RL algorithms that you can check out. Also you can check the repo where I share raw python RL code for both environments and algorithms. Any comments are appreciated!\nFor full code\n","permalink":"https://bedirtapkan.com/posts/blog_posts/semi_gradient_control/","summary":"Prerequisites Semi-Gradient Prediction Intro to Linear Methods If you read the prediction part for the semi gradient methods, it is pretty easy to extend what we know to the control case. We know that control is almost all the time just adding policy improvement over the prediction case. That\u0026rsquo;s exactly the case for us here for semi-gradient control methods as well.\nWe already have describe and understood a formula back in prediction part (if you read it somewhere else that\u0026rsquo;s also fine), and now we want to extend our window a little.","title":"Semi-Gradient Control Methods"},{"content":"Counting sort is a nice in-place sorting algorithm that we can use for sorting instantly. (This is mostly used for competitive programming.) What I meant by this is, we can use counting sort when we are getting the input. This will not get any additional cost for us, and really good technique for using in-place sorting. I\u0026rsquo;ll get there after explaining the algorithm.\nComplexity ? Let me tell you the complexity if you are wondering, but I will explain \u0026ldquo;why\u0026rdquo; after the algorithm itself. O(n+k) : n , is size of the array that we will sort and k is the maximum element we have.\nAlgorithm OK. Let\u0026rsquo;s think of an array. For the sake of simplicity, let\u0026rsquo;s make it short\u0026hellip;\nmyArray = [2, 3, 7, 4, 3, 9] We have an array -unsorted- , minimum number is 2, maximum is 9 and we have 6 elements in the array. Alright so let\u0026rsquo;s think of one more array. Which is from 0 (array starting point) to our maximum number (9) and all the values are initially 0.\nINDEXES 0 1 2 3 4 5 6 7 8 9 weAreCounting = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ATTENTION! We are going to iterate over first array one by one, and we will increment the values that we have (i.e. for first step it is myArray[0] which is 2 so go to weAreCounting array and increment [2] by 1):\nfor i in myArray: weAreCounting[i] += 1 NOW WHAT WE HAVE AT THE END OF THIS LOOP?\nINDEXES 0 1 2 3 4 5 6 7 8 9 weAreCounting = [0, 0, 1, 2, 1, 0, 0, 1, 0, 1] So weAreCounting array basically shows us how many of these numbers we do have (i.e. we have 2 threes so weAreCounting[3] = 2).\nThis part is how you have all the items counted. Now we can use this as sorted array (with iterating over it) or we can have our new array that will have the sorted array directly. For the second version:\nMy logic will be to use same array before (myArray) so that it will be more efficient (space-wise).\nSo I iterate thorough weAreCounting and if the number is bigger than 0 I will add it into myArray. That is all of the logic.\nCode Here is the c++ code, as simplified as possible. ENJOY!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void countingSort (int * arr) { int countingArray[MAX_NUM] = {0}; for (i = 0 ; i \u0026lt; ARRAY_SIZE ; i++) countingArray[arr[i]]++; int output_Index = 0; for (i = 0 ; i \u0026lt; MAX_NUM ; i++) while ( countingArray[i]-- ) // Process will continue until the elements reach to 0 arr[output_Index++] = i; // PS: Incrementing will be after the line_process // Instead of these two lines we could use memset function too... } And here is python3.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def countingSort(arr): k = max(arr) countingArray = [0] * (k + 1) for i in arr: countingArray[i] += 1 j = 0 for i in range(k+1): while(countingArray[i] \u0026gt; 0): arr[j] = i countingArray[i] -= 1 j += 1 return arr a = countingSort([3, 4 ,5 ,1, 10, 3, 2]) If you check the complexity in the code, you will see that we have two loops, one is doing n operation (going through the array that we will sort). Second loop is doing k operation (which is the maximum number that we have in the array that we will sort). So time complexity will be O(n+k).\nThe space complexity: we have two arrays (we could have 3 but we decrease it to 3 because we used the one at the beginig two times. Since at the end we will not going to need that one.) one is size of n and one is size of k so our complexity will be O(n+k).\nNow I suggest you to go to the link below, and try to solve the questions in Week 3/ Counting Sort section. So that you will have full understanding about this question. If you like the concept of it you can star or watch out repository as well. Have a great one!\nYou can also find other algorithms explained and full code samples about this one here (Check Week 3)\n","permalink":"https://bedirtapkan.com/posts/blog_posts/counting_sort/","summary":"\u003cp\u003eCounting sort is a nice in-place sorting algorithm that we can use for sorting instantly. (This is mostly used for competitive programming.) What I meant by this is, we can use counting sort when we are getting the input. This will not get any additional cost for us, and really good technique for using in-place sorting. I\u0026rsquo;ll get there after explaining the algorithm.\u003c/p\u003e","title":"Counting Sort"},{"content":"Before starting- What is Prime Factorization ? What is a Prime number ? If you are curious about these please checkout this link before proceed because I will not explain them here :)\nWikipedia - Prime Numbers\nSince we all know what a prime number and composite number is, let\u0026rsquo;s look at our realllly simple algorithm. Actually there is nothing fancy here, we are just using simple Sieve of Eratoshenes(Hardest name to pronounce, I checked online if I am right) algorithm. By the way that topic also pre-requised for this post, but fortunetely we already have a tutorial-explanation for it. If you don\u0026rsquo;t know or confused about it in some ways please check the links below:\nSome sources to learn about Sieve of Eretosthenes\nSince \u0026ldquo;WE\u0026rdquo; covered everything required, let me involve in this learning process too\u0026hellip; Prime factorization: This is highly important topic. All your passwords , your bank accounts and stuff are protected by these numbers. Anyway that is why we actually have couple algorithms about Prime Factorization. There is a good answer on quora.com about the prime number algorithms:\nDifferent algorithms get used based on how large the number is. It goes something like this:\nSmall Numbers : Use simple sieve algorithms to create list of primes and do plain factorization. Works blazingly fast for small numbers.\nBig Numbers : Use Pollard\u0026rsquo;s rho algorithm, Shanks\u0026rsquo; square forms factorization (Thanks to Dana Jacobsen for the pointer)\nLess Than 10^25 : Use Lenstra elliptic curve factorization\nLess Than 10^100 : Use Quadratic sieve\nMore Than 10^100 : Use General number field sieve\nCurrently, in the very large integer factorization arena, GNFS is the leader. It was the winner of the RSA factoring challenge of the 232 digit number\nArun Iyer quora.com\nOK cool, we have a lot of options, although you can see that these numbers are gigantic. $10^{25}$ ?? This was the smallest one mentioned above by the way. So we don\u0026rsquo;t really care about them, they are exist because like I said before, these numbers are extremely powerful so people need biiig ones. Since our languages supports (for C++) until $10^{19}$ , and our tutorials are for ACM-ICPC kind programming contests, considering that these contests have time limit and %100 sure that if $10^{25}$ will given\u0026hellip; you probably should search for some trick in question, because we cannot compete that many operations on time.\nFinally the Algorithm Anyway after all explanation lets talk about our \u0026ldquo;small\u0026rdquo; algorithm. It really is nothing much than using Sieve algorithm. We are just going to optimize it a little bit. Let\u0026rsquo;s say we already runned our sieve function:\n1 sieve(10001); Now we have an array or vector , I don\u0026rsquo;t know how you implemented so I will go with mine -\u0026gt; you can check it out:\n1 2 3 4 5 6 7 8 9 10 11 12 vector\u0026lt;int\u0026gt; primes; void sieve(int size) { bitset\u0026lt;10000010\u0026gt; was; // You can also use boolean array or vector, but this is optimized for bool (C++ is best :) ) was.set(); // Initilizing all bitset to true was[0] = was[1] = 0;\t// Except 0 and 1 of course for (int i = 2; i \u0026lt;= size; i++) if (was[i]) { primes.push_back(i); for (int j = i * i; j \u0026lt;= size; j += i) was[j] = 0; } } We have a vector named primes and it has all the primes from begining(2) to size.\n$$primes -\u0026gt; [ 2 , 3 , 5 , 7 , 11 , 13 , 19 , 21 , \u0026hellip; ]$$\nWhat will we do is we will use basic logic and check every prime number and if it can divide our number N. If it can divide , we will just put it into our new vector (If you don\u0026rsquo;t know vector you still can use list or array, depends on the language). If we can divide we will divide it, with this way we will decrement our operations. So let\u0026rsquo;s say we have 18 as our N. We start with first element in the primes which is 2.\nIs 2 dividing N = 18 ? Yes obviously so:\nPut 2 into our Factors vector; So -\u0026gt;\n$$Factors -\u0026gt; [ 2 ]$$\nAnd we will divide our N by 2:\nN = 18/2 = 9 Continue to check if 2 is dividing N which is not becuese N = 9. So lets pass 2 and go to 3:\nIs 3 dividing N = 9 ? Yes Put 3 into our Factors vector; $$Factors -\u0026gt; [ 2 , 3 ]$$\nN = 9/3 = 3 Is 3 dividing N = 3 ? Yes Put 3 into our Factors vector; $$Factors -\u0026gt; [ 2 , 3 , 3 ]$$\nN = 3/3 = 1 Is 3 dividing N = 1 ? Nope Proceed to 5. 5 ? Yes we will stop here. This is the next optimization, at most we will go until $p^2 \\leq N$ (and p is my prime number that I am checking). This is what determines my complexity in this method. So I have $\\sqrt{N}$ here. This is also my number that will go into O notation -\u0026gt; O($\\sqrt{N}$). (Mathematically this complexity is represented with $O(\\pi(\\sqrt{N})) = O(\\sqrt{N}\\times lnN)$)You can further check the code C++ implementation. I commented it so you can see what is going on in each step. After understanding the code I highly recommend you to solve questions about this topic, we have our list for this question as well, check the link at the bottom.\nImplementation C++ 1 2 3 4 5 6 7 8 9 10 11 12 13 vector\u0026lt;int\u0026gt; primeFactors(int N){ vector\u0026lt;int\u0026gt; vc; // An empty vector for us to fill with our numbers factors. int idx = 0, f = primes[idx]; // f standing for FACTOR - idx is index that we will increment while(N != 1 \u0026amp;\u0026amp; N \u0026gt;= f * f){ // f * f ... This is the part with sqrt(N) so the loop continues until our factor is bigger than sqrt(N) while(N % f == 0){ // I will continuously check if N is divisible by this prime, until it become wrong. N /= f; // Dividing N to my prime. vc.push_back(f); // adding that prime to my vector. } } if(N != 1) vc.push_back(N); // This case is for prime numbers itself, if the number is prime than we should add it to our vector. // If some value, after our loop is still not equals to 1 than it is a prime itself. (because of sqrt(N)) return vc; } We have a well designed Curriculum on Github, also the questions about this algorithm are there too, check it out here\nACM-ICPC Curriculum\n","permalink":"https://bedirtapkan.com/posts/blog_posts/prime_factorization/","summary":"\u003cp\u003e\u003cstrong\u003eBefore starting-\u003c/strong\u003e What is Prime Factorization ? What is a Prime number ? If you are curious about these please checkout this link before proceed because I will not explain them here :)\u003c/p\u003e","title":"Factorizing a Number"},{"content":"What we will learn today is, how to find the , in optimal solution. First, let\u0026rsquo;s clarify the goal a bit.\nWhat is subarray? Subarray is an array that is included in the bigger array. So if we have an array that has 7 elements in it. What we have is elements that have indexes of: 0, 1, 2, 3, 4, 5, 6 . A subarray is smaller array inside of this big array. So for example 1, 2, 3 or 4, 5 are subarrays. But 1, 3 is not a subarray because the subarray should be contiguous. So our task is to find the largest contiguous array in our big array.\nSince we clarify our objective let\u0026rsquo;s look at the solutions we have. First let\u0026rsquo;s see what will be the brute force solution since that will be the first one which comes to mind. What we would do is, we would start from 0 index and hold it, check every elements before that index and keep the largest one. So if we have [3, 5, 7, 9] in the array. We would first check the 3. We would see that it is the largest subarray since there is none other than that. And than we would check the index 1 -\u0026gt; 5, we have 5 and 5 + 3 = 8. The bigger one is 8 so we keep 8. Than we go for 2nd index -\u0026gt; 7. We have 7, 7+5, 7+5+3 so that biggest one will be 7+5+3 which is 15, we keep it. Then next one : index 3 -\u0026gt; 9. We have 9, 9+7, 9+7+5, 9+7+5+3. Largest one will be 9+7+5+3 = 24. So we compare the ones we found as sum of subarrays and the greatest one will be 24, the last one we checked, that\u0026rsquo;s because we have no negative elements in the array. Anyways that would be the brute force solution and still a smart one. But the time complexity would be O(n^2). Since we take the index and check every others that we can combine with this index. Let me visualize this one:\nOK, we got this part. So we got the question, now what is the optimal solution for this problem. What is this guy , Kadane , found. Here is the algorithm then. This algorithm is dynamic, which means we will approach the result using the ones we find before. OK, this guy teaches us a way that has complexity O(n), linear time.\nLet\u0026rsquo;s go with an example so it will be more clear. Our array is [5, -2, -4, 4, 4]. Kadane says that in each iteration we have only two options to get the max subarray:\nIt can be only itself It can be itself combined with the maximum subarray that previous index has. Man, this is a smart solution. OK, what he says is let\u0026rsquo;s say we calculated the sum of maximum subarrays until index 1 which has value -2. For the sake of understanding let\u0026rsquo;s calculate with brute force. We have -2 and -2+5. The greater one is -2+5 = 3. So let\u0026rsquo;s proceed. This time let\u0026rsquo;s use Kadane\u0026rsquo;s Algorithm for calculating the 3rd step. What are the options:\nIt can be only itself (We have -4 as the 2nd indexed value so = -4) It can be itself combined with the maximum subarray that previous index has. (We have 3 as 1st index\u0026rsquo;s max Subarray sum, so = 3-4 = -1) So we have a -1 and -4 \u0026hellip; -1 indeed. But is it always the second option then? Let\u0026rsquo;s see with another part of our array. Let\u0026rsquo;s proceed one more step. Don\u0026rsquo;t forget that we have -1 as our current max. 3rd index -\u0026gt;\nIt can be only itself (We have 4 as the value = 4) It can be itself combined with the maximum subarray that previous index has. (We have -1 as the previous index\u0026rsquo;s maximum -\u0026gt; -1+4=3) Now we approached the first option, 4 \u0026gt; 3 so we will keep 4 instead of 3. And repeat this until the end \u0026hellip; Really that\u0026rsquo;s all.\nNow that we understand the logic. Let\u0026rsquo;s proceed to the code. I will give pseudocode here.\nkadane(Array){ generalMaximum = currentMaximum = Array[0] for (i = 1 until n) { currentMaximum = maximum of(Array[i], currentMaximum + Array[i]); if(currentMaximum \u0026gt;= generalMaximum) generalMaximum = currentMaximum; } return generalMaximum; } If you are interested on learning or practicing more algorithms, you can visit our curriculum from github ACM-ICPC Preparation. There are also questions and source code\u0026rsquo;s about this topic. ENJOY!\n","permalink":"https://bedirtapkan.com/posts/blog_posts/kadane/","summary":"\u003cp\u003eWhat we will learn today is, how to find the , in optimal solution. First, let\u0026rsquo;s clarify the goal a bit.\u003c/p\u003e","title":"Kadane's Algorithm"},{"content":"Reaaally introductory post for Machine Learning basics. What is Supervised and Unsupervised learning?\nSupervised Learning Supervised learning is the most common machine learning problem. In Supervised Learning we already know what the correct output should be like.\nThere are two categories for supervised learning, first one is \u0026ldquo;regression problem\u0026rdquo; and the second one is \u0026ldquo;classification problem\u0026rdquo;. I will explain both with examples.\nRegression Problems In regression we have an output that is continous and we are trying to predict what will be the correct answer/output/label for our input. Lets see an example to understand the concept better.\nExamples: Let\u0026rsquo;s say we have a friend who has a chocalate company. He has a lot of money and he wants to make his product sell as many as Snickers. OK. But his chocalates are not famous as Snickers. Now , what he should do is, take a look at the competitor. There is a chart which has two dimensions. One is the price. Another is the popularity. Now that since we have continous output for the prices. We will predict the one that we are looking for. (I will just give the popularities according to myself.)\nNow, looking at this output. What should we do is, putting a straight or polinomial line to the outputs.\nThen we will have our line that will help us to predict the price. According to the surveys, our chocalates have 8 point for the popularity. So what will be the best price according to the survey and the industry\u0026hellip;\nIt seems something like 70¢ \u0026hellip;\nThis is the regression problem \u0026hellip;\nClassification Problems In classification, the simplest one, binary classification, we have two options, either true or false. We also can say that we will predict the result in a binary map. Let\u0026rsquo;s check an example.\nExamples: Let\u0026rsquo;s give an absurd example so that it will be more permament. So we have a friend who just ate 5 kilos of Nutella and he is 24 years old. We want to predict if he will get sick or not. And we have a dataset that have people\u0026rsquo;s ages that ate 5 kilos of Nutella and got the sick or not !!\nSo according to this graph our friend will get sick or not. It is a binary example. There is just two probabalities. This is a classification problem. Let\u0026rsquo;s see the expected result \u0026hellip;\n(He will probably get sick, according to our prediction.)\nUnsupervised Learning The unsupervised learning is the second most common machine learning problem. In unsupervised learning we don\u0026rsquo;t know the result for each input. We will obtain a structure form the data. We do not know what are the exact effects of our inputs. We will use clustering for this.\nClustering Problem We basically will seperate the data according to variables.\nLet\u0026rsquo;s say you got hundred different composition classes\u0026rsquo; final essays. They all have different topics. What clustering do is, classifying all the essays according to their topics. So that if we use clustering, all these classes\u0026rsquo; articles will be separated. This is just one variable (topic). If you want, you can add more variables to make the groups more specific. In this case we can add words count for example.\n","permalink":"https://bedirtapkan.com/posts/blog_posts/ml_basics/","summary":"\u003cp\u003eReaaally introductory post for Machine Learning basics. What is Supervised and Unsupervised learning?\u003c/p\u003e","title":"Machine Learning Basics"},{"content":"\nHi! Let me talk a bit about myself, without boring you I hope. I am a Machine Learning engineer \u0026amp; researcher, currently focusing on Reinforcement Learning and Game Theory.\nProfessional Me Now I have recently finished my Masters degree at University of Alberta where I was supervised by Martin Mueller and Ryan Hayward. My thesis titled Dark Hex: A Large Scale Imperfect Information Game was on a zero-sum imperfect information game called Dark Hex. I have also worked on some open source projects including DeepMinds very own OpenSpiel. We are expecting multiple papers to be published soon!\nProfessional Me Before Let me mention my background a little more! I have started my programming career, and interest a year before my undergrad with mobile app development. I have learned Swift and iOS development and have completed multiple projects with multiple teams. After the first year I have shifted my interest on competetive programming. This marks also the time I got really involved with my community and got managing positions on my universities ACM branch. I have then participated ACM-ICPC competitions multiple times, and thought lectures at our ACM branch to people who were also interested in competitive programming. I have created an open source curriculum for studying competitive programming and coding questions in general that received a lot of interest from the community (ACM-ICPC Curriculum).\nLast 2 years of my undergrad was where I got really interested in Machine Learning. I have taken multiple courses on Machine Learning and Deep Learning, and have also taken a course on Reinforcement Learning. I got a gold medal on HackHouston hackathon with a successful machine learning project (MLRPro). With this new interest, I started looking for more opportunities to develop my skills and hopefully apply them on a real world project. NASA gave me an oppotunity at that moment, I have worked with a professional team, and brought a real-life working product under their tool belt.\nI applied for Masters degree at University of Alberta due to my extreme interest in Reinforcement Learning, and got accepted. During my wait I have got a short Computer Science teaching position.\nWhen I got to UofA, I took on a project with Martha White on Co-Agent Networks. After a while I discovered my interest on Imperfect Information Games, or Partially Observable Environments. I then started working on Dark Hex, and got supervised by Martin Mueller and Ryan Hayward. During this period I have learned a lot on Game Theory, and took Advanced Reinforcement Learning class from the legend himself, Richard Sutton.\nDuring these times I have had a lot of teaching experience. I TAed many times, I thought many classes voluntarily as well. I have thought Reinforcement Learning, Machine Learning, Python, C++ and many more.\nFor the past year, I took up Blender and learned 3D modelling and sculpting. For the past couple months I also have learned Unreal Engine, and have been working on it since.\nPersonal Me Other than my professional background: I love going outdoors; the fresh air and the quiet is just phenomenal. I love reading, especially philosophy, you can follow me on my goodreads. I enjoy baking, I am pretty sure my friends do too. I love playing soccer and volleyball when I get the chance. Time to time I play some Dota 2 or FIFA with some friends. I also did a lot of professional Graphic Designing during my undergrad. I still do it as a hobby, I am specialized on logo design and branding. I have a Behance account if you are interested.\nI am recently working on my youtube channels, one for my blender projects where I sculpt and model, and the other is for lecture series I have been developing on certain expertise I have. Currently three series are on the way: Introduction to Multi-Agent Reinforcement Learning, Imperfect Information Games, and Single Agent Reinforcement Learning.\n","permalink":"https://bedirtapkan.com/about/","summary":"Hi! Let me talk a bit about myself, without boring you I hope. I am a Machine Learning engineer \u0026amp; researcher, currently focusing on Reinforcement Learning and Game Theory.\nProfessional Me Now I have recently finished my Masters degree at University of Alberta where I was supervised by Martin Mueller and Ryan Hayward. My thesis titled Dark Hex: A Large Scale Imperfect Information Game was on a zero-sum imperfect information game called Dark Hex.","title":"About"},{"content":"Just got my HUION 22 Pro a couple weeks ago, had to jump in and make something interesting. Didn\u0026rsquo;t think much :) Just pen on the tablet!\nDidn\u0026rsquo;t spend much time texturing or adding fine details. Just a quick sculpting and some basic lighting. Still took me around an hour and half to get it done.\nEnjoy!\n","permalink":"https://bedirtapkan.com/design/not_a_beast/","summary":"Just got my HUION 22 Pro a couple weeks ago, had to jump in and make something interesting. Didn\u0026rsquo;t think much :) Just pen on the tablet!\nDidn\u0026rsquo;t spend much time texturing or adding fine details. Just a quick sculpting and some basic lighting. Still took me around an hour and half to get it done.\nEnjoy!","title":"I'm Not a Beast"},{"content":"","permalink":"https://bedirtapkan.com/tags/","summary":"","title":"Tags"}]